{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a39a75e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model Evaluation (Weighted) â€“ Notebook Guide\n",
    "\n",
    "This notebook evaluates models with class/observation weights applied.\n",
    "\n",
    "## What this notebook does\n",
    "- Compute weighted metrics (e.g., weighted AUC, threshold metrics)\n",
    "- Plot diagnostic figures considering weights\n",
    "- Summarize results per model/run and export\n",
    "\n",
    "## Inputs\n",
    "- Predictions/scores, ground-truth labels, and weights per observation\n",
    "- Optional: CV fold info or test set indicators\n",
    "\n",
    "## Workflow\n",
    "1. Load predictions, labels, and weights\n",
    "2. Validate alignment and handle missing values\n",
    "3. Compute weighted metrics across thresholds/folds\n",
    "4. Plot weighted ROC/curves and summaries\n",
    "5. Save metrics tables and figures\n",
    "\n",
    "## Outputs\n",
    "- Weighted per-model/per-fold metrics tables\n",
    "- Plots reflecting weights\n",
    "- CSV/JSON exports for downstream use\n",
    "\n",
    "## Notes\n",
    "- Ensure weights are normalized or in intended scale\n",
    "- Use consistent preprocessing as training\n",
    "- Fix random seeds for reproducibility where applicable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce62eb",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook evaluates weighted SDMs with metrics and plots, mirroring standard evaluation but accounting for weights in analysis where relevant.\n",
    "\n",
    "- Key steps: load weighted predictions, compute metrics, plot curves, thresholds, reporting\n",
    "- Inputs: weighted model predictions and labels\n",
    "- Outputs: evaluation tables and plots\n",
    "- Run order: After weighted model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b15204-7112-48a4-97c4-bc90bc40550d",
   "metadata": {},
   "source": [
    "# Weighted MaxEnt Model Evaluation and Performance Assessment\n",
    "\n",
    "This notebook provides comprehensive evaluation of **weighted MaxEnt species distribution models**, focusing on performance assessment that accounts for sample weights and data quality differences. Unlike standard model evaluation, this version incorporates **weighted metrics** to properly assess model performance when training data has been weighted.\n",
    "\n",
    "## Key Features of Weighted Model Evaluation:\n",
    "\n",
    "### 1. **Weighted Performance Metrics**:\n",
    "- **Weighted AUC**: Area Under ROC Curve accounting for sample weights\n",
    "- **Weighted PR-AUC**: Precision-Recall AUC with weight integration\n",
    "- **Weighted Sensitivity/Specificity**: Performance metrics adjusted for data quality\n",
    "- **Weighted Precision/Recall**: Classification metrics incorporating sample weights\n",
    "\n",
    "### 2. **Advanced Evaluation Approaches**:\n",
    "- **Cross-Validation**: K-fold validation with weighted samples\n",
    "- **Spatial Validation**: Geographic partitioning with weight consideration\n",
    "- **Temporal Validation**: Time-based splits accounting for temporal weights\n",
    "- **Bootstrap Validation**: Resampling with weight preservation\n",
    "\n",
    "### 3. **Bias Assessment**:\n",
    "- **Spatial Bias Analysis**: Evaluate model performance across different regions\n",
    "- **Temporal Bias Assessment**: Performance across different time periods\n",
    "- **Source Bias Evaluation**: Performance across different data sources\n",
    "- **Quality Bias Analysis**: Performance across different data quality levels\n",
    "\n",
    "## Applications:\n",
    "- **Model Validation**: Comprehensive assessment of weighted model performance\n",
    "- **Bias Detection**: Identify remaining biases after weighting\n",
    "- **Performance Comparison**: Compare weighted vs. unweighted models\n",
    "- **Quality Control**: Validate that weighting improves model reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f4dda-d3ca-47d5-91ad-7caa0a434170",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### WEIGHTED MODEL EVALUATION CONFIGURATION - MODIFY AS NEEDED ###############\n",
    "\n",
    "# Species and region settings for weighted model evaluation\n",
    "#specie = 'leptocybe-invasa'  # Target species: 'leptocybe-invasa' or 'thaumastocoris-peregrinus'\n",
    "#pseudoabsence = 'random'  # Background point strategy: 'random', 'biased', 'biased-land-cover'\n",
    "#training = 'east-asia'  # Training region: 'sea', 'australia', 'east-asia', etc.\n",
    "#interest = 'south-east-asia'  # Test region: can be same as training or different\n",
    "#savefig = True  # Save generated evaluation plots and metrics\n",
    "\n",
    "# Environmental variable configuration\n",
    "bio = bio1  # Bioclimatic variable identifier\n",
    "\n",
    "# Evaluation settings (specific to weighted model evaluation)\n",
    "# evaluation_method = 'cross_validation'  # 'cross_validation', 'spatial_validation', 'temporal_validation'\n",
    "# n_folds = 5  # Number of folds for cross-validation\n",
    "# spatial_buffer = 100  # Buffer distance (km) for spatial validation\n",
    "# temporal_split = 0.7  # Proportion of data for training in temporal validation\n",
    "\n",
    "# Weighted metrics configuration\n",
    "# include_weighted_metrics = True  # Calculate weighted performance metrics\n",
    "# include_unweighted_metrics = True  # Calculate standard metrics for comparison\n",
    "# weight_threshold = 0.1  # Minimum weight threshold for sample inclusion\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e46ce-499c-4676-9ff0-f796122a3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os  # File system operations\n",
    "\n",
    "import numpy as np  # Numerical computing\n",
    "import xarray as xr  # Multi-dimensional labeled arrays (raster data)\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import geopandas as gpd  # Geospatial data handling\n",
    "\n",
    "import elapid as ela  # Species distribution modeling library\n",
    "\n",
    "from shapely import wkt  # Well-Known Text (WKT) geometry parsing\n",
    "from elapid import utils  # Utility functions for elapid\n",
    "from sklearn import metrics, inspection  # Machine learning metrics and model inspection\n",
    "\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warning messages for cleaner output\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6724e-cd4f-4099-aba5-4b81214f135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_layout(nplots):\n",
    "    \"\"\"\n",
    "    Calculate optimal subplot layout for given number of plots\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nplots : int\n",
    "        Number of plots to arrange\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ncols, nrows : tuple\n",
    "        Number of columns and rows for subplot layout\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate square root and round up for balanced layout\n",
    "    ncols = min(int(np.ceil(np.sqrt(nplots))), 4)  # Max 4 columns\n",
    "    nrows = int(np.ceil(nplots / ncols))  # Calculate rows needed\n",
    "    \n",
    "    return ncols, nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6db49b-be58-4919-b395-1e6978805f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SET UP FILE PATHS\n",
    "# =============================================================================\n",
    "# Define directory structure for organizing weighted model evaluation outputs\n",
    "\n",
    "docs_path = os.path.join(os.path.dirname(os.getcwd()), 'docs')  # Documentation directory\n",
    "out_path = os.path.join(os.path.dirname(os.getcwd()), 'out', specie)  # Species-specific output directory\n",
    "figs_path = os.path.join(os.path.dirname(os.getcwd()), 'figs')  # Figures directory\n",
    "output_path = os.path.join(out_path, 'output')  # Model output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7a900-85cd-41d4-87e6-7179c9320233",
   "metadata": {},
   "source": [
    "## 1. Weighted Training Model Performance Assessment\n",
    "\n",
    "This section evaluates the performance of the weighted MaxEnt model on the training data. Key aspects include:\n",
    "\n",
    "### **Weighted vs. Unweighted Metrics**:\n",
    "- **Standard Metrics**: Traditional AUC, PR-AUC, sensitivity, specificity\n",
    "- **Weighted Metrics**: Performance metrics accounting for sample weights\n",
    "- **Comparison Analysis**: Evaluate improvement from weighting approach\n",
    "\n",
    "### **Performance Indicators**:\n",
    "- **ROC-AUC**: Area Under Receiver Operating Characteristic curve\n",
    "- **PR-AUC**: Area Under Precision-Recall curve (important for imbalanced data)\n",
    "- **Sensitivity**: True Positive Rate (ability to detect presences)\n",
    "- **Specificity**: True Negative Rate (ability to detect absences)\n",
    "- **Precision**: Positive Predictive Value\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### **Weighted Evaluation Benefits**:\n",
    "- **Quality-Aware Assessment**: Metrics reflect data quality differences\n",
    "- **Bias-Corrected Performance**: Reduced influence of low-quality samples\n",
    "- **Robust Validation**: More reliable performance estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d23f6-4868-4e17-8dca-2d60b2411509",
   "metadata": {},
   "source": [
    "## References for Species Distribution Model Evaluation\n",
    "\n",
    "### **Model Output Interpretation**:\n",
    "- [SDM Model Outputs Interpretation](https://support.ecocommons.org.au/support/solutions/articles/6000256107-interpretation-of-sdm-model-outputs)\n",
    "- [Presence-Only Prediction in GIS](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/how-presence-only-prediction-works.htm)\n",
    "- [MaxEnt 101: Species Distribution Modeling](https://www.esri.com/arcgis-blog/products/arcgis-pro/analytics/presence-only-prediction-maxent-101-using-gis-to-model-species-distribution/)\n",
    "\n",
    "### **Performance Metrics**:\n",
    "- [ROC Curves Demystified](https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0)\n",
    "- [Precision-Recall AUC Guide](https://www.aporia.com/learn/ultimate-guide-to-precision-recall-auc-understanding-calculating-using-pr-auc-in-ml/)\n",
    "- [F1-Score, Accuracy, ROC-AUC, and PR-AUC Metrics](https://deepchecks.com/f1-score-accuracy-roc-auc-and-pr-auc-metrics-for-models/)\n",
    "\n",
    "### **Weighted Model Evaluation**:\n",
    "- **Sample Weighting**: How to properly evaluate models trained with sample weights\n",
    "- **Bias Correction**: Assessing the effectiveness of weighting strategies\n",
    "- **Quality Integration**: Incorporating data quality into performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa4635-ab08-4b3d-9fa0-07271788cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD WEIGHTED MODEL AND TRAINING DATA\n",
    "# =============================================================================\n",
    "# Load the trained weighted MaxEnt model and associated training data for evaluation\n",
    "\n",
    "# Build experiment directory name (keeps runs organized by config)\n",
    "# Alternate naming (older): 'exp_%s_%s_%s' % (pseudoabsence, training, interest)\n",
    "experiment_name = 'exp_%s_%s_%s_%s_%s' % (model_prefix, pseudoabsence, training, topo, ndvi)\n",
    "exp_path = os.path.join(output_path, experiment_name)  # Path to experiment directory\n",
    "\n",
    "# Construct expected filenames produced during training for this run\n",
    "train_input_data_name = '%s_model-train_input-data_%s_%s_%s_%s_%s.csv' % (model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "run_name = '%s_model-train_%s_%s_%s_%s_%s.ela' % (model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "nc_name = '%s_model-train_%s_%s_%s_%s_%s.nc' % (model_prefix, specie, pseudoabsence, training, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428443d1-2a5b-403e-a99c-a3395954e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD TRAINING DATA WITH SAMPLE WEIGHTS\n",
    "# =============================================================================\n",
    "# Load training data including sample weights for weighted model evaluation\n",
    "\n",
    "# Load training data from CSV file (index_col=0 to drop old index column)\n",
    "df = pd.read_csv(os.path.join(exp_path, train_input_data_name), index_col=0)\n",
    "# Parse WKT strings into shapely geometries\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# Wrap as GeoDataFrame with WGS84 CRS\n",
    "train = gpd.GeoDataFrame(df, crs='EPSG:4326')\n",
    "\n",
    "# Split predictors/labels/weights for weighted evaluation\n",
    "x_train = train.drop(columns=['class', 'SampleWeight', 'geometry'])  # Environmental variables only\n",
    "y_train = train['class']  # Presence/absence labels (0/1)\n",
    "sample_weight_train = train['SampleWeight']  # Sample weights aligned with rows\n",
    "\n",
    "# Load fitted weighted MaxEnt model\n",
    "model_train = utils.load_object(os.path.join(exp_path, run_name))\n",
    "\n",
    "# Predict probabilities on training set (for curves/metrics)\n",
    "y_train_predict = model_train.predict(x_train)\n",
    "# Optional: impute NaN probabilities to 0.5 (neutral)\n",
    "# y_train_predict = np.nan_to_num(y_train_predict, nan=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abc1a9-3db2-4960-ad8f-e042eb214fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training performance metrics\n",
    "\n",
    "# ROC curve and AUC (unweighted vs weighted)\n",
    "# fpr/tpr are computed from predicted probabilities; weights adjust contribution per sample\n",
    "fpr_train, tpr_train, thresholds = metrics.roc_curve(y_train, y_train_predict)\n",
    "auc_train = metrics.roc_auc_score(y_train, y_train_predict)\n",
    "auc_train_weighted = metrics.roc_auc_score(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "\n",
    "# Precision-Recall curve and PR-AUC (more informative on class imbalance)\n",
    "precision_train, recall_train, _ = metrics.precision_recall_curve(y_train, y_train_predict)\n",
    "pr_auc_train = metrics.auc(recall_train, precision_train)\n",
    "# Weighted PR curve uses sample weights to compute precision/recall\n",
    "precision_train_w, recall_train_w, _ = metrics.precision_recall_curve(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "pr_auc_train_weighted = metrics.auc(recall_train_w, precision_train_w)\n",
    "\n",
    "# Report metrics\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score  : {auc_train_weighted:0.3f}\")\n",
    "print(f\"PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cccf4-2b98-44d3-8725-227a49bb3c31",
   "metadata": {},
   "source": [
    "|  |  | Specie existance |  |\n",
    "| ------ | :-------: | :------: | :-------: |\n",
    "| |  | **+** | **--** |\n",
    "| **Specie observed** | **+** | True Positive (TP) | False Positive (FP) |\n",
    "| | **--** | False Negative (FN) | True Negative (TN) |\n",
    "| | | **All existing species (TP + FN)** | **All non-existing species (FP + TN)** |\n",
    "\n",
    "\n",
    "$$TPR = \\frac{TP}{TP + FN}$$\n",
    "$$FPR = \\frac{FP}{FP + TN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588ba66-5615-4d10-b8db-26ab26462e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training distributions and curves\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Left: Predicted probability distributions for presence vs pseudo-absence\n",
    "ax[0].hist(y_train_predict[y_train == 0], bins=np.linspace(0, 1, int((y_train == 0).sum() / 100 + 1)),\n",
    "           density=True, color='tab:red', alpha=0.7, label='pseudo-absence')\n",
    "ax[0].hist(y_train_predict[y_train == 1], bins=np.linspace(0, 1, int((y_train == 1).sum() / 10 + 1)),\n",
    "           density=True, color='tab:green', alpha=0.7, label='presence')\n",
    "ax[0].set_xlabel('Relative Occurrence Probability')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_title('Probability Distribution')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "# Middle: ROC curve (random vs perfect baselines + model)\n",
    "ax[1].plot([0, 1], [0, 1], '--', label='AUC score: 0.5 (No Skill)', color='gray')\n",
    "ax[1].text(0.4, 0.4, 'random classifier', fontsize=12, color='gray', rotation=45, rotation_mode='anchor',\n",
    "           horizontalalignment='left', verticalalignment='bottom', transform=ax[1].transAxes)\n",
    "ax[1].plot([0, 0, 1], [0, 1, 1], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[1].text(0, 1, '  perfect classifier', fontsize=12, color='tab:blue', horizontalalignment='left', verticalalignment='bottom')\n",
    "ax[1].scatter(0, 1, marker='*', s=100, color='tab:blue')\n",
    "# Overlay model ROC (unweighted and weighted AUC labels)\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC score: {auc_train:0.3f}', color='tab:orange')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC Weighted score: {auc_train_weighted:0.3f}', color='tab:cyan', linestyle='-.')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('MaxEnt ROC Curve')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Right: Precision-Recall curve (random/perfect baselines + model)\n",
    "ax[2].plot([0, 1], [0.5, 0.5], '--', color='gray', label='AUC score: 0.5 (No Skill)')\n",
    "ax[2].text(0.5, 0.52, 'random classifier', fontsize=12, color='gray', horizontalalignment='center', verticalalignment='center')\n",
    "ax[2].plot([0, 1, 1], [1, 1, 0], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[2].text(1, 1, 'perfect classifier  ', fontsize=12, color='tab:blue', horizontalalignment='right', verticalalignment='bottom')\n",
    "ax[2].scatter(1, 1, marker='*', s=100, color='tab:blue')\n",
    "# Overlay model PR curves (unweighted and weighted AUC labels)\n",
    "ax[2].plot(recall_train, precision_train, label=f'AUC score: {pr_auc_train:0.3f}', color='tab:orange')\n",
    "ax[2].plot(recall_train_w, precision_train_w, label=f\"AUC Weighted score: {pr_auc_train_weighted:0.3f}\", color='tab:cyan', linestyle='-.')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set_xlabel('Recall')\n",
    "ax[2].set_ylabel('Precision')\n",
    "ax[2].set_title('MaxEnt PR Curve')\n",
    "ax[2].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b30af-f85b-4419-baa9-ad808d2dfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figures if requested. Uses different filename patterns for current vs future scenarios.\n",
    "# Note: 'models' is used to gate inclusion of model prefix; ensure it exists in your session.\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:  # include model identifier when available\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: omit model prefix when not specified\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993984c-f068-4953-8a73-841a09f30b72",
   "metadata": {},
   "source": [
    "## 2. Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f358d3-224d-4be8-a1d2-82f1f3457b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data_name = '%s_model-test_input-data_%s_%s_%s_%s_%s.csv' %(model_prefix, specie, pseudoabsence, interest, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7166b-a73a-441c-a066-ec8e957e04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load held-out test dataset for evaluation\n",
    "# Note: index_col=0 drops the old index saved during export\n",
    "df = pd.read_csv(os.path.join(exp_path, test_input_data_name), index_col=0)\n",
    "# Convert WKT geometry back to shapely objects\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# Wrap as GeoDataFrame (WGS84 CRS)\n",
    "test = gpd.GeoDataFrame(df, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550482cb-b0da-489c-894d-711a890cd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split predictors/labels/weights for test set\n",
    "x_test = test.drop(columns=['class', 'SampleWeight', 'geometry'])\n",
    "y_test = test['class']\n",
    "sample_weight_test = test['SampleWeight']\n",
    "\n",
    "# Predict probabilities on the test set using the trained model\n",
    "y_test_predict = model_train.predict(x_test)\n",
    "# Optional: impute NaN probabilities to 0.5 if present\n",
    "# y_test_predict = np.nan_to_num(y_test_predict, nan=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c432c6-86ef-421f-a4fe-2f1cee794e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set metrics: ROC/PR curves and AUCs (unweighted vs weighted)\n",
    "# ROC\n",
    "fpr_test, tpr_test, _ = metrics.roc_curve(y_test, y_test_predict)\n",
    "auc_test = metrics.roc_auc_score(y_test, y_test_predict)\n",
    "auc_test_weighted = metrics.roc_auc_score(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "\n",
    "# Precision-Recall (PR)\n",
    "precision_test, recall_test, _ = metrics.precision_recall_curve(y_test, y_test_predict)\n",
    "pr_auc_test = metrics.auc(recall_test, precision_test)\n",
    "precision_test_w, recall_test_w, _ = metrics.precision_recall_curve(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "pr_auc_test_weighted = metrics.auc(recall_test_w, precision_test_w)\n",
    "\n",
    "# Print summary of training vs test for quick comparison\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score: {auc_train_weighted:0.3f}\")\n",
    "print(f\"Test ROC-AUC score: {auc_test:0.3f}\")\n",
    "print(f\"Test ROC-AUC Weighted score: {auc_test_weighted:0.3f}\")\n",
    "\n",
    "print(f\"Training PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"Training PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")\n",
    "print(f\"Test PR-AUC Score: {pr_auc_test:0.3f}\")\n",
    "print(f\"Test PR-AUC Weighted Score: {pr_auc_test_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22fa49-a574-4a0f-b998-4028ab09cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test distributions and curves alongside training for comparison\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Left: Predicted probability distributions on test set\n",
    "ax[0].hist(y_test_predict[y_test == 0], bins=np.linspace(0, 1, int((y_test == 0).sum() / 100 + 1)),\n",
    "           density=True, color='tab:red', alpha=0.7, label='pseudo-absence')\n",
    "ax[0].hist(y_test_predict[y_test == 1], bins=np.linspace(0, 1, int((y_test == 1).sum() / 10 + 1)),\n",
    "           density=True, color='tab:green', alpha=0.7, label='presence')\n",
    "ax[0].set_xlabel('Relative Occurrence Probability')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_title('Probability Distribution')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "# Middle: ROC curves (train vs test, with weighted variants labeled)\n",
    "ax[1].plot([0, 1], [0, 1], '--', label='AUC score: 0.5 (No Skill)', color='gray')\n",
    "ax[1].text(0.4, 0.4, 'random classifier', fontsize=12, color='gray', rotation=45, rotation_mode='anchor',\n",
    "           horizontalalignment='left', verticalalignment='bottom', transform=ax[1].transAxes)\n",
    "ax[1].plot([0, 0, 1], [0, 1, 1], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[1].text(0, 1, '  perfect classifier', fontsize=12, color='tab:blue', horizontalalignment='left', verticalalignment='bottom')\n",
    "ax[1].scatter(0, 1, marker='*', s=100, color='tab:blue')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC train score: {auc_train:0.3f}', color='tab:orange')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC Weighted train score: {auc_train_weighted:0.3f}', color='tab:cyan', linestyle='-.')\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'AUC test score: {auc_test:0.3f}', color='tab:green')\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'AUC Weighted test score: {auc_test_weighted:0.3f}', color='tab:olive', linestyle='-.')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('MaxEnt ROC Curve')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Right: PR curves (train vs test)\n",
    "ax[2].plot([0, 1], [0.5, 0.5], '--', color='gray', label='AUC score: 0.5 (No Skill)')\n",
    "ax[2].text(0.5, 0.52, 'random classifier', fontsize=12, color='gray', horizontalalignment='center', verticalalignment='center')\n",
    "ax[2].plot([0, 1, 1], [1, 1, 0], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[2].text(1, 1, 'perfect classifier  ', fontsize=12, color='tab:blue', horizontalalignment='right', verticalalignment='bottom')\n",
    "ax[2].scatter(1, 1, marker='*', s=100, color='tab:blue')\n",
    "ax[2].plot(recall_train, precision_train, label=f'AUC train score: {pr_auc_train:0.3f}', color='tab:orange')\n",
    "ax[2].plot(recall_train_w, precision_train_w, label=f\"AUC train Weighted score: {pr_auc_train_weighted:0.3f}\", color='tab:cyan', linestyle='-.')\n",
    "ax[2].plot(recall_test, precision_test, label=f'AUC test score: {pr_auc_test:0.3f}', color='tab:green')\n",
    "ax[2].plot(recall_test_w, precision_test_w, label=f'AUC test Weighted score: {pr_auc_test_weighted:0.3f}', color='tab:olive', linestyle='-.')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set_xlabel('Recall')\n",
    "ax[2].set_ylabel('Precision')\n",
    "ax[2].set_title('MaxEnt PR Curve')\n",
    "ax[2].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b39edc-a114-46ba-a329-503387ab4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test figures if requested (future vs current naming handled similarly to training)\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s_future.png' % (specie, interest, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_future.png' % (specie, interest, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if model_prefix:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s.png' % (specie, interest, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s.png' % (specie, interest, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd5319-c762-4c96-ad41-ebeacbe27217",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeacf3a",
   "metadata": {},
   "source": [
    "## 3.1 Comprehensive Variable Importance Analysis\n",
    "\n",
    "This section performs a thorough analysis of variable importance by:\n",
    "\n",
    "1. **Initial Analysis**: Running the model with all 19 bioclimatic variables to establish baseline importance\n",
    "2. **Iterative Removal**: Systematically removing the least important variables until we reach ~5 most important variables\n",
    "3. **Performance Tracking**: Monitoring model performance as variables are removed\n",
    "4. **Final Recommendations**: Identifying the optimal subset of variables for the species distribution model\n",
    "\n",
    "### Methodology:\n",
    "- **Permutation Importance**: Measures the drop in model performance when each variable is randomly shuffled\n",
    "- **Iterative Backward Elimination**: Removes least important variables one at a time\n",
    "- **Performance Monitoring**: Tracks AUC, PR-AUC, and other metrics throughout the process\n",
    "- **Cross-Validation**: Ensures robust importance estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b286503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE VARIABLE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Initialize storage for results\n",
    "importance_results = {}\n",
    "performance_history = {}\n",
    "variable_subsets = {}\n",
    "\n",
    "# Get current variable names from training data\n",
    "current_variables = list(x_train.columns)\n",
    "\n",
    "# Store initial performance metrics\n",
    "initial_metrics = {\n",
    "    'train_auc': auc_train,\n",
    "    'train_auc_weighted': auc_train_weighted,\n",
    "    'train_pr_auc': pr_auc_train,\n",
    "    'train_pr_auc_weighted': pr_auc_train_weighted,\n",
    "    'test_auc': auc_test,\n",
    "    'test_auc_weighted': auc_test_weighted,\n",
    "    'test_pr_auc': pr_auc_test,\n",
    "    'test_pr_auc_weighted': pr_auc_test_weighted\n",
    "}\n",
    "\n",
    "performance_history['all_variables'] = initial_metrics\n",
    "variable_subsets['all_variables'] = current_variables.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46769501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ITERATIVE VARIABLE REMOVAL FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def iterative_variable_removal(x_train, y_train, sample_weight_train, x_test, y_test, sample_weight_test, \n",
    "                              target_variables=5, min_variables=3):\n",
    "    \"\"\"\n",
    "    Iteratively remove least important variables until reaching target number.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train, y_train, sample_weight_train : training data\n",
    "    x_test, y_test, sample_weight_test : test data  \n",
    "    target_variables : int, target number of variables to keep\n",
    "    min_variables : int, minimum number of variables to keep\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict, containing importance rankings and performance history\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'importance_rankings': {},\n",
    "        'performance_history': {},\n",
    "        'removed_variables': [],\n",
    "        'final_variables': []\n",
    "    }\n",
    "    \n",
    "    current_x_train = x_train.copy()\n",
    "    current_x_test = x_test.copy()\n",
    "    current_vars = list(current_x_train.columns)\n",
    "    iteration = 0\n",
    "    \n",
    "    print(f\"Starting iterative removal from {len(current_vars)} to {target_variables} variables...\")\n",
    "    \n",
    "    while len(current_vars) > max(target_variables, min_variables):\n",
    "        iteration += 1\n",
    "        print(f\"\\n--- Iteration {iteration}: {len(current_vars)} variables remaining ---\")\n",
    "        \n",
    "        # Train model with current variables\n",
    "        model_iter = ela.MaxentModel()\n",
    "        model_iter.fit(current_x_train, y_train, sample_weight=sample_weight_train)\n",
    "        \n",
    "        # Calculate permutation importance\n",
    "        pi = inspection.permutation_importance(\n",
    "            model_iter, current_x_train, y_train, \n",
    "            sample_weight=sample_weight_train, n_repeats=10\n",
    "        )\n",
    "        \n",
    "        # Get importance scores and rank variables\n",
    "        importance_scores = pi.importances.mean(axis=1)\n",
    "        var_importance = dict(zip(current_vars, importance_scores))\n",
    "        sorted_vars = sorted(var_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Store ranking for this iteration\n",
    "        results['importance_rankings'][f'iteration_{iteration}'] = {\n",
    "            'variables': current_vars.copy(),\n",
    "            'importance_scores': var_importance.copy(),\n",
    "            'sorted_ranking': sorted_vars.copy()\n",
    "        }\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        y_train_pred = model_iter.predict(current_x_train)\n",
    "        y_test_pred = model_iter.predict(current_x_test)\n",
    "        \n",
    "        # Training metrics\n",
    "        train_auc = metrics.roc_auc_score(y_train, y_train_pred)\n",
    "        train_auc_weighted = metrics.roc_auc_score(y_train, y_train_pred, sample_weight=sample_weight_train)\n",
    "        train_precision, train_recall, _ = metrics.precision_recall_curve(y_train, y_train_pred)\n",
    "        train_pr_auc = metrics.auc(train_recall, train_precision)\n",
    "        train_precision_w, train_recall_w, _ = metrics.precision_recall_curve(y_train, y_train_pred, sample_weight=sample_weight_train)\n",
    "        train_pr_auc_weighted = metrics.auc(train_recall_w, train_precision_w)\n",
    "        \n",
    "        # Test metrics\n",
    "        test_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "        test_auc_weighted = metrics.roc_auc_score(y_test, y_test_pred, sample_weight=sample_weight_test)\n",
    "        test_precision, test_recall, _ = metrics.precision_recall_curve(y_test, y_test_pred)\n",
    "        test_pr_auc = metrics.auc(test_recall, test_precision)\n",
    "        test_precision_w, test_recall_w, _ = metrics.precision_recall_curve(y_test, y_test_pred, sample_weight=sample_weight_test)\n",
    "        test_pr_auc_weighted = metrics.auc(test_recall_w, test_precision_w)\n",
    "        \n",
    "        # Store performance\n",
    "        results['performance_history'][f'iteration_{iteration}'] = {\n",
    "            'n_variables': len(current_vars),\n",
    "            'train_auc': train_auc,\n",
    "            'train_auc_weighted': train_auc_weighted,\n",
    "            'train_pr_auc': train_pr_auc,\n",
    "            'train_pr_auc_weighted': train_pr_auc_weighted,\n",
    "            'test_auc': test_auc,\n",
    "            'test_auc_weighted': test_auc_weighted,\n",
    "            'test_pr_auc': test_pr_auc,\n",
    "            'test_pr_auc_weighted': test_pr_auc_weighted\n",
    "        }\n",
    "        \n",
    "        # Print current performance\n",
    "        print(f\"Performance with {len(current_vars)} variables:\")\n",
    "        print(f\"  Train AUC: {train_auc:.3f} (weighted: {train_auc_weighted:.3f})\")\n",
    "        print(f\"  Test AUC: {test_auc:.3f} (weighted: {test_auc_weighted:.3f})\")\n",
    "        print(f\"  Train PR-AUC: {train_pr_auc:.3f} (weighted: {train_pr_auc_weighted:.3f})\")\n",
    "        print(f\"  Test PR-AUC: {test_pr_auc:.3f} (weighted: {test_pr_auc_weighted:.3f})\")\n",
    "        \n",
    "        # Identify least important variable\n",
    "        least_important_var = sorted_vars[-1][0]\n",
    "        least_important_score = sorted_vars[-1][1]\n",
    "        \n",
    "        print(f\"Least important variable: {least_important_var} (importance: {least_important_score:.4f})\")\n",
    "        \n",
    "        # Remove least important variable\n",
    "        current_x_train = current_x_train.drop(columns=[least_important_var])\n",
    "        current_x_test = current_x_test.drop(columns=[least_important_var])\n",
    "        current_vars.remove(least_important_var)\n",
    "        results['removed_variables'].append(least_important_var)\n",
    "        \n",
    "        print(f\"Removed {least_important_var}. Variables remaining: {current_vars}\")\n",
    "    \n",
    "    results['final_variables'] = current_vars.copy()\n",
    "    \n",
    "    print(f\"\\nFinal variable set ({len(current_vars)} variables): {current_vars}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN ITERATIVE VARIABLE REMOVAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# print(\"=\"*80)\n",
    "# print(\"COMPREHENSIVE VARIABLE IMPORTANCE ANALYSIS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# Run the iterative removal process\n",
    "start_time = time.time()\n",
    "\n",
    "# Set target to 5 variables (can be adjusted)\n",
    "target_vars = 3\n",
    "min_vars = 3\n",
    "\n",
    "# Run iterative removal\n",
    "removal_results = iterative_variable_removal(\n",
    "    x_train, y_train, sample_weight_train,\n",
    "    x_test, y_test, sample_weight_test,\n",
    "    target_variables=target_vars,\n",
    "    min_variables=min_vars\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "# print(f\"\\nAnalysis completed in {end_time - start_time:.1f} seconds\")\n",
    "\n",
    "# Store results for later analysis\n",
    "importance_results['iterative_removal'] = removal_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALYZE AND VISUALIZE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "# Extract performance trends\n",
    "iterations = list(removal_results['performance_history'].keys())\n",
    "n_vars = [removal_results['performance_history'][iter]['n_variables'] for iter in iterations]\n",
    "train_aucs = [removal_results['performance_history'][iter]['train_auc'] for iter in iterations]\n",
    "test_aucs = [removal_results['performance_history'][iter]['test_auc'] for iter in iterations]\n",
    "train_aucs_weighted = [removal_results['performance_history'][iter]['train_auc_weighted'] for iter in iterations]\n",
    "test_aucs_weighted = [removal_results['performance_history'][iter]['test_auc_weighted'] for iter in iterations]\n",
    "\n",
    "# Add initial performance (all variables)\n",
    "n_vars.insert(0, len(x_train.columns))\n",
    "train_aucs.insert(0, auc_train)\n",
    "test_aucs.insert(0, auc_test)\n",
    "train_aucs_weighted.insert(0, auc_train_weighted)\n",
    "test_aucs_weighted.insert(0, auc_test_weighted)\n",
    "\n",
    "# # Get final variable ranking\n",
    "final_iteration = f\"iteration_{len(iterations)}\"\n",
    "final_ranking = removal_results['importance_rankings'][final_iteration]['sorted_ranking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae31219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE COMPREHENSIVE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create a comprehensive figure showing the analysis results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Comprehensive Variable Importance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Performance vs Number of Variables\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(n_vars, train_aucs, 'o-', label='Train AUC', color='tab:blue', linewidth=2)\n",
    "ax1.plot(n_vars, test_aucs, 's-', label='Test AUC', color='tab:orange', linewidth=2)\n",
    "ax1.plot(n_vars, train_aucs_weighted, 'o--', label='Train AUC (Weighted)', color='tab:blue', alpha=0.7)\n",
    "ax1.plot(n_vars, test_aucs_weighted, 's--', label='Test AUC (Weighted)', color='tab:orange', alpha=0.7)\n",
    "ax1.set_xlabel('Number of Variables')\n",
    "ax1.set_ylabel('AUC Score')\n",
    "ax1.set_title('Model Performance vs Number of Variables')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.invert_xaxis()  # Show decreasing variables\n",
    "\n",
    "# 2. Final Variable Importance (Top 10)\n",
    "ax2 = axes[0, 1]\n",
    "top_vars = final_ranking[:10]  # Top 10 variables\n",
    "var_names = [var[0] for var in top_vars]\n",
    "var_importance = [var[1] for var in top_vars]\n",
    "\n",
    "bars = ax2.barh(range(len(var_names)), var_importance, color='tab:green', alpha=0.7)\n",
    "ax2.set_yticks(range(len(var_names)))\n",
    "ax2.set_yticklabels(var_names)\n",
    "ax2.set_xlabel('Permutation Importance')\n",
    "ax2.set_title('Top 10 Most Important Variables')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, val) in enumerate(zip(bars, var_importance)):\n",
    "    ax2.text(val + 0.001, i, f'{val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# 3. Variable Removal Timeline\n",
    "ax3 = axes[1, 0]\n",
    "removed_vars = removal_results['removed_variables']\n",
    "removal_order = list(range(1, len(removed_vars) + 1))\n",
    "ax3.bar(removal_order, [1] * len(removed_vars), color='tab:red', alpha=0.7)\n",
    "ax3.set_xlabel('Removal Order')\n",
    "ax3.set_ylabel('Variables Removed')\n",
    "ax3.set_title('Variable Removal Timeline')\n",
    "ax3.set_xticks(removal_order)\n",
    "ax3.set_xticklabels([f'#{i}' for i in removal_order])\n",
    "\n",
    "# Add variable names as text\n",
    "for i, var in enumerate(removed_vars):\n",
    "    ax3.text(i + 1, 0.5, var, rotation=90, ha='center', va='center', fontsize=8)\n",
    "\n",
    "# 4. Performance Degradation Analysis\n",
    "ax4 = axes[1, 1]\n",
    "# Calculate performance drop from initial\n",
    "initial_test_auc = test_aucs[0]\n",
    "initial_train_auc = train_aucs[0]\n",
    "test_drop = [(initial_test_auc - auc) / initial_test_auc * 100 for auc in test_aucs]\n",
    "train_drop = [(initial_train_auc - auc) / initial_train_auc * 100 for auc in train_aucs]\n",
    "\n",
    "ax4.plot(n_vars, test_drop, 'o-', label='Test AUC Drop %', color='tab:red', linewidth=2)\n",
    "ax4.plot(n_vars, train_drop, 's-', label='Train AUC Drop %', color='tab:purple', linewidth=2)\n",
    "ax4.set_xlabel('Number of Variables')\n",
    "ax4.set_ylabel('Performance Drop (%)')\n",
    "ax4.set_title('Performance Degradation with Variable Removal')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.invert_xaxis()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the comprehensive analysis figure\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration)\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration)\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration)\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s.png' % (specie, training, bio, iteration)\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    print(f\"Comprehensive analysis figure saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8afc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS TO CSV FOR FURTHER ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Create summary DataFrame for export\n",
    "summary_data = []\n",
    "\n",
    "# Add initial performance (all variables)\n",
    "summary_data.append({\n",
    "    'iteration': 0,\n",
    "    'n_variables': len(x_train.columns),\n",
    "    'variables_removed': 'none',\n",
    "    'train_auc': auc_train,\n",
    "    'train_auc_weighted': auc_train_weighted,\n",
    "    'test_auc': auc_test,\n",
    "    'test_auc_weighted': auc_test_weighted,\n",
    "    'train_pr_auc': pr_auc_train,\n",
    "    'train_pr_auc_weighted': pr_auc_train_weighted,\n",
    "    'test_pr_auc': pr_auc_test,\n",
    "    'test_pr_auc_weighted': pr_auc_test_weighted\n",
    "})\n",
    "\n",
    "# Add iterative removal results\n",
    "for i, iter_key in enumerate(iterations, 1):\n",
    "    perf = removal_results['performance_history'][iter_key]\n",
    "    removed_var = removal_results['removed_variables'][i-1] if i-1 < len(removal_results['removed_variables']) else 'none'\n",
    "    \n",
    "    summary_data.append({\n",
    "        'iteration': i,\n",
    "        'n_variables': perf['n_variables'],\n",
    "        'variables_removed': removed_var,\n",
    "        'train_auc': perf['train_auc'],\n",
    "        'train_auc_weighted': perf['train_auc_weighted'],\n",
    "        'test_auc': perf['test_auc'],\n",
    "        'test_auc_weighted': perf['test_auc_weighted'],\n",
    "        'train_pr_auc': perf['train_pr_auc'],\n",
    "        'train_pr_auc_weighted': perf['train_pr_auc_weighted'],\n",
    "        'test_pr_auc': perf['test_pr_auc'],\n",
    "        'test_pr_auc_weighted': perf['test_pr_auc_weighted']\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save to CSV\n",
    "if savefig:\n",
    "    csv_filename = f'06_variable_importance_analysis_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "    csv_path = os.path.join(figs_path, csv_filename)\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Analysis summary saved to: {csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a733f05-26cb-4da3-a95d-7adc42870020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels and open training output NetCDF for metadata\n",
    "labels = train.drop(columns=['class', 'geometry', 'SampleWeight']).columns.values\n",
    "training_output = xr.open_dataset(os.path.join(exp_path, nc_name))\n",
    "# display(labels)\n",
    "# display(training_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d9454-dce3-4b7d-922b-8f84e37f1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute partial dependence across features\n",
    "# - percentiles bounds the feature grid to observed range (2.5% to 97.5%)\n",
    "# - nbins controls resolution of the curve\n",
    "percentiles = (0.025, 0.975)\n",
    "nbins = 100\n",
    "\n",
    "mean = {}\n",
    "stdv = {}\n",
    "bins = {}\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    # Request individual PDP curves across samples, then summarize\n",
    "    pda = inspection.partial_dependence(\n",
    "        model_train,\n",
    "        x_train,\n",
    "        [idx],\n",
    "        percentiles=percentiles,\n",
    "        grid_resolution=nbins,\n",
    "        kind=\"individual\",\n",
    "    )\n",
    "\n",
    "    mean[label] = pda[\"individual\"][0].mean(axis=0)  # average response\n",
    "    stdv[label] = pda[\"individual\"][0].std(axis=0)   # variability across samples\n",
    "    bins[label] = pda[\"grid_values\"][0]              # feature grid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890d37e-af5a-4f15-966a-02bde1a1f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDPs with uncertainty bands for each predictor\n",
    "ncols, nrows = subplot_layout(len(labels))\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 6, nrows * 6))\n",
    "\n",
    "# Normalize axes list for consistent indexing\n",
    "if (nrows, ncols) == (1, 1):\n",
    "    ax = [axs]\n",
    "else:\n",
    "    ax = axs.ravel()\n",
    "\n",
    "xlabels = training_output.data_vars\n",
    "for iax, label in enumerate(labels):\n",
    "    ax[iax].set_title(label)\n",
    "    try:\n",
    "        ax[iax].set_xlabel(xlabels[label].long_name)\n",
    "    except (ValueError, AttributeError):\n",
    "        ax[iax].set_xlabel('No variable long_name')\n",
    "\n",
    "    # Uncertainty band: mean Â± std across individuals\n",
    "    ax[iax].fill_between(bins[label], mean[label] - stdv[label], mean[label] + stdv[label], alpha=0.25)\n",
    "    ax[iax].plot(bins[label], mean[label])\n",
    "\n",
    "# Style axes\n",
    "for axi in ax:\n",
    "    axi.set_ylim([0, 1])\n",
    "    axi.set_ylabel('probability of occurrence')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2d9fa-5f37-48b4-a03c-9f53f244b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save response curve figures if requested\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f47155-3512-4f32-9128-9595e1709c6a",
   "metadata": {},
   "source": [
    "## 3.2 Variable importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b404609-047b-4e31-bf5f-28b0de041906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = model_train.permutation_importance_plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4362f3-93b0-4626-a37b-bf36a36900dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance: measures drop in performance when each feature is shuffled\n",
    "# Higher drop => more important feature\n",
    "pi = inspection.permutation_importance(model_train, x_train, y_train, n_repeats=10)\n",
    "importance = pi.importances\n",
    "rank_order = importance.mean(axis=-1).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301c544-3a98-4bac-ab55-d0c07373f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation importances as horizontal boxplots (distribution over repeats)\n",
    "labels_ranked = [labels[idx] for idx in rank_order]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "box = ax.boxplot(importance[rank_order].T, vert=False, labels=labels_ranked)\n",
    "# Decorate legend labels for key boxplot elements\n",
    "box['fliers'][0].set_label('outlier')\n",
    "box['medians'][0].set_label('median')\n",
    "for icap, cap in enumerate(box['caps']):\n",
    "    if icap == 0:\n",
    "        cap.set_label('min-max')\n",
    "    cap.set_color('k')\n",
    "    cap.set_linewidth(2)\n",
    "for ibx, bx in enumerate(box['boxes']):\n",
    "    if ibx == 0:\n",
    "        bx.set_label('25-75%')\n",
    "    bx.set_color('gray')\n",
    "\n",
    "ax.set_xlabel('Importance')\n",
    "ax.legend(loc='lower right')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cc671-1f8a-4375-9af2-8286603483ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'model' variable is not null or empty\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_%s_future.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no model is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_future.png' %(specie, training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_%s.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s.png' %(specie, training, bio,iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e44506b",
   "metadata": {},
   "source": [
    "## 4. Model Performance vs Spatial Spread Analysis\n",
    "\n",
    "This section analyzes the relationship between model performance and spatial distribution characteristics of the species data. This analysis helps understand:\n",
    "\n",
    "### **Spatial Performance Metrics**:\n",
    "- **Geographic Distribution**: Spatial extent and clustering of presence/absence data\n",
    "- **Spatial Autocorrelation**: Degree of spatial clustering in the data\n",
    "- **Performance-Spatial Correlation**: How model performance varies with spatial characteristics\n",
    "- **Regional Bias Assessment**: Performance differences across geographic regions\n",
    "\n",
    "### **Key Spatial Analyses**:\n",
    "1. **Spatial Spread Metrics**: Calculate geographic extent, clustering, and distribution patterns\n",
    "2. **Performance-Spatial Correlation**: Analyze relationship between model accuracy and spatial characteristics\n",
    "3. **Regional Performance Maps**: Visualize model performance across different geographic areas\n",
    "4. **Spatial Bias Detection**: Identify regions where the model performs poorly due to spatial bias\n",
    "\n",
    "### **Applications**:\n",
    "- **Bias Assessment**: Identify spatial biases in model performance\n",
    "- **Transferability**: Evaluate model performance across different geographic regions\n",
    "- **Sampling Strategy**: Inform future data collection based on spatial performance patterns\n",
    "- **Model Validation**: Ensure model performs consistently across the study area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e11aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # SPATIAL SPREAD ANALYSIS - CALCULATE SPATIAL METRICS\n",
    "# # =============================================================================\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def calculate_spatial_metrics(gdf, class_column='class', weight_column='SampleWeight'):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive spatial metrics for presence/absence data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Geospatial data with presence/absence information\n",
    "    class_column : str\n",
    "        Column name containing presence/absence labels\n",
    "    weight_column : str\n",
    "        Column name containing sample weights\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    spatial_metrics : dict\n",
    "        Dictionary containing various spatial metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract coordinates\n",
    "    coords = np.array([[geom.x, geom.y] for geom in gdf.geometry])\n",
    "    presence_mask = gdf[class_column] == 1\n",
    "    absence_mask = gdf[class_column] == 0\n",
    "    \n",
    "    # Separate presence and absence coordinates\n",
    "    presence_coords = coords[presence_mask]\n",
    "    absence_coords = coords[absence_mask]\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Geographic extent metrics\n",
    "    if len(presence_coords) > 0:\n",
    "        metrics['presence_lat_range'] = np.ptp(presence_coords[:, 1])  # Latitude range\n",
    "        metrics['presence_lon_range'] = np.ptp(presence_coords[:, 0])  # Longitude range\n",
    "        metrics['presence_area_approx'] = metrics['presence_lat_range'] * metrics['presence_lon_range']\n",
    "        \n",
    "        # Calculate centroid\n",
    "        metrics['presence_centroid_lat'] = np.mean(presence_coords[:, 1])\n",
    "        metrics['presence_centroid_lon'] = np.mean(presence_coords[:, 0])\n",
    "    \n",
    "    if len(absence_coords) > 0:\n",
    "        metrics['absence_lat_range'] = np.ptp(absence_coords[:, 1])\n",
    "        metrics['absence_lon_range'] = np.ptp(absence_coords[:, 0])\n",
    "        metrics['absence_area_approx'] = metrics['absence_lat_range'] * metrics['absence_lon_range']\n",
    "        \n",
    "        metrics['absence_centroid_lat'] = np.mean(absence_coords[:, 1])\n",
    "        metrics['absence_centroid_lon'] = np.mean(absence_coords[:, 0])\n",
    "    \n",
    "    # 2. Spatial clustering metrics\n",
    "    if len(presence_coords) > 1:\n",
    "        # Calculate pairwise distances for presence points\n",
    "        presence_distances = pdist(presence_coords)\n",
    "        metrics['presence_mean_distance'] = np.mean(presence_distances)\n",
    "        metrics['presence_std_distance'] = np.std(presence_distances)\n",
    "        metrics['presence_min_distance'] = np.min(presence_distances)\n",
    "        metrics['presence_max_distance'] = np.max(presence_distances)\n",
    "        \n",
    "        # Nearest neighbor analysis for presence points\n",
    "        nbrs = NearestNeighbors(n_neighbors=min(6, len(presence_coords))).fit(presence_coords)\n",
    "        distances, indices = nbrs.kneighbors(presence_coords)\n",
    "        metrics['presence_mean_nn_distance'] = np.mean(distances[:, 1])  # Exclude self (index 0)\n",
    "    \n",
    "    if len(absence_coords) > 1:\n",
    "        # Calculate pairwise distances for absence points\n",
    "        absence_distances = pdist(absence_coords)\n",
    "        metrics['absence_mean_distance'] = np.mean(absence_distances)\n",
    "        metrics['absence_std_distance'] = np.std(absence_distances)\n",
    "        metrics['absence_min_distance'] = np.min(absence_distances)\n",
    "        metrics['absence_max_distance'] = np.max(absence_distances)\n",
    "        \n",
    "        # Nearest neighbor analysis for absence points\n",
    "        nbrs = NearestNeighbors(n_neighbors=min(6, len(absence_coords))).fit(absence_coords)\n",
    "        distances, indices = nbrs.kneighbors(absence_coords)\n",
    "        metrics['absence_mean_nn_distance'] = np.mean(distances[:, 1])\n",
    "    \n",
    "    # 3. Spatial separation between presence and absence\n",
    "    if len(presence_coords) > 0 and len(absence_coords) > 0:\n",
    "        # Calculate minimum distance between presence and absence points\n",
    "        all_distances = []\n",
    "        for pres_coord in presence_coords:\n",
    "            for abs_coord in absence_coords:\n",
    "                dist = np.sqrt(np.sum((pres_coord - abs_coord)**2))\n",
    "                all_distances.append(dist)\n",
    "        \n",
    "        metrics['min_presence_absence_distance'] = np.min(all_distances)\n",
    "        metrics['mean_presence_absence_distance'] = np.mean(all_distances)\n",
    "        metrics['max_presence_absence_distance'] = np.max(all_distances)\n",
    "    \n",
    "    # 4. Density metrics\n",
    "    total_area = metrics.get('presence_area_approx', 0) + metrics.get('absence_area_approx', 0)\n",
    "    if total_area > 0:\n",
    "        metrics['presence_density'] = len(presence_coords) / metrics.get('presence_area_approx', 1)\n",
    "        metrics['absence_density'] = len(absence_coords) / metrics.get('absence_area_approx', 1)\n",
    "    \n",
    "    # 5. Weighted spatial metrics\n",
    "    if weight_column in gdf.columns:\n",
    "        presence_weights = gdf[presence_mask][weight_column].values\n",
    "        absence_weights = gdf[absence_mask][weight_column].values\n",
    "        \n",
    "        if len(presence_weights) > 0:\n",
    "            metrics['presence_weighted_centroid_lat'] = np.average(presence_coords[:, 1], weights=presence_weights)\n",
    "            metrics['presence_weighted_centroid_lon'] = np.average(presence_coords[:, 0], weights=presence_weights)\n",
    "        \n",
    "        if len(absence_weights) > 0:\n",
    "            metrics['absence_weighted_centroid_lat'] = np.average(absence_coords[:, 1], weights=absence_weights)\n",
    "            metrics['absence_weighted_centroid_lon'] = np.average(absence_coords[:, 0], weights=absence_weights)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# # Calculate spatial metrics for training and test data\n",
    "train_spatial_metrics = calculate_spatial_metrics(train, 'class', 'SampleWeight')\n",
    "test_spatial_metrics = calculate_spatial_metrics(test, 'class', 'SampleWeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPATIAL AUTOCORRELATION AND CLUSTERING ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def calculate_spatial_autocorrelation(gdf, class_column='class', weight_column='SampleWeight', \n",
    "                                    max_distance=1.0, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Calculate spatial autocorrelation and clustering metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Geospatial data with presence/absence information\n",
    "    class_column : str\n",
    "        Column name containing presence/absence labels\n",
    "    weight_column : str\n",
    "        Column name containing sample weights\n",
    "    max_distance : float\n",
    "        Maximum distance for spatial autocorrelation analysis\n",
    "    n_neighbors : int\n",
    "        Number of neighbors for local spatial analysis\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    autocorr_metrics : dict\n",
    "        Dictionary containing spatial autocorrelation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract coordinates and labels\n",
    "    coords = np.array([[geom.x, geom.y] for geom in gdf.geometry])\n",
    "    labels = gdf[class_column].values\n",
    "    weights = gdf[weight_column].values if weight_column in gdf.columns else None\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Global spatial autocorrelation (Moran's I approximation)\n",
    "    if len(coords) > 1:\n",
    "        # Calculate distance matrix\n",
    "        dist_matrix = squareform(pdist(coords))\n",
    "        \n",
    "        # Create binary weight matrix based on distance threshold\n",
    "        weight_matrix = (dist_matrix <= max_distance).astype(float)\n",
    "        np.fill_diagonal(weight_matrix, 0)  # Remove self-connections\n",
    "        \n",
    "        # Calculate Moran's I\n",
    "        n = len(labels)\n",
    "        mean_label = np.mean(labels)\n",
    "        \n",
    "        # Numerator: sum of weighted deviations\n",
    "        numerator = 0\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if weight_matrix[i, j] > 0:\n",
    "                    numerator += weight_matrix[i, j] * (labels[i] - mean_label) * (labels[j] - mean_label)\n",
    "        \n",
    "        # Denominator: sum of squared deviations\n",
    "        denominator = np.sum((labels - mean_label) ** 2)\n",
    "        \n",
    "        # Moran's I\n",
    "        if denominator > 0 and np.sum(weight_matrix) > 0:\n",
    "            morans_i = (n / np.sum(weight_matrix)) * (numerator / denominator)\n",
    "            metrics['morans_i'] = morans_i\n",
    "        else:\n",
    "            metrics['morans_i'] = 0\n",
    "    \n",
    "    # 2. Local spatial clustering analysis\n",
    "    if len(coords) > n_neighbors:\n",
    "        # DBSCAN clustering\n",
    "        dbscan = DBSCAN(eps=max_distance, min_samples=3)\n",
    "        cluster_labels = dbscan.fit_predict(coords)\n",
    "        \n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        n_noise = list(cluster_labels).count(-1)\n",
    "        \n",
    "        metrics['dbscan_n_clusters'] = n_clusters\n",
    "        metrics['dbscan_n_noise'] = n_noise\n",
    "        metrics['dbscan_noise_ratio'] = n_noise / len(coords)\n",
    "        \n",
    "        # Silhouette score for clustering quality\n",
    "        if n_clusters > 1:\n",
    "            silhouette = silhouette_score(coords, cluster_labels)\n",
    "            metrics['dbscan_silhouette'] = silhouette\n",
    "        else:\n",
    "            metrics['dbscan_silhouette'] = 0\n",
    "    \n",
    "    # 3. Presence-specific clustering\n",
    "    presence_mask = labels == 1\n",
    "    if np.sum(presence_mask) > 3:  # Need at least 3 presence points\n",
    "        presence_coords = coords[presence_mask]\n",
    "        \n",
    "        # K-means clustering for presence points\n",
    "        n_presence = len(presence_coords)\n",
    "        k_clusters = min(5, n_presence // 2)  # Adaptive number of clusters\n",
    "        \n",
    "        if k_clusters > 1:\n",
    "            kmeans = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)\n",
    "            presence_cluster_labels = kmeans.fit_predict(presence_coords)\n",
    "            \n",
    "            # Calculate within-cluster sum of squares\n",
    "            wcss = kmeans.inertia_\n",
    "            metrics['presence_wcss'] = wcss\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            if k_clusters > 1:\n",
    "                presence_silhouette = silhouette_score(presence_coords, presence_cluster_labels)\n",
    "                metrics['presence_silhouette'] = presence_silhouette\n",
    "            else:\n",
    "                metrics['presence_silhouette'] = 0\n",
    "    \n",
    "    # 4. Spatial density analysis\n",
    "    if len(coords) > 0:\n",
    "        # Calculate local density using k-nearest neighbors\n",
    "        nbrs = NearestNeighbors(n_neighbors=min(n_neighbors, len(coords))).fit(coords)\n",
    "        distances, indices = nbrs.kneighbors(coords)\n",
    "        \n",
    "        # Local density as inverse of mean distance to neighbors\n",
    "        local_density = 1.0 / (distances[:, 1:].mean(axis=1) + 1e-6)  # Add small value to avoid division by zero\n",
    "        metrics['mean_local_density'] = np.mean(local_density)\n",
    "        metrics['std_local_density'] = np.std(local_density)\n",
    "        \n",
    "        # Density-weighted presence ratio\n",
    "        if weights is not None:\n",
    "            weighted_density = np.average(local_density, weights=weights)\n",
    "            metrics['weighted_local_density'] = weighted_density\n",
    "    \n",
    "    # 5. Spatial distribution uniformity\n",
    "    if len(coords) > 1:\n",
    "        # Calculate coefficient of variation in distances\n",
    "        all_distances = pdist(coords)\n",
    "        if len(all_distances) > 0:\n",
    "            cv_distances = np.std(all_distances) / np.mean(all_distances)\n",
    "            metrics['spatial_cv'] = cv_distances\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate spatial autocorrelation for training and test data\n",
    "train_autocorr = calculate_spatial_autocorrelation(train, 'class', 'SampleWeight')\n",
    "test_autocorr = calculate_spatial_autocorrelation(test, 'class', 'SampleWeight')\n",
    "\n",
    "# Interpret Moran's I values\n",
    "def interpret_morans_i(morans_i):\n",
    "    \"\"\"Interpret Moran's I values for spatial autocorrelation.\"\"\"\n",
    "    if morans_i > 0.3:\n",
    "        return \"Strong positive spatial autocorrelation (clustered)\"\n",
    "    elif morans_i > 0.1:\n",
    "        return \"Moderate positive spatial autocorrelation (somewhat clustered)\"\n",
    "    elif morans_i > -0.1:\n",
    "        return \"No significant spatial autocorrelation (random)\"\n",
    "    elif morans_i > -0.3:\n",
    "        return \"Moderate negative spatial autocorrelation (dispersed)\"\n",
    "    else:\n",
    "        return \"Strong negative spatial autocorrelation (highly dispersed)\"\n",
    "\n",
    "print(f\"\\nTRAINING DATA SPATIAL PATTERN:\")\n",
    "print(f\"Moran's I = {train_autocorr.get('morans_i', 0):.4f}\")\n",
    "print(f\"Interpretation: {interpret_morans_i(train_autocorr.get('morans_i', 0))}\")\n",
    "\n",
    "print(f\"\\nTEST DATA SPATIAL PATTERN:\")\n",
    "print(f\"Moran's I = {test_autocorr.get('morans_i', 0):.4f}\")\n",
    "print(f\"Interpretation: {interpret_morans_i(test_autocorr.get('morans_i', 0))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85feacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # PERFORMANCE VS SPATIAL SPREAD CORRELATION ANALYSIS\n",
    "# # =============================================================================\n",
    "\n",
    "def calculate_local_performance_metrics(gdf, predictions, true_labels, weights=None, \n",
    "                                      n_neighbors=10, min_samples=5):\n",
    "    \"\"\"\n",
    "    Calculate local performance metrics for spatial analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Geospatial data\n",
    "    predictions : array-like\n",
    "        Model predictions\n",
    "    true_labels : array-like\n",
    "        True labels\n",
    "    weights : array-like, optional\n",
    "        Sample weights\n",
    "    n_neighbors : int\n",
    "        Number of neighbors for local analysis\n",
    "    min_samples : int\n",
    "        Minimum samples required for local analysis\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    local_metrics : dict\n",
    "        Dictionary containing local performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    coords = np.array([[geom.x, geom.y] for geom in gdf.geometry])\n",
    "    local_metrics = {}\n",
    "    \n",
    "    # Calculate local AUC for each point\n",
    "    local_aucs = []\n",
    "    local_accuracies = []\n",
    "    local_precisions = []\n",
    "    local_recalls = []\n",
    "    local_densities = []\n",
    "    \n",
    "    for i, coord in enumerate(coords):\n",
    "        # Find neighbors\n",
    "        distances = np.sqrt(np.sum((coords - coord)**2, axis=1))\n",
    "        neighbor_indices = np.argsort(distances)[:n_neighbors]\n",
    "        \n",
    "        if len(neighbor_indices) >= min_samples:\n",
    "            # Get neighbor data\n",
    "            neighbor_labels = true_labels[neighbor_indices]\n",
    "            neighbor_predictions = predictions[neighbor_indices]\n",
    "            neighbor_weights = weights[neighbor_indices] if weights is not None else None\n",
    "            \n",
    "            # Calculate local metrics\n",
    "            if len(np.unique(neighbor_labels)) > 1:  # Need both classes\n",
    "                try:\n",
    "                    local_auc = metrics.roc_auc_score(neighbor_labels, neighbor_predictions, \n",
    "                                                    sample_weight=neighbor_weights)\n",
    "                    local_aucs.append(local_auc)\n",
    "                except:\n",
    "                    local_aucs.append(0.5)  # Default to random performance\n",
    "            else:\n",
    "                local_aucs.append(0.5)\n",
    "            \n",
    "            # Local accuracy\n",
    "            local_pred_binary = (neighbor_predictions > 0.5).astype(int)\n",
    "            local_accuracy = np.mean(local_pred_binary == neighbor_labels)\n",
    "            local_accuracies.append(local_accuracy)\n",
    "            \n",
    "            # Local precision and recall\n",
    "            if np.sum(neighbor_labels) > 0:  # Has positive samples\n",
    "                local_precision = metrics.precision_score(neighbor_labels, local_pred_binary, \n",
    "                                                        sample_weight=neighbor_weights, zero_division=0)\n",
    "                local_recall = metrics.recall_score(neighbor_labels, local_pred_binary, \n",
    "                                                  sample_weight=neighbor_weights, zero_division=0)\n",
    "                local_precisions.append(local_precision)\n",
    "                local_recalls.append(local_recall)\n",
    "            else:\n",
    "                local_precisions.append(0)\n",
    "                local_recalls.append(0)\n",
    "            \n",
    "            # Local density\n",
    "            local_density = 1.0 / (np.mean(distances[neighbor_indices[1:]]) + 1e-6)\n",
    "            local_densities.append(local_density)\n",
    "        else:\n",
    "            local_aucs.append(np.nan)\n",
    "            local_accuracies.append(np.nan)\n",
    "            local_precisions.append(np.nan)\n",
    "            local_recalls.append(np.nan)\n",
    "            local_densities.append(np.nan)\n",
    "    \n",
    "    local_metrics['local_aucs'] = np.array(local_aucs)\n",
    "    local_metrics['local_accuracies'] = np.array(local_accuracies)\n",
    "    local_metrics['local_precisions'] = np.array(local_precisions)\n",
    "    local_metrics['local_recalls'] = np.array(local_recalls)\n",
    "    local_metrics['local_densities'] = np.array(local_densities)\n",
    "    \n",
    "    return local_metrics\n",
    "\n",
    "def analyze_performance_spatial_correlation(gdf, predictions, true_labels, weights=None, \n",
    "                                          spatial_metrics=None, autocorr_metrics=None):\n",
    "    \"\"\"\n",
    "    Analyze correlation between model performance and spatial characteristics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Geospatial data\n",
    "    predictions : array-like\n",
    "        Model predictions\n",
    "    true_labels : array-like\n",
    "        True labels\n",
    "    weights : array-like, optional\n",
    "        Sample weights\n",
    "    spatial_metrics : dict, optional\n",
    "        Spatial spread metrics\n",
    "    autocorr_metrics : dict, optional\n",
    "        Spatial autocorrelation metrics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    correlation_results : dict\n",
    "        Dictionary containing correlation analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    coords = np.array([[geom.x, geom.y] for geom in gdf.geometry])\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate local performance metrics\n",
    "    local_metrics = calculate_local_performance_metrics(gdf, predictions, true_labels, weights)\n",
    "    \n",
    "    # Calculate spatial characteristics for each point\n",
    "    spatial_chars = {}\n",
    "    \n",
    "    # Distance to centroid\n",
    "    if spatial_metrics:\n",
    "        if 'presence_centroid_lat' in spatial_metrics and 'presence_centroid_lon' in spatial_metrics:\n",
    "            centroid = np.array([spatial_metrics['presence_centroid_lon'], \n",
    "                               spatial_metrics['presence_centroid_lat']])\n",
    "            distances_to_centroid = np.sqrt(np.sum((coords - centroid)**2, axis=1))\n",
    "            spatial_chars['distance_to_centroid'] = distances_to_centroid\n",
    "    \n",
    "    # Local density\n",
    "    spatial_chars['local_density'] = local_metrics['local_densities']\n",
    "    \n",
    "    # Distance to nearest presence/absence\n",
    "    presence_coords = coords[true_labels == 1]\n",
    "    absence_coords = coords[true_labels == 0]\n",
    "    \n",
    "    if len(presence_coords) > 0:\n",
    "        distances_to_presence = []\n",
    "        for coord in coords:\n",
    "            dists = np.sqrt(np.sum((presence_coords - coord)**2, axis=1))\n",
    "            distances_to_presence.append(np.min(dists))\n",
    "        spatial_chars['distance_to_nearest_presence'] = np.array(distances_to_presence)\n",
    "    \n",
    "    if len(absence_coords) > 0:\n",
    "        distances_to_absence = []\n",
    "        for coord in coords:\n",
    "            dists = np.sqrt(np.sum((absence_coords - coord)**2, axis=1))\n",
    "            distances_to_absence.append(np.min(dists))\n",
    "        spatial_chars['distance_to_nearest_absence'] = np.array(distances_to_absence)\n",
    "    \n",
    "    # Calculate correlations\n",
    "    correlations = {}\n",
    "    \n",
    "    # Performance vs spatial characteristics\n",
    "    for perf_name, perf_values in local_metrics.items():\n",
    "        if perf_name == 'local_densities':  # Skip density as it's already a spatial characteristic\n",
    "            continue\n",
    "            \n",
    "        correlations[perf_name] = {}\n",
    "        \n",
    "        for spatial_name, spatial_values in spatial_chars.items():\n",
    "            # Remove NaN values for correlation calculation\n",
    "            valid_mask = ~(np.isnan(perf_values) | np.isnan(spatial_values))\n",
    "            \n",
    "            if np.sum(valid_mask) > 10:  # Need sufficient samples\n",
    "                try:\n",
    "                    pearson_r, pearson_p = pearsonr(perf_values[valid_mask], spatial_values[valid_mask])\n",
    "                    spearman_r, spearman_p = spearmanr(perf_values[valid_mask], spatial_values[valid_mask])\n",
    "                    \n",
    "                    correlations[perf_name][spatial_name] = {\n",
    "                        'pearson_r': pearson_r,\n",
    "                        'pearson_p': pearson_p,\n",
    "                        'spearman_r': spearman_r,\n",
    "                        'spearman_p': spearman_p,\n",
    "                        'n_samples': np.sum(valid_mask)\n",
    "                    }\n",
    "                except:\n",
    "                    correlations[perf_name][spatial_name] = {\n",
    "                        'pearson_r': np.nan,\n",
    "                        'pearson_p': np.nan,\n",
    "                        'spearman_r': np.nan,\n",
    "                        'spearman_p': np.nan,\n",
    "                        'n_samples': 0\n",
    "                    }\n",
    "    \n",
    "    results['local_metrics'] = local_metrics\n",
    "    results['spatial_characteristics'] = spatial_chars\n",
    "    results['correlations'] = correlations\n",
    "    \n",
    "    return results\n",
    "\n",
    "# # Analyze performance vs spatial spread for training data\n",
    "train_perf_spatial = analyze_performance_spatial_correlation(\n",
    "    train, y_train_predict, y_train, sample_weight_train, \n",
    "    train_spatial_metrics, train_autocorr\n",
    ")\n",
    "\n",
    "# # Analyze performance vs spatial spread for test data\n",
    "test_perf_spatial = analyze_performance_spatial_correlation(\n",
    "    test, y_test_predict, y_test, sample_weight_test, \n",
    "    test_spatial_metrics, test_autocorr\n",
    ")\n",
    "\n",
    "\n",
    "def get_significant_correlations(perf_spatial_results, alpha=0.05):\n",
    "    \"\"\"Get significant correlations.\"\"\"\n",
    "    significant_corrs = []\n",
    "    correlations = perf_spatial_results['correlations']\n",
    "    \n",
    "    for perf_metric, spatial_corrs in correlations.items():\n",
    "        for spatial_char, corr_data in spatial_corrs.items():\n",
    "            if (not np.isnan(corr_data['pearson_p']) and \n",
    "                corr_data['pearson_p'] < alpha and \n",
    "                abs(corr_data['pearson_r']) > 0.1):\n",
    "                significant_corrs.append({\n",
    "                    'performance_metric': perf_metric,\n",
    "                    'spatial_characteristic': spatial_char,\n",
    "                    'pearson_r': corr_data['pearson_r'],\n",
    "                    'pearson_p': corr_data['pearson_p'],\n",
    "                    'spearman_r': corr_data['spearman_r'],\n",
    "                    'spearman_p': corr_data['spearman_p']\n",
    "                })\n",
    "    \n",
    "    return significant_corrs\n",
    "\n",
    "train_significant = get_significant_correlations(train_perf_spatial)\n",
    "test_significant = get_significant_correlations(test_perf_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f65110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # SPATIAL BIAS ASSESSMENT AND COMPREHENSIVE ANALYSIS\n",
    "# # =============================================================================\n",
    "\n",
    "def assess_spatial_bias(gdf, predictions, true_labels, weights=None, \n",
    "                       spatial_metrics=None, autocorr_metrics=None):\n",
    "    \"\"\"\n",
    "    Comprehensive spatial bias assessment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Geospatial data\n",
    "    predictions : array-like\n",
    "        Model predictions\n",
    "    true_labels : array-like\n",
    "        True labels\n",
    "    weights : array-like, optional\n",
    "        Sample weights\n",
    "    spatial_metrics : dict, optional\n",
    "        Spatial spread metrics\n",
    "    autocorr_metrics : dict, optional\n",
    "        Spatial autocorrelation metrics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bias_assessment : dict\n",
    "        Dictionary containing spatial bias assessment results\n",
    "    \"\"\"\n",
    "    \n",
    "    coords = np.array([[geom.x, geom.y] for geom in gdf.geometry])\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Geographic bias assessment\n",
    "    geographic_bias = {}\n",
    "    \n",
    "    # Divide study area into quadrants\n",
    "    lon_center = np.mean(coords[:, 0])\n",
    "    lat_center = np.mean(coords[:, 1])\n",
    "    \n",
    "    # Create quadrant masks\n",
    "    nw_mask = (coords[:, 0] <= lon_center) & (coords[:, 1] >= lat_center)\n",
    "    ne_mask = (coords[:, 0] > lon_center) & (coords[:, 1] >= lat_center)\n",
    "    sw_mask = (coords[:, 0] <= lon_center) & (coords[:, 1] < lat_center)\n",
    "    se_mask = (coords[:, 0] > lon_center) & (coords[:, 1] < lat_center)\n",
    "    \n",
    "    quadrants = {'NW': nw_mask, 'NE': ne_mask, 'SW': sw_mask, 'SE': se_mask}\n",
    "    \n",
    "    for quad_name, quad_mask in quadrants.items():\n",
    "        if np.sum(quad_mask) > 0:\n",
    "            quad_predictions = predictions[quad_mask]\n",
    "            quad_labels = true_labels[quad_mask]\n",
    "            quad_weights = weights[quad_mask] if weights is not None else None\n",
    "            \n",
    "            # Calculate performance metrics for this quadrant\n",
    "            try:\n",
    "                quad_auc = metrics.roc_auc_score(quad_labels, quad_predictions, \n",
    "                                               sample_weight=quad_weights)\n",
    "                quad_accuracy = np.mean((quad_predictions > 0.5) == quad_labels)\n",
    "                \n",
    "                # Calculate bias metrics\n",
    "                quad_bias = np.mean(quad_predictions - quad_labels)\n",
    "                quad_mae = np.mean(np.abs(quad_predictions - quad_labels))\n",
    "                \n",
    "                geographic_bias[quad_name] = {\n",
    "                    'n_samples': np.sum(quad_mask),\n",
    "                    'auc': quad_auc,\n",
    "                    'accuracy': quad_accuracy,\n",
    "                    'bias': quad_bias,\n",
    "                    'mae': quad_mae,\n",
    "                    'mean_prediction': np.mean(quad_predictions),\n",
    "                    'presence_ratio': np.mean(quad_labels)\n",
    "                }\n",
    "            except:\n",
    "                geographic_bias[quad_name] = {\n",
    "                    'n_samples': np.sum(quad_mask),\n",
    "                    'auc': np.nan,\n",
    "                    'accuracy': np.nan,\n",
    "                    'bias': np.nan,\n",
    "                    'mae': np.nan,\n",
    "                    'mean_prediction': np.nan,\n",
    "                    'presence_ratio': np.nan\n",
    "                }\n",
    "    \n",
    "    results['geographic_bias'] = geographic_bias\n",
    "    \n",
    "    # 2. Distance-based bias assessment\n",
    "    if spatial_metrics and 'presence_centroid_lat' in spatial_metrics:\n",
    "        centroid = np.array([spatial_metrics['presence_centroid_lon'], \n",
    "                           spatial_metrics['presence_centroid_lat']])\n",
    "        distances_to_centroid = np.sqrt(np.sum((coords - centroid)**2, axis=1))\n",
    "        \n",
    "        # Divide into distance bands\n",
    "        distance_quartiles = np.percentile(distances_to_centroid, [25, 50, 75])\n",
    "        \n",
    "        distance_bias = {}\n",
    "        distance_bands = ['Close', 'Medium', 'Far', 'Very_Far']\n",
    "        distance_masks = [\n",
    "            distances_to_centroid <= distance_quartiles[0],\n",
    "            (distances_to_centroid > distance_quartiles[0]) & (distances_to_centroid <= distance_quartiles[1]),\n",
    "            (distances_to_centroid > distance_quartiles[1]) & (distances_to_centroid <= distance_quartiles[2]),\n",
    "            distances_to_centroid > distance_quartiles[2]\n",
    "        ]\n",
    "        \n",
    "        for band_name, band_mask in zip(distance_bands, distance_masks):\n",
    "            if np.sum(band_mask) > 0:\n",
    "                band_predictions = predictions[band_mask]\n",
    "                band_labels = true_labels[band_mask]\n",
    "                band_weights = weights[band_mask] if weights is not None else None\n",
    "                \n",
    "                try:\n",
    "                    band_auc = metrics.roc_auc_score(band_labels, band_predictions, \n",
    "                                                   sample_weight=band_weights)\n",
    "                    band_bias = np.mean(band_predictions - band_labels)\n",
    "                    band_mae = np.mean(np.abs(band_predictions - band_labels))\n",
    "                    \n",
    "                    distance_bias[band_name] = {\n",
    "                        'n_samples': np.sum(band_mask),\n",
    "                        'mean_distance': np.mean(distances_to_centroid[band_mask]),\n",
    "                        'auc': band_auc,\n",
    "                        'bias': band_bias,\n",
    "                        'mae': band_mae,\n",
    "                        'mean_prediction': np.mean(band_predictions),\n",
    "                        'presence_ratio': np.mean(band_labels)\n",
    "                    }\n",
    "                except:\n",
    "                    distance_bias[band_name] = {\n",
    "                        'n_samples': np.sum(band_mask),\n",
    "                        'mean_distance': np.mean(distances_to_centroid[band_mask]),\n",
    "                        'auc': np.nan,\n",
    "                        'bias': np.nan,\n",
    "                        'mae': np.nan,\n",
    "                        'mean_prediction': np.nan,\n",
    "                        'presence_ratio': np.nan\n",
    "                    }\n",
    "        \n",
    "        results['distance_bias'] = distance_bias\n",
    "    \n",
    "    # 3. Density-based bias assessment\n",
    "    if spatial_metrics and 'mean_local_density' in spatial_metrics:\n",
    "        # Use local density from spatial metrics\n",
    "        local_densities = spatial_metrics.get('local_densities', np.ones(len(coords)))\n",
    "        \n",
    "        # Divide into density bands\n",
    "        density_quartiles = np.percentile(local_densities, [25, 50, 75])\n",
    "        \n",
    "        density_bias = {}\n",
    "        density_bands = ['Low', 'Medium', 'High', 'Very_High']\n",
    "        density_masks = [\n",
    "            local_densities <= density_quartiles[0],\n",
    "            (local_densities > density_quartiles[0]) & (local_densities <= density_quartiles[1]),\n",
    "            (local_densities > density_quartiles[1]) & (local_densities <= density_quartiles[2]),\n",
    "            local_densities > density_quartiles[2]\n",
    "        ]\n",
    "        \n",
    "        for band_name, band_mask in zip(density_bands, density_masks):\n",
    "            if np.sum(band_mask) > 0:\n",
    "                band_predictions = predictions[band_mask]\n",
    "                band_labels = true_labels[band_mask]\n",
    "                band_weights = weights[band_mask] if weights is not None else None\n",
    "                \n",
    "                try:\n",
    "                    band_auc = metrics.roc_auc_score(band_labels, band_predictions, \n",
    "                                                   sample_weight=band_weights)\n",
    "                    band_bias = np.mean(band_predictions - band_labels)\n",
    "                    band_mae = np.mean(np.abs(band_predictions - band_labels))\n",
    "                    \n",
    "                    density_bias[band_name] = {\n",
    "                        'n_samples': np.sum(band_mask),\n",
    "                        'mean_density': np.mean(local_densities[band_mask]),\n",
    "                        'auc': band_auc,\n",
    "                        'bias': band_bias,\n",
    "                        'mae': band_mae,\n",
    "                        'mean_prediction': np.mean(band_predictions),\n",
    "                        'presence_ratio': np.mean(band_labels)\n",
    "                    }\n",
    "                except:\n",
    "                    density_bias[band_name] = {\n",
    "                        'n_samples': np.sum(band_mask),\n",
    "                        'mean_density': np.mean(local_densities[band_mask]),\n",
    "                        'auc': np.nan,\n",
    "                        'bias': np.nan,\n",
    "                        'mae': np.nan,\n",
    "                        'mean_prediction': np.nan,\n",
    "                        'presence_ratio': np.nan\n",
    "                    }\n",
    "        \n",
    "        results['density_bias'] = density_bias\n",
    "    \n",
    "    # 4. Overall bias summary\n",
    "    overall_bias = {\n",
    "        'mean_prediction': np.mean(predictions),\n",
    "        'mean_true': np.mean(true_labels),\n",
    "        'overall_bias': np.mean(predictions - true_labels),\n",
    "        'overall_mae': np.mean(np.abs(predictions - true_labels)),\n",
    "        'prediction_std': np.std(predictions),\n",
    "        'true_std': np.std(true_labels)\n",
    "    }\n",
    "    \n",
    "    results['overall_bias'] = overall_bias\n",
    "    \n",
    "    return results\n",
    "\n",
    "# # Assess spatial bias for training and test data\n",
    "train_bias = assess_spatial_bias(train, y_train_predict, y_train, sample_weight_train, \n",
    "                               train_spatial_metrics, train_autocorr)\n",
    "\n",
    "test_bias = assess_spatial_bias(test, y_test_predict, y_test, sample_weight_test, \n",
    "                              test_spatial_metrics, test_autocorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # EXPORT SPATIAL ANALYSIS RESULTS\n",
    "# # =============================================================================\n",
    "\n",
    "# # Create comprehensive summary of spatial analysis results\n",
    "spatial_analysis_summary = {\n",
    "    'species': specie,\n",
    "    'training_region': training,\n",
    "    'test_region': interest,\n",
    "    'bioclimatic_variable': bio,\n",
    "    'iteration': iteration,\n",
    "    'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "    \n",
    "    # Spatial metrics\n",
    "    'training_spatial_metrics': train_spatial_metrics,\n",
    "    'test_spatial_metrics': test_spatial_metrics,\n",
    "    \n",
    "    # Spatial autocorrelation\n",
    "    'training_autocorrelation': train_autocorr,\n",
    "    'test_autocorrelation': test_autocorr,\n",
    "    \n",
    "    # Performance vs spatial correlation\n",
    "    'training_performance_spatial': {\n",
    "        'significant_correlations': train_significant,\n",
    "        'local_metrics_summary': {\n",
    "            'mean_local_auc': np.nanmean(train_perf_spatial['local_metrics']['local_aucs']),\n",
    "            'std_local_auc': np.nanstd(train_perf_spatial['local_metrics']['local_aucs']),\n",
    "            'mean_local_accuracy': np.nanmean(train_perf_spatial['local_metrics']['local_accuracies']),\n",
    "            'std_local_accuracy': np.nanstd(train_perf_spatial['local_metrics']['local_accuracies'])\n",
    "        }\n",
    "    },\n",
    "    'test_performance_spatial': {\n",
    "        'significant_correlations': test_significant,\n",
    "        'local_metrics_summary': {\n",
    "            'mean_local_auc': np.nanmean(test_perf_spatial['local_metrics']['local_aucs']),\n",
    "            'std_local_auc': np.nanstd(test_perf_spatial['local_metrics']['local_aucs']),\n",
    "            'mean_local_accuracy': np.nanmean(test_perf_spatial['local_metrics']['local_accuracies']),\n",
    "            'std_local_accuracy': np.nanstd(test_perf_spatial['local_metrics']['local_accuracies'])\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Spatial bias assessment\n",
    "    'training_bias_assessment': train_bias,\n",
    "    'test_bias_assessment': test_bias,\n",
    "    \n",
    "    # Model performance comparison\n",
    "    'model_performance_comparison': {\n",
    "        'training': {\n",
    "            'auc': auc_train,\n",
    "            'auc_weighted': auc_train_weighted,\n",
    "            'pr_auc': pr_auc_train,\n",
    "            'pr_auc_weighted': pr_auc_train_weighted\n",
    "        },\n",
    "        'test': {\n",
    "            'auc': auc_test,\n",
    "            'auc_weighted': auc_test_weighted,\n",
    "            'pr_auc': pr_auc_test,\n",
    "            'pr_auc_weighted': pr_auc_test_weighted\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# # Save comprehensive results to JSON\n",
    "if savefig:\n",
    "    import json\n",
    "    \n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    def convert_numpy_types(obj):\n",
    "        \"\"\"Convert numpy types to Python types for JSON serialization.\"\"\"\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy_types(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Convert the summary for JSON serialization\n",
    "    json_summary = convert_numpy_types(spatial_analysis_summary)\n",
    "    \n",
    "    # Save to JSON file\n",
    "    json_filename = f'06_spatial_analysis_summary_{specie}_{training}_{bio}_{iteration}.json'\n",
    "    json_path = os.path.join(figs_path, json_filename)\n",
    "    \n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Spatial analysis summary saved to: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992fe59",
   "metadata": {},
   "source": [
    "## 5. Spatial Spread Analysis Across Variable Set Iterations\n",
    "\n",
    "This section analyzes how spatial spread characteristics change across different variable set iterations from the comprehensive variable importance analysis. This helps identify the optimal variable combination that maintains both model performance and spatial representativeness.\n",
    "\n",
    "### **Key Objectives**:\n",
    "- **Variable Set Comparison**: Compare spatial spread metrics across different variable combinations\n",
    "- **Spatial Performance Optimization**: Find the variable set that best balances model performance with spatial coverage\n",
    "- **Spatial Bias Minimization**: Identify variable sets that minimize spatial bias\n",
    "- **Transferability Assessment**: Evaluate how different variable sets affect spatial transferability\n",
    "\n",
    "### **Analysis Components**:\n",
    "1. **Spatial Metrics by Variable Set**: Calculate spatial spread metrics for each variable combination\n",
    "2. **Performance-Spatial Trade-offs**: Analyze the relationship between model performance and spatial characteristics\n",
    "3. **Optimal Variable Selection**: Identify the best variable set based on combined performance and spatial criteria\n",
    "4. **Spatial Transferability**: Assess how variable selection affects spatial model transferability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPATIAL SPREAD ANALYSIS ACROSS VARIABLE SET ITERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_spatial_spread_by_variable_sets(x_train, y_train, sample_weight_train, \n",
    "                                          x_test, y_test, sample_weight_test,\n",
    "                                          train_gdf, test_gdf, removal_results):\n",
    "    \"\"\"\n",
    "    Analyze spatial spread characteristics for each variable set iteration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train, y_train, sample_weight_train : training data\n",
    "    x_test, y_test, sample_weight_test : test data\n",
    "    train_gdf, test_gdf : GeoDataFrames with spatial information\n",
    "    removal_results : dict, results from iterative variable removal\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    spatial_iteration_results : dict\n",
    "        Dictionary containing spatial analysis for each variable set\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Get all variable sets from removal results\n",
    "    iterations = list(removal_results['performance_history'].keys())\n",
    "    \n",
    "    # Add initial full variable set\n",
    "    all_iterations = ['initial'] + iterations\n",
    "    \n",
    "    # print(\"Analyzing spatial spread for each variable set iteration...\")\n",
    "    # print(f\"Total iterations to analyze: {len(all_iterations)}\")\n",
    "    \n",
    "    for i, iter_key in enumerate(all_iterations):\n",
    "        # print(f\"\\nProcessing iteration: {iter_key}\")\n",
    "        \n",
    "        # Get variable set for this iteration\n",
    "        if iter_key == 'initial':\n",
    "            current_vars = list(x_train.columns)\n",
    "            n_vars = len(current_vars)\n",
    "        else:\n",
    "            # Get variables from the ranking at this iteration\n",
    "            ranking_key = iter_key\n",
    "            if ranking_key in removal_results['importance_rankings']:\n",
    "                current_vars = removal_results['importance_rankings'][ranking_key]['variables']\n",
    "                n_vars = len(current_vars)\n",
    "            else:\n",
    "                print(f\"  Skipping {iter_key} - no ranking data available\")\n",
    "                continue\n",
    "        \n",
    "        # print(f\"  Variables ({n_vars}): {current_vars}\")\n",
    "        \n",
    "        # Select variables for this iteration\n",
    "        x_train_iter = x_train[current_vars]\n",
    "        x_test_iter = x_test[current_vars]\n",
    "        \n",
    "        # Train model with current variable set\n",
    "        try:\n",
    "            model_iter = ela.MaxentModel()\n",
    "            model_iter.fit(x_train_iter, y_train, sample_weight=sample_weight_train)\n",
    "            \n",
    "            # Get predictions\n",
    "            y_train_pred = model_iter.predict(x_train_iter)\n",
    "            y_test_pred = model_iter.predict(x_test_iter)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            train_auc = metrics.roc_auc_score(y_train, y_train_pred)\n",
    "            train_auc_weighted = metrics.roc_auc_score(y_train, y_train_pred, sample_weight=sample_weight_train)\n",
    "            test_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "            test_auc_weighted = metrics.roc_auc_score(y_test, y_test_pred, sample_weight=sample_weight_test)\n",
    "            \n",
    "            # Calculate spatial metrics for training data\n",
    "            train_spatial_iter = calculate_spatial_metrics(train_gdf, 'class', 'SampleWeight')\n",
    "            test_spatial_iter = calculate_spatial_metrics(test_gdf, 'class', 'SampleWeight')\n",
    "            \n",
    "            # Calculate spatial autocorrelation\n",
    "            train_autocorr_iter = calculate_spatial_autocorrelation(train_gdf, 'class', 'SampleWeight')\n",
    "            test_autocorr_iter = calculate_spatial_autocorrelation(test_gdf, 'class', 'SampleWeight')\n",
    "            \n",
    "            # Calculate performance vs spatial correlation\n",
    "            train_perf_spatial_iter = analyze_performance_spatial_correlation(\n",
    "                train_gdf, y_train_pred, y_train, sample_weight_train, \n",
    "                train_spatial_iter, train_autocorr_iter\n",
    "            )\n",
    "            \n",
    "            test_perf_spatial_iter = analyze_performance_spatial_correlation(\n",
    "                test_gdf, y_test_pred, y_test, sample_weight_test, \n",
    "                test_spatial_iter, test_autocorr_iter\n",
    "            )\n",
    "            \n",
    "            # Calculate spatial bias\n",
    "            train_bias_iter = assess_spatial_bias(\n",
    "                train_gdf, y_train_pred, y_train, sample_weight_train, \n",
    "                train_spatial_iter, train_autocorr_iter\n",
    "            )\n",
    "            \n",
    "            test_bias_iter = assess_spatial_bias(\n",
    "                test_gdf, y_test_pred, y_test, sample_weight_test, \n",
    "                test_spatial_iter, test_autocorr_iter\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            results[iter_key] = {\n",
    "                'n_variables': n_vars,\n",
    "                'variables': current_vars,\n",
    "                'performance': {\n",
    "                    'train_auc': train_auc,\n",
    "                    'train_auc_weighted': train_auc_weighted,\n",
    "                    'test_auc': test_auc,\n",
    "                    'test_auc_weighted': test_auc_weighted\n",
    "                },\n",
    "                'spatial_metrics': {\n",
    "                    'train': train_spatial_iter,\n",
    "                    'test': test_spatial_iter\n",
    "                },\n",
    "                'autocorrelation': {\n",
    "                    'train': train_autocorr_iter,\n",
    "                    'test': test_autocorr_iter\n",
    "                },\n",
    "                'performance_spatial': {\n",
    "                    'train': train_perf_spatial_iter,\n",
    "                    'test': test_perf_spatial_iter\n",
    "                },\n",
    "                'bias_assessment': {\n",
    "                    'train': train_bias_iter,\n",
    "                    'test': test_bias_iter\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"  Completed - Train AUC: {train_auc:.3f}, Test AUC: {test_auc:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {iter_key}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run spatial spread analysis across variable iterations\n",
    "# print(\"=\"*80)\n",
    "# print(\"SPATIAL SPREAD ANALYSIS ACROSS VARIABLE SET ITERATIONS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "spatial_iteration_results = analyze_spatial_spread_by_variable_sets(\n",
    "    x_train, y_train, sample_weight_train,\n",
    "    x_test, y_test, sample_weight_test,\n",
    "    train, test, removal_results\n",
    ")\n",
    "\n",
    "# print(f\"\\nAnalysis completed for {len(spatial_iteration_results)} variable set iterations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6202b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # COMPREHENSIVE SPATIAL PERFORMANCE COMPARISON ACROSS VARIABLE SETS\n",
    "# # =============================================================================\n",
    "\n",
    "def compare_spatial_performance_across_variable_sets(spatial_iteration_results):\n",
    "    \"\"\"\n",
    "    Compare spatial performance metrics across different variable sets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spatial_iteration_results : dict\n",
    "        Results from spatial analysis across variable iterations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    comparison_results : dict\n",
    "        Comprehensive comparison of spatial performance across variable sets\n",
    "    \"\"\"\n",
    "    \n",
    "    comparison = {}\n",
    "    \n",
    "    # Extract key metrics for comparison\n",
    "    iterations = list(spatial_iteration_results.keys())\n",
    "    \n",
    "    # Performance metrics\n",
    "    n_vars = []\n",
    "    train_aucs = []\n",
    "    test_aucs = []\n",
    "    train_aucs_weighted = []\n",
    "    test_aucs_weighted = []\n",
    "    \n",
    "    # Spatial spread metrics\n",
    "    train_spatial_extents = []\n",
    "    test_spatial_extents = []\n",
    "    train_spatial_clustering = []\n",
    "    test_spatial_clustering = []\n",
    "    \n",
    "    # Spatial autocorrelation\n",
    "    train_morans_i = []\n",
    "    test_morans_i = []\n",
    "    \n",
    "    # Spatial bias metrics\n",
    "    train_overall_bias = []\n",
    "    test_overall_bias = []\n",
    "    train_bias_std = []\n",
    "    test_bias_std = []\n",
    "    \n",
    "    # Performance-spatial correlation strength\n",
    "    train_corr_strength = []\n",
    "    test_corr_strength = []\n",
    "    \n",
    "    for iter_key in iterations:\n",
    "        result = spatial_iteration_results[iter_key]\n",
    "        \n",
    "        # Basic metrics\n",
    "        n_vars.append(result['n_variables'])\n",
    "        train_aucs.append(result['performance']['train_auc'])\n",
    "        test_aucs.append(result['performance']['test_auc'])\n",
    "        train_aucs_weighted.append(result['performance']['train_auc_weighted'])\n",
    "        test_aucs_weighted.append(result['performance']['test_auc_weighted'])\n",
    "        \n",
    "        # Spatial extent metrics\n",
    "        train_spatial = result['spatial_metrics']['train']\n",
    "        test_spatial = result['spatial_metrics']['test']\n",
    "        \n",
    "        train_extent = train_spatial.get('presence_area_approx', 0) + train_spatial.get('absence_area_approx', 0)\n",
    "        test_extent = test_spatial.get('presence_area_approx', 0) + test_spatial.get('absence_area_approx', 0)\n",
    "        \n",
    "        train_spatial_extents.append(train_extent)\n",
    "        test_spatial_extents.append(test_extent)\n",
    "        \n",
    "        # Spatial clustering (mean distance between points)\n",
    "        train_clustering = train_spatial.get('presence_mean_distance', 0)\n",
    "        test_clustering = test_spatial.get('presence_mean_distance', 0)\n",
    "        \n",
    "        train_spatial_clustering.append(train_clustering)\n",
    "        test_spatial_clustering.append(test_clustering)\n",
    "        \n",
    "        # Moran's I\n",
    "        train_morans_i.append(result['autocorrelation']['train'].get('morans_i', 0))\n",
    "        test_morans_i.append(result['autocorrelation']['test'].get('morans_i', 0))\n",
    "        \n",
    "        # Spatial bias\n",
    "        train_bias = result['bias_assessment']['train']['overall_bias']\n",
    "        test_bias = result['bias_assessment']['test']['overall_bias']\n",
    "        \n",
    "        train_overall_bias.append(train_bias['overall_bias'])\n",
    "        test_overall_bias.append(test_bias['overall_bias'])\n",
    "        \n",
    "        # Calculate bias standard deviation across quadrants\n",
    "        train_geo_bias = result['bias_assessment']['train'].get('geographic_bias', {})\n",
    "        test_geo_bias = result['bias_assessment']['test'].get('geographic_bias', {})\n",
    "        \n",
    "        train_bias_values = [metrics['bias'] for metrics in train_geo_bias.values() \n",
    "                           if not np.isnan(metrics['bias'])]\n",
    "        test_bias_values = [metrics['bias'] for metrics in test_geo_bias.values() \n",
    "                          if not np.isnan(metrics['bias'])]\n",
    "        \n",
    "        train_bias_std.append(np.std(train_bias_values) if train_bias_values else 0)\n",
    "        test_bias_std.append(np.std(test_bias_values) if test_bias_values else 0)\n",
    "        \n",
    "        # Performance-spatial correlation strength\n",
    "        train_perf_spatial = result['performance_spatial']['train']\n",
    "        test_perf_spatial = result['performance_spatial']['test']\n",
    "        \n",
    "        # Calculate average absolute correlation strength\n",
    "        train_corrs = []\n",
    "        test_corrs = []\n",
    "        \n",
    "        for perf_metric, spatial_corrs in train_perf_spatial['correlations'].items():\n",
    "            for spatial_char, corr_data in spatial_corrs.items():\n",
    "                if not np.isnan(corr_data['pearson_r']):\n",
    "                    train_corrs.append(abs(corr_data['pearson_r']))\n",
    "        \n",
    "        for perf_metric, spatial_corrs in test_perf_spatial['correlations'].items():\n",
    "            for spatial_char, corr_data in spatial_corrs.items():\n",
    "                if not np.isnan(corr_data['pearson_r']):\n",
    "                    test_corrs.append(abs(corr_data['pearson_r']))\n",
    "        \n",
    "        train_corr_strength.append(np.mean(train_corrs) if train_corrs else 0)\n",
    "        test_corr_strength.append(np.mean(test_corrs) if test_corrs else 0)\n",
    "    \n",
    "    # Store comparison results\n",
    "    comparison = {\n",
    "        'iterations': iterations,\n",
    "        'n_variables': n_vars,\n",
    "        'performance': {\n",
    "            'train_auc': train_aucs,\n",
    "            'test_auc': test_aucs,\n",
    "            'train_auc_weighted': train_aucs_weighted,\n",
    "            'test_auc_weighted': test_aucs_weighted\n",
    "        },\n",
    "        'spatial_extent': {\n",
    "            'train': train_spatial_extents,\n",
    "            'test': test_spatial_extents\n",
    "        },\n",
    "        'spatial_clustering': {\n",
    "            'train': train_spatial_clustering,\n",
    "            'test': test_spatial_clustering\n",
    "        },\n",
    "        'spatial_autocorrelation': {\n",
    "            'train_morans_i': train_morans_i,\n",
    "            'test_morans_i': test_morans_i\n",
    "        },\n",
    "        'spatial_bias': {\n",
    "            'train_overall_bias': train_overall_bias,\n",
    "            'test_overall_bias': test_overall_bias,\n",
    "            'train_bias_std': train_bias_std,\n",
    "            'test_bias_std': test_bias_std\n",
    "        },\n",
    "        'performance_spatial_correlation': {\n",
    "            'train_corr_strength': train_corr_strength,\n",
    "            'test_corr_strength': test_corr_strength\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# # Compare spatial performance across variable sets\n",
    "spatial_comparison = compare_spatial_performance_across_variable_sets(spatial_iteration_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # OPTIMAL VARIABLE SET IDENTIFICATION BASED ON SPATIAL SPREAD CRITERIA\n",
    "# # =============================================================================\n",
    "\n",
    "def identify_optimal_variable_set(spatial_comparison, spatial_iteration_results, \n",
    "                                performance_weight=0.4, spatial_weight=0.3, bias_weight=0.3):\n",
    "    \"\"\"\n",
    "    Identify the optimal variable set based on combined performance and spatial criteria.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spatial_comparison : dict\n",
    "        Comparison results across variable sets\n",
    "    spatial_iteration_results : dict\n",
    "        Detailed results for each variable set\n",
    "    performance_weight : float\n",
    "        Weight for performance criteria (default: 0.4)\n",
    "    spatial_weight : float\n",
    "        Weight for spatial criteria (default: 0.3)\n",
    "    bias_weight : float\n",
    "        Weight for bias criteria (default: 0.3)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    optimal_results : dict\n",
    "        Results identifying the optimal variable set\n",
    "    \"\"\"\n",
    "    \n",
    "    iterations = spatial_comparison['iterations']\n",
    "    n_iterations = len(iterations)\n",
    "    \n",
    "    # Normalize metrics to 0-1 scale for comparison\n",
    "    def normalize_metric(values, reverse=False):\n",
    "        \"\"\"Normalize values to 0-1 scale.\"\"\"\n",
    "        min_val = min(values)\n",
    "        max_val = max(values)\n",
    "        if max_val == min_val:\n",
    "            return [0.5] * len(values)\n",
    "        \n",
    "        normalized = [(v - min_val) / (max_val - min_val) for v in values]\n",
    "        if reverse:\n",
    "            normalized = [1 - v for v in normalized]\n",
    "        return normalized\n",
    "    \n",
    "    # Performance score (higher is better)\n",
    "    test_auc_norm = normalize_metric(spatial_comparison['performance']['test_auc'])\n",
    "    \n",
    "    # Spatial score (higher spatial extent and moderate clustering is better)\n",
    "    spatial_extent_norm = normalize_metric(spatial_comparison['spatial_extent']['test'])\n",
    "    \n",
    "    # Spatial clustering score (moderate clustering is better than extreme clustering)\n",
    "    spatial_clustering = spatial_comparison['spatial_clustering']['test']\n",
    "    clustering_optimal = np.median(spatial_clustering)  # Use median as optimal\n",
    "    clustering_scores = [1 - abs(c - clustering_optimal) / max(spatial_clustering) for c in spatial_clustering]\n",
    "    \n",
    "    # Spatial autocorrelation score (moderate autocorrelation is better)\n",
    "    morans_i_values = spatial_comparison['spatial_autocorrelation']['test_morans_i']\n",
    "    mi_optimal = 0.1  # Moderate positive autocorrelation is ideal\n",
    "    mi_scores = [1 - abs(mi - mi_optimal) / max([abs(mi) for mi in morans_i_values] + [0.1]) for mi in morans_i_values]\n",
    "    \n",
    "    # Bias score (lower absolute bias is better)\n",
    "    bias_scores = normalize_metric([abs(b) for b in spatial_comparison['spatial_bias']['test_overall_bias']], reverse=True)\n",
    "    \n",
    "    # Bias consistency score (lower standard deviation is better)\n",
    "    bias_std_scores = normalize_metric(spatial_comparison['spatial_bias']['test_bias_std'], reverse=True)\n",
    "    \n",
    "    # Calculate composite scores\n",
    "    composite_scores = []\n",
    "    detailed_scores = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Performance component\n",
    "        perf_score = test_auc_norm[i]\n",
    "        \n",
    "        # Spatial component (average of extent, clustering, and autocorrelation)\n",
    "        spatial_score = (spatial_extent_norm[i] + clustering_scores[i] + mi_scores[i]) / 3\n",
    "        \n",
    "        # Bias component (average of overall bias and consistency)\n",
    "        bias_score = (bias_scores[i] + bias_std_scores[i]) / 2\n",
    "        \n",
    "        # Composite score\n",
    "        composite_score = (performance_weight * perf_score + \n",
    "                          spatial_weight * spatial_score + \n",
    "                          bias_weight * bias_score)\n",
    "        \n",
    "        composite_scores.append(composite_score)\n",
    "        detailed_scores.append({\n",
    "            'iteration': iterations[i],\n",
    "            'n_variables': spatial_comparison['n_variables'][i],\n",
    "            'performance_score': perf_score,\n",
    "            'spatial_score': spatial_score,\n",
    "            'bias_score': bias_score,\n",
    "            'composite_score': composite_score,\n",
    "            'test_auc': spatial_comparison['performance']['test_auc'][i],\n",
    "            'test_bias': spatial_comparison['spatial_bias']['test_overall_bias'][i],\n",
    "            'spatial_extent': spatial_comparison['spatial_extent']['test'][i],\n",
    "            'morans_i': spatial_comparison['spatial_autocorrelation']['test_morans_i'][i]\n",
    "        })\n",
    "    \n",
    "    # Find optimal variable set\n",
    "    optimal_idx = np.argmax(composite_scores)\n",
    "    optimal_iteration = iterations[optimal_idx]\n",
    "    \n",
    "    # Get top 3 variable sets\n",
    "    sorted_indices = np.argsort(composite_scores)[::-1]\n",
    "    top_3_indices = sorted_indices[:3]\n",
    "    \n",
    "    optimal_results = {\n",
    "        'optimal_iteration': optimal_iteration,\n",
    "        'optimal_idx': optimal_idx,\n",
    "        'optimal_score': composite_scores[optimal_idx],\n",
    "        'optimal_variables': spatial_iteration_results[optimal_iteration]['variables'],\n",
    "        'optimal_n_variables': spatial_comparison['n_variables'][optimal_idx],\n",
    "        'detailed_scores': detailed_scores,\n",
    "        'top_3_iterations': [iterations[i] for i in top_3_indices],\n",
    "        'top_3_scores': [composite_scores[i] for i in top_3_indices],\n",
    "        'criteria_weights': {\n",
    "            'performance_weight': performance_weight,\n",
    "            'spatial_weight': spatial_weight,\n",
    "            'bias_weight': bias_weight\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return optimal_results\n",
    "\n",
    "# # Identify optimal variable set\n",
    "optimal_results = identify_optimal_variable_set(spatial_comparison, spatial_iteration_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26eff7",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Summary: Variable Importance vs Spatial Spread Analysis\n",
    "\n",
    "This section provides a comprehensive comparison between the **Comprehensive Variable Importance Analysis** and the **Spatial Spread Analysis Across Variable Set Iterations**, synthesizing key findings and providing integrated insights for optimal variable selection.\n",
    "\n",
    "### **Analysis Integration Overview**:\n",
    "\n",
    "#### **Variable Importance Analysis **:\n",
    "- **Iterative Variable Removal**: Systematic removal of least important variables\n",
    "- **Performance Tracking**: Monitoring AUC and PR-AUC as variables are removed\n",
    "- **Permutation Importance**: Statistical importance ranking of variables\n",
    "- **Final Variable Set**: Identification of top 5 most important variables\n",
    "\n",
    "#### **Spatial Spread Analysis **:\n",
    "- **Spatial Metrics**: Geographic extent, clustering, and autocorrelation analysis\n",
    "- **Spatial Bias Assessment**: Geographic and distance-based bias evaluation\n",
    "- **Performance-Spatial Correlation**: Relationship between model performance and spatial characteristics\n",
    "- **Multi-criteria Optimization**: Combined performance, spatial, and bias criteria\n",
    "\n",
    "### **Key Integration Points**:\n",
    "1. **Variable Selection Strategy**: How variable importance relates to spatial performance\n",
    "2. **Spatial Transferability**: Impact of variable selection on spatial model transferability\n",
    "3. **Bias Minimization**: Role of variable selection in reducing spatial bias\n",
    "4. **Optimal Balance**: Finding the best compromise between performance and spatial coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # COMPREHENSIVE SUMMARY: VARIABLE IMPORTANCE vs SPATIAL SPREAD ANALYSIS\n",
    "# # =============================================================================\n",
    "\n",
    "def create_comprehensive_analysis_summary(removal_results, spatial_comparison, optimal_results, \n",
    "                                        spatial_iteration_results):\n",
    "    \"\"\"\n",
    "    Create comprehensive summary comparing Variable Importance Analysis with Spatial Spread Analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    removal_results : dict\n",
    "        Results from iterative variable removal\n",
    "    spatial_comparison : dict\n",
    "        Spatial comparison results across variable sets\n",
    "    optimal_results : dict\n",
    "        Optimal variable set identification results\n",
    "    spatial_iteration_results : dict\n",
    "        Detailed spatial results for each variable set\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    summary_results : dict\n",
    "        Comprehensive analysis summary\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"ðŸ”¬ COMPREHENSIVE ANALYSIS SUMMARY: VARIABLE IMPORTANCE vs SPATIAL SPREAD\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # 1. VARIABLE IMPORTANCE ANALYSIS SUMMARY\n",
    "    print(f\"\\nðŸ“Š VARIABLE IMPORTANCE ANALYSIS SUMMARY:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get final variable set from importance analysis\n",
    "    final_vars = removal_results.get('final_variables', [])\n",
    "    removed_vars = removal_results.get('removed_variables', [])\n",
    "    \n",
    "    initial_n_vars = removal_results.get('performance_history', {}).get('iteration_1', {}).get('n_variables', 0)\n",
    "    print(f\"â€¢ Initial Variables: {initial_n_vars}\")\n",
    "    print(f\"â€¢ Final Variables: {len(final_vars)}\")\n",
    "    print(f\"â€¢ Variables Removed: {len(removed_vars)}\")\n",
    "    print(f\"â€¢ Final Variable Set: {final_vars}\")\n",
    "    \n",
    "    # Performance degradation from importance analysis\n",
    "    initial_perf = removal_results.get('performance_history', {}).get('iteration_1', {})\n",
    "    final_perf = removal_results.get('performance_history', {}).get(f'iteration_{len(removed_vars)}', {})\n",
    "    \n",
    "    if initial_perf and final_perf:\n",
    "        initial_auc = initial_perf.get('test_auc', 0)\n",
    "        final_auc = final_perf.get('test_auc', 0)\n",
    "        perf_drop = ((initial_auc - final_auc) / initial_auc * 100) if initial_auc > 0 else 0\n",
    "        print(f\"â€¢ Performance Drop: {perf_drop:.1f}%\")\n",
    "    \n",
    "    # 2. SPATIAL SPREAD ANALYSIS SUMMARY\n",
    "    print(f\"\\nðŸŒ SPATIAL SPREAD ANALYSIS SUMMARY:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    optimal_iteration = optimal_results['optimal_iteration']\n",
    "    optimal_vars = optimal_results['optimal_variables']\n",
    "    optimal_score = optimal_results['optimal_score']\n",
    "    \n",
    "    print(f\"â€¢ Optimal Variable Set: {optimal_iteration}\")\n",
    "    print(f\"â€¢ Optimal Variables: {len(optimal_vars)}\")\n",
    "    print(f\"â€¢ Composite Score: {optimal_score:.4f}\")\n",
    "    print(f\"â€¢ Optimal Variables: {optimal_vars}\")\n",
    "    \n",
    "    # Spatial characteristics of optimal set\n",
    "    if optimal_iteration in spatial_iteration_results:\n",
    "        optimal_result = spatial_iteration_results[optimal_iteration]\n",
    "        spatial_metrics = optimal_result['spatial_metrics']['test']\n",
    "        autocorr_metrics = optimal_result['autocorrelation']['test']\n",
    "        bias_metrics = optimal_result['bias_assessment']['test']\n",
    "        \n",
    "        print(f\"â€¢ Spatial Extent: {spatial_metrics.get('presence_area_approx', 0):.4f}\")\n",
    "        print(f\"â€¢ Moran's I: {autocorr_metrics.get('morans_i', 0):.3f}\")\n",
    "        print(f\"â€¢ Spatial Bias: {bias_metrics['overall_bias']['overall_bias']:.4f}\")\n",
    "    \n",
    "    # 3. COMPARISON AND INTEGRATION\n",
    "    print(f\"\\nðŸ”„ ANALYSIS COMPARISON & INTEGRATION:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Compare variable sets\n",
    "    importance_vars = set(final_vars)\n",
    "    spatial_vars = set(optimal_vars)\n",
    "    \n",
    "    common_vars = importance_vars.intersection(spatial_vars)\n",
    "    importance_only = importance_vars - spatial_vars\n",
    "    spatial_only = spatial_vars - importance_vars\n",
    "    \n",
    "    print(f\"â€¢ Common Variables: {len(common_vars)} ({list(common_vars)})\")\n",
    "    print(f\"â€¢ Importance-Only Variables: {len(importance_only)} ({list(importance_only)})\")\n",
    "    print(f\"â€¢ Spatial-Only Variables: {len(spatial_only)} ({list(spatial_only)})\")\n",
    "    print(f\"â€¢ Variable Set Overlap: {len(common_vars)/max(len(importance_vars), len(spatial_vars))*100:.1f}%\")\n",
    "    \n",
    "    # 4. PERFORMANCE COMPARISON\n",
    "    print(f\"\\nðŸ“ˆ PERFORMANCE COMPARISON:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get performance metrics for both approaches\n",
    "    if optimal_iteration in spatial_iteration_results:\n",
    "        optimal_perf = spatial_iteration_results[optimal_iteration]['performance']\n",
    "        print(f\"â€¢ Importance Analysis Final AUC: {final_perf.get('test_auc', 0):.3f}\")\n",
    "        print(f\"â€¢ Spatial Analysis Optimal AUC: {optimal_perf['test_auc']:.3f}\")\n",
    "        print(f\"â€¢ Performance Difference: {abs(final_perf.get('test_auc', 0) - optimal_perf['test_auc']):.3f}\")\n",
    "    \n",
    "    # 5. SPATIAL CHARACTERISTICS COMPARISON\n",
    "    print(f\"\\nðŸ—ºï¸ SPATIAL CHARACTERISTICS COMPARISON:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Compare spatial characteristics between approaches\n",
    "    if optimal_iteration in spatial_iteration_results:\n",
    "        optimal_spatial = spatial_iteration_results[optimal_iteration]\n",
    "        \n",
    "        # Get spatial metrics for importance analysis final set\n",
    "        importance_iteration = f\"iteration_{len(removed_vars)}\"\n",
    "        if importance_iteration in spatial_iteration_results:\n",
    "            importance_spatial = spatial_iteration_results[importance_iteration]\n",
    "            \n",
    "            print(f\"â€¢ Importance Analysis Spatial Bias: {importance_spatial['bias_assessment']['test']['overall_bias']['overall_bias']:.4f}\")\n",
    "            print(f\"â€¢ Spatial Analysis Spatial Bias: {optimal_spatial['bias_assessment']['test']['overall_bias']['overall_bias']:.4f}\")\n",
    "            \n",
    "            print(f\"â€¢ Importance Analysis Moran's I: {importance_spatial['autocorrelation']['test'].get('morans_i', 0):.3f}\")\n",
    "            print(f\"â€¢ Spatial Analysis Moran's I: {optimal_spatial['autocorrelation']['test'].get('morans_i', 0):.3f}\")\n",
    "    \n",
    "    # 6. KEY INSIGHTS\n",
    "    print(f\"\\nðŸ’¡ KEY INSIGHTS:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # Variable overlap insight\n",
    "    if len(common_vars) > 0:\n",
    "        insights.append(f\"â€¢ High variable overlap ({len(common_vars)} variables) suggests consistent importance across both approaches\")\n",
    "    else:\n",
    "        insights.append(\"â€¢ Low variable overlap suggests different optimization criteria\")\n",
    "    \n",
    "    # Performance insight\n",
    "    if optimal_iteration in spatial_iteration_results:\n",
    "        optimal_perf = spatial_iteration_results[optimal_iteration]['performance']\n",
    "        if abs(final_perf.get('test_auc', 0) - optimal_perf['test_auc']) < 0.05:\n",
    "            insights.append(\"â€¢ Similar performance suggests both approaches are valid\")\n",
    "        else:\n",
    "            insights.append(\"â€¢ Performance differences highlight trade-offs between approaches\")\n",
    "    \n",
    "    # Spatial insight\n",
    "    if len(spatial_only) > 0:\n",
    "        insights.append(f\"â€¢ Spatial analysis includes {len(spatial_only)} additional variables for spatial optimization\")\n",
    "    \n",
    "    # Bias insight\n",
    "    if optimal_iteration in spatial_iteration_results:\n",
    "        optimal_bias = spatial_iteration_results[optimal_iteration]['bias_assessment']['test']['overall_bias']['overall_bias']\n",
    "        if abs(optimal_bias) < 0.1:\n",
    "            insights.append(\"â€¢ Spatial analysis successfully minimizes spatial bias\")\n",
    "        else:\n",
    "            insights.append(\"â€¢ Spatial bias remains a challenge requiring further optimization\")\n",
    "    \n",
    "    for insight in insights:\n",
    "        print(insight)\n",
    "    \n",
    "    # 7. RECOMMENDATIONS\n",
    "    print(f\"\\nðŸŽ¯ INTEGRATED RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Primary recommendation\n",
    "    if len(common_vars) >= 3:\n",
    "        recommendations.append(f\"1. USE HYBRID APPROACH: Combine {len(common_vars)} common variables with spatial optimization\")\n",
    "        recommendations.append(f\"   â€¢ Core Variables: {list(common_vars)}\")\n",
    "        if spatial_only:\n",
    "            recommendations.append(f\"   â€¢ Add Spatial Variables: {list(spatial_only)}\")\n",
    "    else:\n",
    "        recommendations.append(\"1. EVALUATE CONTEXT: Choose approach based on primary objective\")\n",
    "        recommendations.append(\"   â€¢ For Performance: Use Variable Importance Analysis results\")\n",
    "        recommendations.append(\"   â€¢ For Spatial Transferability: Use Spatial Spread Analysis results\")\n",
    "    \n",
    "    # Secondary recommendations\n",
    "    recommendations.append(\"2. VALIDATION STRATEGY:\")\n",
    "    recommendations.append(\"   â€¢ Test both variable sets on independent data\")\n",
    "    recommendations.append(\"   â€¢ Assess spatial transferability across different regions\")\n",
    "    recommendations.append(\"   â€¢ Monitor temporal stability of variable importance\")\n",
    "    \n",
    "    recommendations.append(\"3. IMPLEMENTATION CONSIDERATIONS:\")\n",
    "    recommendations.append(\"   â€¢ Document variable selection rationale\")\n",
    "    recommendations.append(\"   â€¢ Consider ecological significance of selected variables\")\n",
    "    recommendations.append(\"   â€¢ Plan for model updates as new data becomes available\")\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    # 8. CREATE SUMMARY DICTIONARY\n",
    "    summary_results = {\n",
    "        'variable_importance_summary': {\n",
    "            'initial_variables': initial_n_vars,\n",
    "            'final_variables': len(final_vars),\n",
    "            'removed_variables': len(removed_vars),\n",
    "            'final_variable_set': final_vars,\n",
    "            'performance_drop': perf_drop if 'perf_drop' in locals() else 0\n",
    "        },\n",
    "        'spatial_spread_summary': {\n",
    "            'optimal_iteration': optimal_iteration,\n",
    "            'optimal_variables': optimal_vars,\n",
    "            'composite_score': optimal_score,\n",
    "            'spatial_characteristics': {\n",
    "                'spatial_extent': spatial_metrics.get('presence_area_approx', 0) if 'spatial_metrics' in locals() else 0,\n",
    "                'morans_i': autocorr_metrics.get('morans_i', 0) if 'autocorr_metrics' in locals() else 0,\n",
    "                'spatial_bias': bias_metrics['overall_bias']['overall_bias'] if 'bias_metrics' in locals() else 0\n",
    "            }\n",
    "        },\n",
    "        'comparison_analysis': {\n",
    "            'common_variables': list(common_vars),\n",
    "            'importance_only_variables': list(importance_only),\n",
    "            'spatial_only_variables': list(spatial_only),\n",
    "            'variable_overlap_percentage': len(common_vars)/max(len(importance_vars), len(spatial_vars))*100,\n",
    "            'performance_difference': abs(final_perf.get('test_auc', 0) - optimal_perf['test_auc']) if 'optimal_perf' in locals() else 0\n",
    "        },\n",
    "        'key_insights': insights,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "    \n",
    "    return summary_results\n",
    "\n",
    "# # Create comprehensive analysis summary\n",
    "# print(\"Creating comprehensive analysis summary...\")\n",
    "comprehensive_summary = create_comprehensive_analysis_summary(\n",
    "    removal_results, spatial_comparison, optimal_results, spatial_iteration_results\n",
    ")\n",
    "\n",
    "# Save comprehensive summary to JSON\n",
    "if savefig:\n",
    "    import json\n",
    "    \n",
    "    # Convert the summary for JSON serialization\n",
    "    json_comprehensive_summary = convert_numpy_types(comprehensive_summary)\n",
    "    \n",
    "    # Save to JSON file\n",
    "    json_filename = f'06_comprehensive_analysis_summary_{specie}_{training}_{bio}_{iteration}.json'\n",
    "    json_path = os.path.join(figs_path, json_filename)\n",
    "    \n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_comprehensive_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nComprehensive analysis summary saved to: {json_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aciar",
   "language": "python",
   "name": "aciar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
