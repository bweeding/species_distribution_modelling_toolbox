{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3837f719-d44e-4043-9821-1571b5d160b7",
   "metadata": {},
   "source": [
    "# Occurrence data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f625b-862e-406e-8b5f-663f480d7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION SECTION - MODIFY THESE SETTINGS AS NEEDED\n",
    "# =============================================================================\n",
    "\n",
    "# Species selection for analysis\n",
    "# Change this variable to select your target species for analysis\n",
    "\n",
    "#specie = 'leptocybe-invasa' # 'leptocybe-invasa' # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ab52c-29cb-4015-a971-79ef2bd11938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9923859c-4246-4127-a8b8-05dc507d06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SET UP FILE PATHS\n",
    "# =============================================================================\n",
    "# Define the main data directory path (one level up from current working directory)\n",
    "\n",
    "data_path = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "specie_path = os.path.join(data_path, 'species', specie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa67a4-f920-41a8-800e-f0c6ee1df760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "# This section handles the specific data sources\n",
    "# Each source has different column names and formats that need standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c0167-b8d1-4e65-a406-166b81164093",
   "metadata": {},
   "outputs": [],
   "source": [
    "if specie == 'leptocybe-invasa':\n",
    "\n",
    "    file_name_gbif = 'Gbif_L-invasa_0120814-230530130749713.csv'\n",
    "    file_name_cabi = 'Cabi_2017_L-invasa-108923.csv'\n",
    "    file_name_eppo = 'Eppo_2010_L-invasa.csv' # presence only on state level, no coordinates\n",
    "    file_name_otieno = 'Otieno_2019_L-invasa.csv'\n",
    "    file_name_peng = 'Peng_2021_L-invasa.csv'\n",
    "    file_name_mcam = 'Leptocybe Overseas collections SEP 2017 from Madeline_cambodia.csv'\n",
    "    file_name_mlaos = 'Leptocybe Overseas collections SEP 2017 from Madeline_Laos.csv'\n",
    "    file_name_mthai = 'Leptocybe Overseas collections SEP 2017 from Madeline_Thailand.csv'\n",
    "    file_name_sea = 'SE_Asia_Research_L-Invasa.csv'\n",
    "    \n",
    "    gbif = pd.read_csv(os.path.join(specie_path, file_name_gbif), sep='\\t', usecols=['decimalLatitude', 'decimalLongitude', 'eventDate'])#\tday\tmonth\tyear])\n",
    "    cabi = pd.read_csv(os.path.join(specie_path, file_name_cabi), skiprows=2, usecols=['Latitude', 'Longitude', 'First Reported', 'References'], na_values='Not recorded')\n",
    "    eppo = pd.read_csv(os.path.join(data_path, 'species', specie, file_name_eppo), usecols=['country', 'state', 'country code', 'state code', 'Status', 'continent'])\n",
    "    otieno = pd.read_csv(os.path.join(specie_path, file_name_otieno), usecols=['Lat', 'Lon'])\n",
    "    peng = pd.read_csv(os.path.join(specie_path, file_name_peng), usecols=['Lat', 'Lon', 'Date', 'Elevation (m)'])\n",
    "    mcam = pd.read_csv(os.path.join(specie_path, file_name_mcam),usecols=['Date','Lat.','Long'])\n",
    "    mlaos = pd.read_csv(os.path.join(specie_path, file_name_mlaos),usecols=['Date','Lat.','Long'])\n",
    "    mthai = pd.read_csv(os.path.join(specie_path, file_name_mthai),usecols=['Date','Lat.','Long'])\n",
    "    sea = pd.read_csv(os.path.join(specie_path, file_name_sea),usecols=['Lat ','Long'])\n",
    "\n",
    "    # =============================================================================\n",
    "    # DATA CLEANING - REMOVE MISSING VALUES\n",
    "    # =============================================================================\n",
    "    # Remove rows with missing values \n",
    "    \n",
    "    mcam = mcam.dropna()\n",
    "    mlaos = mlaos.dropna()\n",
    "    mthai = mthai.dropna()\n",
    "\n",
    "    # =============================================================================\n",
    "    # STANDARDIZE COLUMN NAMES AND ADD SOURCE INFORMATION\n",
    "    # =============================================================================\n",
    "    # Rename columns to standardized format and add source attribution for tracking\n",
    "    \n",
    "    gbif_renamed = gbif.rename(columns={'decimalLatitude':'lat', 'decimalLongitude': 'lon', 'eventDate': 'date'})\n",
    "    gbif_renamed['source'] = 'GBIF'\n",
    "    cabi_renamed = cabi.rename(columns={'Latitude':'lat', 'Longitude': 'lon', 'First Reported': 'date'})\n",
    "    cabi_renamed['source'] = 'CABI'\n",
    "    otieno_renamed = otieno.rename(columns={'Lat':'lon', 'Lon': 'lat'})\n",
    "    otieno_renamed['source'] = 'Otieno et al. 2019'\n",
    "    peng_renamed = peng.rename(columns={'Lat':'lat', 'Lon': 'lon', 'Date': 'date'})\n",
    "    peng_renamed['source'] = 'Peng et al. 2021'\n",
    "    mcam_renamed = mcam.rename(columns={'Lat.':'lat','Long':'lon','Date':'date'})\n",
    "    mcam_renamed['source'] = 'Madeline overseas collections - Cambodia'\n",
    "    mlaos_renamed = mlaos.rename(columns={'Lat.':'lat','Long':'lon','Date':'date'})\n",
    "    mlaos_renamed['source'] = 'Madeline overseas collections - Laos'\n",
    "    mthai_renamed = mthai.rename(columns={'Lat.':'lat','Long':'lon','Date':'date'})\n",
    "    mthai_renamed['source'] = 'Madeline overseas collections - Thailand'\n",
    "    sea_renamed = sea.rename(columns={'Lat ':'lat', 'Long': 'lon'})\n",
    "    sea_renamed['source'] = 'SE Asia'\n",
    "\n",
    "    # =============================================================================\n",
    "    # COMBINE ALL DATA SOURCES INTO SINGLE DATASET\n",
    "    # =============================================================================\n",
    "    # Concatenate all standardized datasets into one comprehensive occurrence dataset\n",
    "    # This creates a unified dataset for downstream species distribution modeling\n",
    "    \n",
    "    occurences_global = pd.concat([gbif_renamed, cabi_renamed, otieno_renamed, peng_renamed, mcam_renamed, mlaos_renamed, mthai_renamed,sea_renamed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122968b-3ee9-43f9-87fb-c3a0437564d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if specie == 'thaumastocoris-peregrinus':\n",
    "    \n",
    "    file_name_montemayor = 'Montemayor_2015_Thaumastocoris_peregrinus_bronze_bug_transcribed_from_SUPP2.csv'\n",
    "    file_name_cabi = 'CABI_T-peregrinus.csv'\n",
    "    file_name_gbif = 'GBIF_T-peregrinus_0026124-240321170329656.csv'\n",
    "\n",
    "    montemayor = pd.read_csv(os.path.join(specie_path, file_name_montemayor), usecols=['LATITUDE', 'LONGITUDE', 'REFERENCE'], encoding='latin1')\n",
    "    cabi = pd.read_csv(os.path.join(specie_path, file_name_cabi), usecols=['Latitude', 'Longitude', 'References'], skiprows=2)\n",
    "    gbif = pd.read_csv(os.path.join(specie_path, file_name_gbif), sep='\\t', usecols=['decimalLatitude', 'decimalLongitude', 'eventDate'])\n",
    "\n",
    "    # =============================================================================\n",
    "    # STANDARDIZE COLUMN NAMES AND ADD SOURCE INFORMATION\n",
    "    # =============================================================================\n",
    "    # Rename columns to standardized format and add source attribution for tracking\n",
    "    \n",
    "    montemayor_renamed = montemayor.rename(columns={'LATITUDE':'lat', 'LONGITUDE': 'lon'})\n",
    "    montemayor_renamed['source'] = 'Montemayor et al. 2015'\n",
    "    cabi_renamed = cabi.rename(columns={\"Latitude\": \"lat\", \"Longitude\": \"lon\"})\n",
    "    cabi_renamed['source'] = 'CABI'\n",
    "    gbif_renamed = gbif.rename(columns={'decimalLatitude':'lat', 'decimalLongitude': 'lon', 'eventDate': 'date'})\n",
    "    gbif_renamed['source'] = 'GBIF'\n",
    "\n",
    "    # =============================================================================\n",
    "    # COMBINE ALL DATA SOURCES INTO SINGLE DATASET\n",
    "    # =============================================================================\n",
    "    # Concatenate all standardized datasets into one comprehensive occurrence dataset\n",
    "    # This creates a unified dataset for downstream species distribution modeling\n",
    "    \n",
    "    occurences_global = pd.concat([montemayor_renamed, cabi_renamed, gbif_renamed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef8826-da74-4264-9de5-0e973300469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA EXPLORATION AND EXPORT\n",
    "# =============================================================================\n",
    "# Display the first few rows of the aggregated dataset to verify structure and content\n",
    "\n",
    "print(occurences_global.head())\n",
    "print('Number of occurences globally is: %s' %len(occurences_global)) # Display the total number of occurrence records collected from all sources\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE AGGREGATED DATA TO FILE\n",
    "# =============================================================================\n",
    "# Export the combined dataset to a CSV file for use in subsequent analyses\n",
    "# File will be saved in the species-specific directory with standardized naming convention\n",
    "# This file can be used as input for species distribution modeling workflows\n",
    "\n",
    "occurences_global.to_csv(os.path.join(specie_path, '%s_aggregated.csv' %specie)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aciar",
   "language": "python",
   "name": "aciar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
