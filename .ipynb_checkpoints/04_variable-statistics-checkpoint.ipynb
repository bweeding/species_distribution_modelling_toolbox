{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b15204-7112-48a4-97c4-bc90bc40550d",
   "metadata": {},
   "source": [
    "# Environmental Variable Statistics and Experimental Design\n",
    "\n",
    "This notebook performs statistical analysis of environmental variables and designs experiments for species distribution modeling. It provides crucial insights into variable relationships, distributions, and helps optimize model configuration.\n",
    "\n",
    "## Key Analyses:\n",
    "\n",
    "### 1. **Variable Statistics**:\n",
    "- Descriptive statistics for all environmental variables\n",
    "- Distribution analysis (mean, median, standard deviation, skewness)\n",
    "- Correlation analysis between variables\n",
    "- Outlier detection and data quality assessment\n",
    "\n",
    "### 2. **Experimental Design**:\n",
    "- Variable selection optimization\n",
    "- Multicollinearity assessment\n",
    "- Principal Component Analysis (PCA)\n",
    "- Variable importance ranking\n",
    "\n",
    "### 3. **Data Exploration**:\n",
    "- Spatial patterns in environmental variables\n",
    "- Temporal trends and seasonality\n",
    "- Cross-correlation matrices\n",
    "- Distribution plots and histograms\n",
    "\n",
    "## Applications:\n",
    "- **Model Optimization**: Select optimal variable combinations\n",
    "- **Quality Control**: Identify problematic variables or outliers\n",
    "- **Ecological Interpretation**: Understand environmental gradients\n",
    "- **Experimental Planning**: Design robust modeling experiments\n",
    "\n",
    "## Statistical Methods:\n",
    "- Correlation analysis (Pearson, Spearman)\n",
    "- Principal Component Analysis (PCA)\n",
    "- Variance Inflation Factor (VIF) for multicollinearity\n",
    "- Distribution fitting and normality tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58289e21-bf34-4217-823c-21bebe49034c",
   "metadata": {},
   "source": [
    "# Maximum Entropy Principle in Species Distribution Modeling\n",
    "\n",
    "The Maximum Entropy (MaxEnt) principle is fundamental to species distribution modeling. It states that among all possible probability distributions that satisfy given constraints, the one with maximum entropy is the most unbiased and should be preferred.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Entropy**: Measures uncertainty or information content in a probability distribution\n",
    "- **Constraints**: Environmental conditions and species occurrence data\n",
    "- **Unbiased**: No assumptions beyond the available data\n",
    "- **Optimal**: Provides the most conservative estimate given the constraints\n",
    "\n",
    "**Reference**: [The Principle of Maximum Entropy](https://medium.com/intuition/the-principle-of-maximum-entropy-ec5fa2f84a0c)\n",
    "\n",
    "This principle guides our variable selection and model design to ensure robust, unbiased predictions of species distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6c801-82ef-4e90-a975-632b41509cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### EXPERIMENTAL CONFIGURATION - MODIFY AS NEEDED ###############\n",
    "\n",
    "# =============================================================================\n",
    "# SPECIES AND MODELING CONFIGURATION\n",
    "# =============================================================================\n",
    "# Define the target species and modeling parameters for statistical analysis\n",
    "# Uncomment and modify the lines below to change the analysis configuration\n",
    "\n",
    "# specie = 'leptocybe-invasa'  # Target species: 'leptocybe-invasa' or 'thaumastocoris-peregrinus'\n",
    "# pseudoabsence = 'random'  # Background point strategy: 'random', 'biased', 'biased-land-cover'\n",
    "# training = 'east-asia'  # Training region for statistical analysis\n",
    "# interest = 'south-east-asia'  # Region of interest for comparison\n",
    "\n",
    "# =============================================================================\n",
    "# ENVIRONMENTAL VARIABLE SELECTION FOR STATISTICAL ANALYSIS\n",
    "# =============================================================================\n",
    "# Choose which bioclimatic variables to include in the analysis\n",
    "# Each number corresponds to a specific bioclimatic variable (1-19)\n",
    "\n",
    "# bioclim = [i for i in range(1,20)]  # All 19 bioclimatic variables (comprehensive analysis)\n",
    "\n",
    "# =============================================================================\n",
    "# CURRENT CONFIGURATION (USING PREVIOUSLY DEFINED VARIABLES)\n",
    "# =============================================================================\n",
    "bioclim = bioclim  # Use current bioclimatic variable set from previous analysis\n",
    "topo = topo1  # Topographic variables (elevation, slope, aspect)\n",
    "savefig = True  # Save generated plots and statistics to output directory\n",
    "bio = bio1  # Bioclimatic variable identifier for file naming\n",
    "ndvi = ndvi1  # NDVI (Normalized Difference Vegetation Index) variable inclusion\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffd08e-a2d8-40a9-8d8e-160d5e6e353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "# Standard library imports\n",
    "import os  # File system operations and path manipulation\n",
    "\n",
    "# Core scientific computing libraries\n",
    "import numpy as np  # Numerical computing and array operations\n",
    "import xarray as xr  # Multi-dimensional labeled arrays for raster data handling\n",
    "import rioxarray as rioxr  # Raster I/O operations for xarray (geospatial data)\n",
    "\n",
    "# Data manipulation and analysis libraries\n",
    "import pandas as pd  # Data manipulation and analysis for tabular data\n",
    "import geopandas as gpd  # Geospatial data handling and spatial operations\n",
    "import elapid as ela  # Species distribution modeling library for SDM analysis\n",
    "import scipy.stats as stats  # Statistical functions and tests for probability distributions\n",
    "\n",
    "# Visualization library\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization for statistical plots\n",
    "\n",
    "# =============================================================================\n",
    "# MATPLOTLIB CONFIGURATION FOR PUBLICATION-QUALITY PLOTS\n",
    "# =============================================================================\n",
    "# Configure matplotlib parameters for consistent, publication-ready figure formatting\n",
    "params = {'legend.fontsize': 'x-large',      # Large legend text for readability\n",
    "         'axes.labelsize': 'x-large',        # Large axis labels\n",
    "         'axes.titlesize':'x-large',         # Large plot titles\n",
    "         'xtick.labelsize':'x-large',        # Large x-axis tick labels\n",
    "         'ytick.labelsize':'x-large'}        # Large y-axis tick labels\n",
    "plt.rcParams.update(params)  # Apply the formatting parameters globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de7740-f93f-48c1-925b-3947024bb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_layout(nplots):\n",
    "    \"\"\"\n",
    "    Calculate optimal subplot layout for given number of plots.\n",
    "    \n",
    "    This function determines the best arrangement of subplots to create a balanced\n",
    "    grid layout that minimizes empty space while maintaining readability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nplots : int\n",
    "        Number of plots to arrange in the subplot grid.\n",
    "        Must be a positive integer.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ncols, nrows : tuple of int\n",
    "        Number of columns and rows for the subplot layout.\n",
    "        - ncols: Number of columns (maximum 4 for readability)\n",
    "        - nrows: Number of rows needed to accommodate all plots\n",
    "    \n",
    "    Algorithm:\n",
    "    ----------\n",
    "    1. Calculate the square root of the number of plots for a balanced layout\n",
    "    2. Round up to ensure all plots fit\n",
    "    3. Limit maximum columns to 4 for optimal readability\n",
    "    4. Calculate required rows based on columns and total plots\n",
    "    \n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> subplot_layout(6)\n",
    "    (3, 2)  # 3 columns, 2 rows\n",
    "    >>> subplot_layout(12)\n",
    "    (4, 3)  # 4 columns, 3 rows (max columns reached)\n",
    "    >>> subplot_layout(1)\n",
    "    (1, 1)  # Single plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate square root and round up for balanced layout\n",
    "    # This ensures a roughly square arrangement when possible\n",
    "    ncols = min(int(np.ceil(np.sqrt(nplots))), 4)  # Max 4 columns for readability\n",
    "    \n",
    "    # Calculate rows needed to accommodate all plots\n",
    "    # Ceiling division ensures we have enough rows for all plots\n",
    "    nrows = int(np.ceil(nplots / ncols))\n",
    "    \n",
    "    return ncols, nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be718b-c750-4619-a952-d3b76e21d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SET UP FILE PATHS AND DIRECTORY STRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "out_path = os.path.join(os.path.dirname(os.getcwd()), 'out', specie)\n",
    "docs_path = os.path.join(os.path.dirname(os.getcwd()), 'docs')\n",
    "figs_path = os.path.join(os.path.dirname(os.getcwd()), 'figs')\n",
    "input_path = os.path.join(out_path, 'input')\n",
    "output_path = os.path.join(out_path, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1ad75-8797-4e3a-a6bd-232ad54d9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENTAL VARIABLE CONFIGURATION AND DATA LOADING\n",
    "# =============================================================================\n",
    "# Build lists of raster files and labels for statistical analysis\n",
    "# This section dynamically constructs file paths based on configuration settings\n",
    "\n",
    "# =============================================================================\n",
    "# INITIALIZE RASTER AND LABEL LISTS\n",
    "# =============================================================================\n",
    "# Start with empty lists that will be populated based on enabled variables\n",
    "\n",
    "# Initialize with topographic variables if enabled\n",
    "# SRTM (Shuttle Radar Topography Mission) provides elevation data\n",
    "rasters, labels = (\n",
    "    (['srtm_%s.tif' % training], ['srtm']) if topo else ([], [])\n",
    ")\n",
    "\n",
    "# Add NDVI (Normalized Difference Vegetation Index) variables if enabled\n",
    "# NDVI provides vegetation health and density information\n",
    "rasters, labels = (\n",
    "    rasters + (['ndvi_%s.tif' % training] if ndvi else []),\n",
    "    labels  + (['ndvi'] if ndvi else [])\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# ADD BIOCLIMATIC VARIABLES (HISTORICAL OR FUTURE SCENARIOS)\n",
    "# =============================================================================\n",
    "# Add bioclimatic variables based on whether we're analyzing historical or future data\n",
    "# Each bioclimatic variable represents a specific climate characteristic\n",
    "\n",
    "if Future:\n",
    "    # Future climate projections using climate model data\n",
    "    # File naming convention: {model_prefix}_bio_{variable_number}_{training_region}_future.tif\n",
    "    for no in bioclim:\n",
    "        rasters.append('%s_bio_%s_%s_future.tif' %(model_prefix, no, training))\n",
    "        labels.append('bioclim_%02d' %no)  # Format with zero-padding (e.g., bioclim_01, bioclim_12)\n",
    "else:\n",
    "    # Historical climate data from WorldClim or similar sources\n",
    "    # File naming convention: {model_prefix}_bio_{variable_number}_{training_region}.tif\n",
    "    for no in bioclim:\n",
    "        rasters.append('%s_bio_%s_%s.tif' %(model_prefix, no, training))\n",
    "        labels.append('bioclim_%02d' %no)  # Format with zero-padding\n",
    "\n",
    "# =============================================================================\n",
    "# CONSTRUCT FULL FILE PATHS AND LOAD RASTER DATA\n",
    "# =============================================================================\n",
    "# Create complete file paths by joining with the input directory\n",
    "raster_paths = [os.path.join(input_path, raster) for raster in rasters]\n",
    "\n",
    "# Initialize xarray Dataset to store all environmental variables\n",
    "# xarray provides labeled multi-dimensional arrays ideal for geospatial data\n",
    "training_data = xr.Dataset()\n",
    "\n",
    "# Load each raster file and add to the dataset\n",
    "for raster, label in zip(raster_paths, labels):\n",
    "    # Open raster with rioxarray (handles geospatial metadata)\n",
    "    # masked=True ensures missing values are properly handled\n",
    "    da = rioxr.open_rasterio(raster, masked=True)\n",
    "    training_data[label] = da  # Add to dataset with the specified label\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIONAL: VISUALIZATION AND DATA EXPORT (COMMENTED OUT)\n",
    "# =============================================================================\n",
    "# Uncomment the following sections if you want to:\n",
    "# 1. Print raster and label information for debugging\n",
    "# 2. Create a visualization of all loaded rasters\n",
    "# 3. Export the dataset to NetCDF format\n",
    "\n",
    "# # Debug: Print loaded rasters and labels\n",
    "# # print(rasters)\n",
    "# # print(labels)\n",
    "\n",
    "# Create a comprehensive plot of all raster data\n",
    "# This creates a grid showing all environmental variables\n",
    "# num_plots = len(labels)\n",
    "# fig, axes = plt.subplots(4, 5, figsize=(20, 16))  # 4 rows, 5 columns\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for ax, label in zip(axes, labels):\n",
    "#     training_data[label].plot(ax=ax, cmap='viridis')\n",
    "#     ax.set_title(label)\n",
    "#     ax.axis('off')\n",
    "\n",
    "# # Hide empty subplots if there are fewer variables than subplot spaces\n",
    "# for ax in axes[len(labels):]:\n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Print dataset information\n",
    "# print(training_data)\n",
    "\n",
    "# # Export dataset to NetCDF format for later use\n",
    "# training_data.to_netcdf('../data/training_dataxx.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1505bd0c-8454-4f81-ac6c-3f7b5d51f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD AND ANNOTATE OCCURRENCE DATA\n",
    "# =============================================================================\n",
    "# Load presence and background points, then extract environmental values\n",
    "# This section prepares the training data for statistical analysis by combining\n",
    "# species occurrence data with environmental variables\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD PRESENCE POINT DATA\n",
    "# =============================================================================\n",
    "# Load species presence records from CSV file\n",
    "# File naming convention: {specie}_presence_{training_region}_{iteration}.csv\n",
    "presence_file_name = '%s_presence_%s_%s.csv' %(specie, training, iteration)\n",
    "presence_csv = pd.read_csv(os.path.join(input_path, 'train', presence_file_name))\n",
    "\n",
    "# Convert longitude and latitude coordinates to GeoPandas geometry\n",
    "# This creates point geometries from coordinate pairs\n",
    "geometry = gpd.points_from_xy(presence_csv['lon'], presence_csv['lat'])\n",
    "presence_gdf = gpd.GeoDataFrame(geometry=geometry, crs='EPSG:4326')  # WGS84 coordinate system\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD BACKGROUND POINT DATA\n",
    "# =============================================================================\n",
    "# Load background/pseudo-absence points from CSV file\n",
    "# File naming convention: {specie}_background_{pseudoabsence_strategy}_{training_region}.csv\n",
    "background_file_name = '%s_background_%s_%s.csv' %(specie, pseudoabsence, training)\n",
    "background_csv = pd.read_csv(os.path.join(input_path, 'train', background_file_name))\n",
    "\n",
    "# Convert longitude and latitude coordinates to GeoPandas geometry\n",
    "geometry = gpd.points_from_xy(background_csv['lon'], background_csv['lat'])\n",
    "background_gdf = gpd.GeoDataFrame(geometry=geometry, crs='EPSG:4326')  # WGS84 coordinate system\n",
    "\n",
    "# =============================================================================\n",
    "# EXTRACT ENVIRONMENTAL VALUES AT OCCURRENCE POINTS\n",
    "# =============================================================================\n",
    "# Use elapid's annotate function to extract environmental variable values\n",
    "# at each occurrence point location\n",
    "\n",
    "# Extract environmental values at presence points\n",
    "# This creates a DataFrame with environmental variables for each presence point\n",
    "presence_train = ela.annotate(\n",
    "    presence_gdf.geometry,        # Point geometries for presence locations\n",
    "    raster_paths=raster_paths,    # List of environmental raster file paths\n",
    "    labels=labels,                # Variable names corresponding to each raster\n",
    "    drop_na=True,                 # Remove points with missing environmental data\n",
    "    quiet=True                    # Suppress progress messages\n",
    ")\n",
    "\n",
    "# Extract environmental values at background points\n",
    "# This creates a DataFrame with environmental variables for each background point\n",
    "background_train = ela.annotate(\n",
    "    background_gdf,               # Point geometries for background locations\n",
    "    raster_paths=raster_paths,    # List of environmental raster file paths\n",
    "    labels=labels,                # Variable names corresponding to each raster\n",
    "    drop_na=True,                 # Remove points with missing environmental data\n",
    "    quiet=True                    # Suppress progress messages\n",
    ")\n",
    "\n",
    "# Additional data cleaning step to ensure no missing values remain\n",
    "background_train = background_train.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cda153-1ad9-4a2a-a445-6669a6233439",
   "metadata": {},
   "source": [
    "## 1. Entropy\n",
    "Entropy is an old concept in physics, and describes the measure of chaos or disorder in a system. Higher entropy means lower chaos. The mathematician Claude Shannon introduced the entropy in information theory in 1948. Entropy in information theory is defined as the expected number of bits of information contained in an event.[1](https://medium.com/intro-to-artificial-intelligence/maximum-entropy-reinforcement-learning-ee7ad77289c0)\n",
    "\n",
    "$$ f(X) = - \\sum_{i=1}^n P(x_i) \\log P_(x_i) \\qquad (\\mathrm{entropy}) $$ \n",
    "\n",
    "$$ g(X) = \\sum_{i=1}^n P(x_i) = 1 \\qquad (\\mathrm{constraint})$$\n",
    "\n",
    "where $X=\\{x_1, x_2, ..., x_n\\}$ are the environmental variables.\n",
    "\n",
    "Maximising the entropy\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial p_j} - \\lambda\\frac{\\partial g}{\\partial p_j} = 0 $$\n",
    "\n",
    "where j = 1,2, ... m\n",
    "\n",
    "$$ -\\log p_j - 1 - \\lambda \\cdot 1 = 0 $$\n",
    "[solution](https://www.youtube.com/watch?v=ol8-kZFTLfg)\n",
    "\n",
    "\n",
    "Gibbs probability density function\n",
    "\n",
    "$$ p_1({\\bf x}) = p({\\bf x})e^{-{\\bf x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2a0a2-f423-45d3-a2c6-5f0225513d93",
   "metadata": {},
   "source": [
    "## 1.1 Probability density plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0867c59-2523-4ecc-9518-6b0ce9a7d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROBABILITY DENSITY FUNCTION CALCULATION\n",
    "# =============================================================================\n",
    "# Calculate probability density functions for environmental variables\n",
    "# This analysis fits Maxwell distributions to background data to understand\n",
    "# the environmental space available to the species\n",
    "\n",
    "# Initialize dictionary to store probability density data for each variable\n",
    "var_data = {}\n",
    "\n",
    "# Map bioclimatic variable numbers to standardized labels with zero-padding\n",
    "# This ensures consistent naming (e.g., bioclim_01, bioclim_12)\n",
    "aa = [f'bioclim_{str(num).zfill(2)}' for num in bioclim]\n",
    "\n",
    "# =============================================================================\n",
    "# PROBABILITY DENSITY FUNCTION FITTING\n",
    "# =============================================================================\n",
    "# Fit Maxwell distributions to background data for each environmental variable\n",
    "# The Maxwell distribution is often suitable for environmental data\n",
    "\n",
    "nbins = 100  # Number of bins for probability density calculation\n",
    "\n",
    "for name in aa:\n",
    "    # Extract background data for the current variable\n",
    "    var = background_train[name]\n",
    "    \n",
    "    # Calculate bin width and create extended bin range\n",
    "    # Extended range helps capture the full distribution\n",
    "    dx = (var.max() - var.min()) / nbins\n",
    "    bins = np.linspace(var.min() - 10*dx, var.max() + 10*dx, nbins)\n",
    "    \n",
    "    # Fit Maxwell distribution to the background data\n",
    "    # Maxwell distribution is a continuous probability distribution\n",
    "    # commonly used for modeling environmental variables\n",
    "    fit = stats.maxwell.fit(var)\n",
    "    \n",
    "    # Calculate probability density function values for the fitted distribution\n",
    "    pdf = stats.maxwell.pdf(bins, *fit)\n",
    "    \n",
    "    # Store results in the var_data dictionary\n",
    "    var_data[name] = {}\n",
    "    var_data[name]['bins'] = bins          # Bin centers for plotting\n",
    "    var_data[name]['pdf'] = pdf            # Probability density values\n",
    "    var_data[name]['long_name'] = training_data[name].attrs['long_name']  # Descriptive name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbf9f6-cba9-45a3-a9ac-c09554c6115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE HISTOGRAM PLOTS WITH FITTED PROBABILITY DENSITY FUNCTIONS\n",
    "# =============================================================================\n",
    "# Generate subplot layout and create histograms showing both empirical data\n",
    "# and fitted Maxwell probability density functions for each environmental variable\n",
    "\n",
    "# Calculate optimal subplot layout using the custom function\n",
    "ncols, nrows = subplot_layout(len(aa))\n",
    "\n",
    "# Create figure with subplots\n",
    "# Figure size scales with number of subplots for optimal readability\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*6, nrows*6))\n",
    "\n",
    "# Handle single subplot case (when only one variable is selected)\n",
    "if (nrows, ncols) == (1, 1):\n",
    "    ax = [axs]  # Convert single axis to list for consistent indexing\n",
    "else:\n",
    "    ax = axs.ravel()  # Flatten 2D array of axes to 1D for easy iteration\n",
    "\n",
    "# =============================================================================\n",
    "# PLOT HISTOGRAMS AND FITTED DISTRIBUTIONS\n",
    "# =============================================================================\n",
    "# Create histogram plots for each environmental variable\n",
    "# Each plot shows both the empirical distribution and fitted Maxwell PDF\n",
    "\n",
    "i = 0\n",
    "for i in range(len(aa)):\n",
    "    # Plot histogram of background data\n",
    "    # density=True normalizes the histogram to show probability density\n",
    "    background_train[aa[i]].plot.hist(ax=ax[i], bins=20, density=True, facecolor='lightgray', edgecolor='darkgray')\n",
    "    \n",
    "    # Overlay fitted Maxwell probability density function\n",
    "    ax[i].plot(var_data[aa[i]]['bins'], var_data[aa[i]]['pdf'], lw=3, label='maxwell pdf')\n",
    "    \n",
    "    # Set axis labels using descriptive names from raster metadata\n",
    "    ax[i].set_xlabel(var_data[aa[i]]['long_name'])\n",
    "    ax[i].set_ylabel('Probability Density')\n",
    "    \n",
    "    # Hide empty subplots if there are more subplot spaces than variables\n",
    "    if ncols * nrows > len(aa):\n",
    "        ax[len(aa)].set_axis_off()\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd4e37-ff86-4173-a81e-55f3d2d3e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'models' variable is not null or empty\n",
    "        if models:\n",
    "            # If a models is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '04_pdf_bioclim_%s_%s_%s_%s_future.png' % (training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no models is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '04_pdf_bioclim_%s_%s_%s_future.png' % (training, bio,iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True)\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a models is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '04_pdf_bioclim_%s_%s_%s_%s.png' % (training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '04_pdf_bioclim_%s_%s_%s.png' % (training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f5e17-aff1-4fec-b93c-8651176ae663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(ncols=1, figsize=(8,6))\n",
    "\n",
    "# background_train['bioclim_01'].plot.hist(ax=ax, bins=20, density=True, facecolor='lightgray', edgecolor='darkgray')\n",
    "# plt.plot(var_data['bioclim_01']['bins'], var_data['bioclim_01']['pdf'], lw=3, label='maxwell pdf')\n",
    "# ax.set_xlabel(var_data['bioclim_01']['long_name'])\n",
    "# ax.set_ylabel('Probability Density')\n",
    "# fig.savefig(os.path.join(docs_path, '04_pdf_bioclim-1_%s.png' %training), transparent=True, dpi=600)\n",
    "\n",
    "# background_train['bioclim_12'].plot.hist(ax=ax, bins=20, density=True, facecolor='lightgray', edgecolor='darkgray')\n",
    "# ax.plot(var_data['bioclim_12']['bins'], var_data['bioclim_12']['pdf'], lw=3, label='maxwell pdf')\n",
    "# ax.set_xlabel(var_data['bioclim_12']['long_name'])\n",
    "# ax.set_ylabel('Probability Density')\n",
    "# fig.savefig(os.path.join(docs_path, '04_pdf_bioclim-12.png'), transparent=True, dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0bafd8-142f-4e07-9c04-015e14d92a4d",
   "metadata": {},
   "source": [
    "## 1.2 Probability density presence and background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7866a-6828-439f-add3-136231564935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPARATIVE HISTOGRAM PLOTS: PRESENCE VS BACKGROUND\n",
    "# =============================================================================\n",
    "# Create side-by-side histograms comparing environmental variable distributions\n",
    "# between species presence points and background points for all variables\n",
    "\n",
    "# Define color scheme for presence and background data\n",
    "pair_colors = ['tab:blue', 'tab:red']  # Blue for presence, red for background\n",
    "\n",
    "# Calculate optimal subplot layout for all environmental variables\n",
    "ncols, nrows = subplot_layout(len(labels))\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*6, nrows*6))\n",
    "\n",
    "# Handle single subplot case\n",
    "if (nrows, ncols) == (1, 1):\n",
    "    ax = [axs]\n",
    "else:\n",
    "    ax = axs.ravel()\n",
    "\n",
    "# Get list of variable names for accessing metadata\n",
    "xlabels = list(training_data.data_vars.keys())\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE COMPARATIVE HISTOGRAMS FOR EACH VARIABLE\n",
    "# =============================================================================\n",
    "# Plot overlapping histograms showing distribution differences between\n",
    "# presence and background points for each environmental variable\n",
    "\n",
    "for iax, label in enumerate(labels):\n",
    "    # Extract presence and background data for current variable\n",
    "    pvar = presence_train[label]  # Environmental values at presence points\n",
    "    bvar = background_train[label]  # Environmental values at background points\n",
    "    \n",
    "    # Create overlapping histograms\n",
    "    ax[iax].hist(\n",
    "        [pvar, bvar],                    # Data arrays for presence and background\n",
    "        density=True,                    # Normalize to show probability density\n",
    "        alpha=0.7,                       # Semi-transparent for overlap visibility\n",
    "        label=['presence', 'background'], # Legend labels\n",
    "        color=pair_colors,               # Color scheme defined above\n",
    "    )\n",
    "    \n",
    "    # Set subplot title to variable name\n",
    "    ax[iax].set_title(label)\n",
    "    \n",
    "    # Set x-axis label using descriptive name from raster metadata\n",
    "    try:\n",
    "        ax[iax].set_xlabel(training_data[xlabels[iax]].long_name)\n",
    "    except AttributeError:\n",
    "        # Fallback if long_name attribute is not available\n",
    "        ax[iax].set_xlabel('No variable long_name')\n",
    "\n",
    "# =============================================================================\n",
    "# ADD LEGEND AND FORMATTING\n",
    "# =============================================================================\n",
    "# Create a single legend for the entire figure\n",
    "handles, lbls = ax[iax].get_legend_handles_labels()\n",
    "fig.legend(handles, lbls, loc='upper right', bbox_to_anchor=(0.16, 0.965))\n",
    "\n",
    "# Adjust layout to prevent overlapping elements\n",
    "plt.tight_layout()\n",
    "\n",
    "# Hide empty subplots if there are more subplot spaces than variables\n",
    "for axi in ax:\n",
    "    if not axi.title.get_text():\n",
    "        axi.set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf4ae3-3a80-4a84-b401-259e3fc2cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if savefig:\n",
    "#     if Future:\n",
    "#         fig.savefig(os.path.join(figs_path, '04_pdf_env-variables_%s_%s_future.png' %(training, bio)), transparent=True)\n",
    "#     else:    \n",
    "#         fig.savefig(os.path.join(figs_path, '04_pdf_env-variables_%s_%s.png' %(training, bio)), transparent=True)\n",
    "\n",
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'models' variable is not null or empty\n",
    "        if models:\n",
    "            # If a models is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '04_pdf_env-variables_%s_%s_%s_%s_future.png' % (training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no models is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '04_pdf_env-variables_%s_%s_%s_future.png' % (training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True)\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a models is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '04_pdf_env-variables_%s_%s_%s_%s.png' % (training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '04_pdf_env-variables_%s_%s_%s.png' % (training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e034a3-7167-4fb4-b6f7-d0b6fa69cff2",
   "metadata": {},
   "source": [
    "## 2. Variable correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b141e-b3db-4983-b69f-d8dce581a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPARE DATA FOR CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "# Convert xarray dataset to pandas DataFrame for correlation analysis\n",
    "# This step extracts the environmental variable values from the raster data\n",
    "\n",
    "# Alternative approach (commented out): Merge multiple datasets\n",
    "# ds = xr.merge([bioclim, srtm_region])\n",
    "\n",
    "# Convert xarray dataset to pandas DataFrame\n",
    "# isel(band=0): Select first band (most raster data has single band)\n",
    "# reset_coords: Remove coordinate variables that aren't needed for correlation\n",
    "# to_dataframe(): Convert to pandas DataFrame for statistical analysis\n",
    "df = training_data.isel(band=0).reset_coords(['band', 'spatial_ref'], drop=True).to_dataframe()\n",
    "\n",
    "# Calculate Spearman correlation matrix\n",
    "# Spearman correlation is rank-based and more robust to outliers than Pearson\n",
    "# It measures monotonic relationships between variables\n",
    "correlation_matrix = df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af201e-c111-4e89-950f-1fe8e8d5e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE CORRELATION MATRIX HEATMAP VISUALIZATION\n",
    "# =============================================================================\n",
    "# Generate a heatmap showing correlations between all environmental variables\n",
    "# This helps identify multicollinearity and variable relationships\n",
    "\n",
    "# Create figure with constrained layout for better spacing\n",
    "fig, ax = plt.subplots(figsize=(12, 12), constrained_layout=True)\n",
    "\n",
    "# Create heatmap using imshow with coolwarm colormap\n",
    "# coolwarm: blue for negative correlations, red for positive correlations\n",
    "im = ax.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')\n",
    "\n",
    "# =============================================================================\n",
    "# ADD CORRELATION VALUES AS TEXT OVERLAY\n",
    "# =============================================================================\n",
    "# Display numerical correlation values on each cell of the heatmap\n",
    "# This provides precise correlation coefficients for interpretation\n",
    "\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(correlation_matrix.shape[1]):\n",
    "        plt.text(j, i, f\"{correlation_matrix.iloc[i, j]:.2f}\",\n",
    "                 ha='center', va='center', color='white', weight='bold')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURE AXIS LABELS AND TITLES\n",
    "# =============================================================================\n",
    "# Set up axis labels and formatting for the correlation matrix\n",
    "\n",
    "# Get column names for axis labels\n",
    "columns = df.columns.tolist()\n",
    "\n",
    "# Set x-axis ticks and labels (rotated 90 degrees for readability)\n",
    "ax.set_xticks(range(len(columns)), columns, rotation=90)\n",
    "\n",
    "# Set y-axis ticks and labels\n",
    "ax.set_yticks(range(len(columns)), columns)\n",
    "\n",
    "# Add title to the plot\n",
    "ax.set_title(\"Variable correlation matrix\")\n",
    "\n",
    "# Add colorbar with label\n",
    "fig.colorbar(im, label=\"Spearman Correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68542677-d322-43c4-8f0a-dabf7f46e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if savefig:\n",
    "#     if Future:\n",
    "#         fig.savefig(os.path.join(figs_path, '04_var-corr-matrix_%s_%s_future.png' %(training, bio)), transparent=True, bbox_inches='tight')\n",
    "#     else:\n",
    "#         fig.savefig(os.path.join(figs_path, '04_var-corr-matrix_%s_%s.png' %(training, bio)), transparent=True, bbox_inches='tight')\n",
    "\n",
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'models' variable is not null or empty\n",
    "        if models:\n",
    "            # If a models is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '04_var-corr-matrix_%s_%s_%s_%s_future.png' % (training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no models is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '04_var-corr-matrix_%s_%s_%s_future.png' % (training, bio, iteration))\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a models is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '04_var-corr-matrix_%s_%s_%s_%s.png' % (training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '04_var-corr-matrix_%s_%s_%s.png' % (training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e3e9f-e234-4778-872c-a8ca9c2fe4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HIERARCHICAL CLUSTERING AND DENDROGRAM VISUALIZATION\n",
    "# =============================================================================\n",
    "# Perform hierarchical clustering on environmental variables based on correlation\n",
    "# This helps identify groups of similar variables and potential redundancy\n",
    "\n",
    "# Import required functions for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# =============================================================================\n",
    "# PREPARE DISSIMILARITY MATRIX FOR CLUSTERING\n",
    "# =============================================================================\n",
    "# Convert correlation matrix to dissimilarity matrix\n",
    "# Correlation values range from -1 to 1, dissimilarity should be 0 to 1\n",
    "# Using absolute correlation ensures both positive and negative correlations\n",
    "# are treated as similarity (both indicate strong relationships)\n",
    "\n",
    "dissimilarity = 1 - abs(correlation_matrix)\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORM HIERARCHICAL CLUSTERING\n",
    "# =============================================================================\n",
    "# Convert dissimilarity matrix to condensed distance matrix for linkage function\n",
    "# 'complete' linkage uses maximum distance between clusters\n",
    "# This tends to create compact, well-separated clusters\n",
    "\n",
    "Z = linkage(squareform(dissimilarity), 'complete', optimal_ordering=True)\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE DENDROGRAM VISUALIZATION\n",
    "# =============================================================================\n",
    "# Generate dendrogram showing the hierarchical clustering structure\n",
    "# This helps visualize which variables cluster together\n",
    "\n",
    "xx = plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create dendrogram with variable labels\n",
    "dendrogram(Z, \n",
    "           labels=df.columns,      # Variable names as leaf labels\n",
    "           orientation='top',      # Root at top, leaves at bottom\n",
    "           leaf_rotation=90)       # Rotate leaf labels for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a7156-2cdc-46b7-bc61-91838de52b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if savefig:\n",
    "#     if Future:\n",
    "#         xx.savefig(os.path.join(figs_path, '04_Dendogram_%s_%s_future.png' %(training, bio)), transparent=True)\n",
    "#     else:\n",
    "#         xx.savefig(os.path.join(figs_path, '04_Dendogram_%s_%s.png' %(training, bio)), transparent=True)\n",
    "\n",
    "\n",
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'models' variable is not null or empty\n",
    "        if models:\n",
    "            # If a models is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '04_Dendogram_%s_%s_%s_%s_future.png' % (training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no models is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '04_Dendogram_%s_%s_%s_future.png' % (training, bio, iteration))\n",
    "        \n",
    "        xx.savefig(file_path, transparent=True)\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a models is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '04_Dendogram_%s_%s_%s_%s.png' % (training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '04_Dendogram_%s_%s_%s.png' % (training, bio, iteration))\n",
    "        \n",
    "        xx.savefig(file_path, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ed3ab-5ccd-4bd0-9efb-f52478fed8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ben's k medoids experiment\n",
    "# import kmedoids\n",
    "# from sklearn.metrics.pairwise import (\n",
    "#     pairwise_distances)\n",
    "\n",
    "# print(dissimilarity)\n",
    "# dissimilarity.to_csv(\"/scratch/gito_aciar/sdm-toolbox/figs/dissmililarity.csv\")\n",
    "# kmin = 1\n",
    "# kmax = 6\n",
    "# dm = kmedoids.dynmsc(dissimilarity, kmax, kmin)\n",
    "\n",
    "# print(\"Optimal number of clusters according to the Medoid Silhouette:\", dm.bestk)\n",
    "\n",
    "# k_medoid_results = kmedoids.fasterpam(dissimilarity,6)\n",
    "\n",
    "# print(k_medoid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7741674-383c-4466-8e79-e38878a0505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/osgeokr/pySDM-geemap/blob/main/pySDM-geemap_Case%20Study%201_Pitta%20nympha.ipynb\n",
    "# https://github.com/dennisbakhuis/Tutorials/blob/master/3_Covariance_PCA/Principle%20component%20analysis%20and%20the%20covariance%20matrix.ipynb\n",
    "# https://www.geeksforgeeks.org/exploring-correlation-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aciar",
   "language": "python",
   "name": "aciar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
