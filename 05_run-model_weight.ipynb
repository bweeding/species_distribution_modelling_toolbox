{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b15204-7112-48a4-97c4-bc90bc40550d",
   "metadata": {},
   "source": [
    "# Weighted MaxEnt Species Distribution Model Training and Prediction\n",
    "\n",
    "This notebook implements a **weighted version** of the MaxEnt (Maximum Entropy) species distribution modealing workflow. Unlike the standard MaxEnt approach, this version incorporates **sample weights** to account for data quality, spatial bias, or temporal differences in occurrence records.\n",
    "\n",
    "## Key Features of Weighted MaxEnt:\n",
    "\n",
    "### 1. **Sample Weighting**:\n",
    "- **Data Quality Weights**: Weight samples based on data source reliability\n",
    "- **Spatial Bias Correction**: Reduce influence of oversampled regions\n",
    "- **Temporal Weights**: Account for temporal bias in occurrence records\n",
    "- **Expert Knowledge Integration**: Incorporate expert assessments of record quality\n",
    "\n",
    "### 2. **Weighting Strategies**:\n",
    "- **Source-based**: Different weights for GBIF, CABI, research publications\n",
    "- **Spatial**: Inverse distance weighting or kernel density weighting\n",
    "- **Temporal**: Recent records weighted higher than historical ones\n",
    "- **Quality-based**: Expert-assessed data quality scores\n",
    "\n",
    "### 3. **Model Configuration**:\n",
    "- **Algorithm**: Weighted MaxEnt with logistic output transformation\n",
    "- **Regularization**: Beta multiplier = 1.5 (controls model complexity)\n",
    "- **Weight Integration**: Weights applied during model training\n",
    "- **Output**: Relative occurrence probability (0-1 scale)\n",
    "\n",
    "## Applications:\n",
    "- **Bias Correction**: Reduce spatial and temporal sampling bias\n",
    "- **Data Integration**: Combine multiple data sources with different quality\n",
    "- **Expert Knowledge**: Incorporate field expert assessments\n",
    "- **Quality Control**: Emphasize high-quality occurrence records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7b778-d906-49be-86c9-9759448f7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### WEIGHTED MODEL CONFIGURATION - MODIFY AS NEEDED ###############\n",
    "\n",
    "# Species and region settings for weighted modeling\n",
    "#specie = 'leptocybe-invasa'  # Target species: 'leptocybe-invasa' or 'thaumastocoris-peregrinus'\n",
    "#pseudoabsence = 'random'  # Background point strategy: 'random', 'biased', 'biased-land-cover'\n",
    "#training = 'india-sri-lanka'  # Training region: 'sea', 'australia', 'east-asia', etc.\n",
    "#interest = 'south-east-asia'  # Test region: can be same as training or different\n",
    "\n",
    "# Environmental variable configuration\n",
    "# bioclim = bioclim_model  # Bioclimatic variables (from previous notebook)\n",
    "bioclim = bioclim  # Current bioclimatic variable set\n",
    "bio = bio1  # Bioclimatic variable identifier\n",
    "\n",
    "# Additional environmental variables\n",
    "topo = topo1  # Topographic variables (elevation, slope, aspect)\n",
    "ndvi = ndvi1  # Normalized Difference Vegetation Index\n",
    "# topo = True  # Alternative: include all topographic variables\n",
    "\n",
    "# Output settings\n",
    "savefig = True  # Save generated maps as PNG files\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d412007-8c3c-4d88-b041-99f47ef4ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os  # File system operations\n",
    "\n",
    "import math  # Mathematical functions\n",
    "import numpy as np  # Numerical computing\n",
    "\n",
    "import xarray as xr  # Multi-dimensional labeled arrays (raster data)\n",
    "import rioxarray as rioxr  # Raster I/O for xarray\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import geopandas as gpd  # Geospatial data handling\n",
    "\n",
    "import elapid as ela  # Species distribution modeling library\n",
    "from sklearn import metrics, inspection  # Machine learning metrics and model inspection\n",
    "\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "import matplotlib.colors as mcolors  # Color mapping utilities\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warning messages for cleaner output\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6557f1-586b-4229-babf-5135b0e90569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_layout(nplots):\n",
    "    \"\"\"\n",
    "    Calculate optimal subplot layout for given number of plots\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nplots : int\n",
    "        Number of plots to arrange\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ncols, nrows : tuple\n",
    "        Number of columns and rows for subplot layout\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate square root and round up for balanced layout\n",
    "    ncols = min(int(np.ceil(np.sqrt(nplots))), 4)  # Max 4 columns\n",
    "    nrows = int(np.ceil(nplots / ncols))  # Calculate rows needed\n",
    "    \n",
    "    return ncols, nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c5c92-7ef0-4dce-bd31-a2cc9fef5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SET UP FILE PATHS\n",
    "# =============================================================================\n",
    "# Define directory structure for organizing weighted model outputs\n",
    "\n",
    "# data_path = os.path.join(os.sep, 'scratch', 'aciar-fst', 'data')  # Alternative server path\n",
    "data_path = os.path.join(os.path.dirname(os.getcwd()), 'data')  # Main data directory\n",
    "figs_path = os.path.join(os.path.dirname(os.getcwd()), 'figs')  # Figures directory\n",
    "docs_path = os.path.join(os.path.dirname(os.getcwd()), 'docs')  # Documentation directory\n",
    "out_path = os.path.join(os.path.dirname(os.getcwd()), 'out', specie)  # Species-specific output directory\n",
    "input_path = os.path.join(out_path, 'input')  # Input data directory\n",
    "train_path = os.path.join(input_path, 'train')  # Training data directory\n",
    "test_path = os.path.join(input_path, 'test')  # Test data directory\n",
    "output_path = os.path.join(out_path, 'output')  # Model output directory\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f19209-7f3b-409d-b4e7-d7a98dac6591",
   "metadata": {},
   "source": [
    "## Load Geographic Boundaries\n",
    "\n",
    "Load shapefiles defining the training and test regions for the weighted MaxEnt model. These boundaries are used to:\n",
    "- **Spatial Cropping**: Extract environmental data for specific regions\n",
    "- **Model Training**: Define the geographic extent for model training\n",
    "- **Model Testing**: Apply trained models to different geographic areas\n",
    "- **Visualization**: Provide geographic context for distribution maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78573035-d8c3-4e4a-b5b5-0d0dec559050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD TRAINING REGION BOUNDARIES\n",
    "# =============================================================================\n",
    "# Load shapefile defining the training region for weighted model development\n",
    "\n",
    "# Alternative approach: Load multiple regions (commented out)\n",
    "# gdf_countries = {}\n",
    "# for region in [training, interest]:\n",
    "#     file_path = train_path if region == training else test_path\n",
    "#     gdf_countries[region] = gpd.read_file(os.path.join(file_path, '%s.shp' %region))\n",
    "\n",
    "# Load training region shapefile\n",
    "gdf_countries = gpd.read_file(os.path.join(input_path, '%s.shp' %training))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7a900-85cd-41d4-87e6-7179c9320233",
   "metadata": {},
   "source": [
    "## 1. Train Weighted MaxEnt Model\n",
    "\n",
    "This section trains the weighted MaxEnt model using occurrence data from the training region. The key difference from standard MaxEnt is the incorporation of **sample weights** to account for:\n",
    "\n",
    "- **Data Source Quality**: Different weights for GBIF, CABI, research publications\n",
    "- **Spatial Bias**: Reduce influence of oversampled areas\n",
    "- **Temporal Bias**: Weight recent records higher than historical ones\n",
    "- **Expert Knowledge**: Incorporate field expert assessments of record quality\n",
    "\n",
    "### Weighted MaxEnt Advantages:\n",
    "- **Bias Reduction**: Mitigate spatial and temporal sampling bias\n",
    "- **Data Integration**: Combine multiple data sources with different quality levels\n",
    "- **Improved Accuracy**: Better model performance through quality-weighted training\n",
    "- **Robust Predictions**: More reliable distribution estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a59e2f-1490-4a88-9e39-92bbd8d76c7f",
   "metadata": {},
   "source": [
    "### 1.1 load predictive variable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e425b37-7c46-4d47-9adf-6de927cc88a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble predictor raster filenames and human-readable labels\n",
    "# - Includes optional topography (SRTM) and NDVI depending on `topo` and `ndvi`\n",
    "# - Appends selected bioclim variables with a `model_prefix` tag for traceability\n",
    "# rasters, labels = (['srtm_%s.tif' %training], ['srtm']) if topo else ([], []) # 'ndvi_east-asia.tif', 'ndvi' # \n",
    "\n",
    "# Base layers (topography)\n",
    "rasters, labels = (\n",
    "    (['srtm_%s.tif' % training], ['srtm']) if topo else ([], [])\n",
    ")\n",
    "# Optional NDVI layer\n",
    "rasters, labels = (\n",
    "    rasters + (['ndvi_%s.tif' % training] if ndvi else []),\n",
    "    labels  + (['ndvi'] if ndvi else [])\n",
    ")\n",
    "\n",
    "# Historical vs future template (kept for reference)\n",
    "# if Future:\n",
    "#     for no in bioclim:\n",
    "#         rasters.append('bio_%s_%s_future.tif' %(no, training))\n",
    "#         labels.append('bioclim_%02d' %no)\n",
    "# else:\n",
    "#     for no in bioclim:\n",
    "#         rasters.append('bio_%s_%s.tif' %(no, training))\n",
    "#         labels.append('bioclim_%02d' %no)\n",
    "\n",
    "# Selected bioclim features (current)\n",
    "for no in bioclim:\n",
    "    rasters.append('%s_bio_%s_%s.tif' %(model_prefix, no, training))\n",
    "    labels.append('%s_bioclim_%02d' %(model_prefix, no))\n",
    "\n",
    "# Resolve full file paths for raster IO\n",
    "raster_paths = [os.path.join(input_path, raster) for raster in rasters]\n",
    "# rasters, raster_paths, labels\n",
    "\n",
    "# Initialize xarray.Dataset to carry predictors and later predictions\n",
    "training_output = xr.Dataset()\n",
    "for raster, label in zip(raster_paths, labels):\n",
    "    # Use masked read to preserve nodata as NaN\n",
    "    da = rioxr.open_rasterio(raster, masked=True)\n",
    "    training_output[label] = da\n",
    "\n",
    "# print(label)\n",
    "# print(da)\n",
    "\n",
    "# # Example plotting block (disabled)\n",
    "# num_plots = len(labels)\n",
    "# fig, axes = plt.subplots(4, 5, figsize=(20, 16))  # 4 rows, 5 cols\n",
    "# axes = axes.flatten()\n",
    "# for ax, label in zip(axes, labels):\n",
    "#     training_data[label].plot(ax=ax, cmap='viridis')\n",
    "#     ax.set_title(label)\n",
    "#     ax.axis('off')\n",
    "# for ax in axes[len(labels):]:\n",
    "#     ax.axis('off')\n",
    "# plt.tight_layout(); plt.show()\n",
    "\n",
    "# print(training_output)\n",
    "# training_output.to_netcdf('../data/training_output.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d49fd-4f91-411c-8ad4-90c0f202894f",
   "metadata": {},
   "source": [
    "### 1.2 load and merge presence and background data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ba83f-5574-4abf-8515-fe905117d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build canonical input filenames for presence, background, and exported train table\n",
    "# Presence file includes occurrence coordinates for the training region\n",
    "presence_file_name = '%s_presence_%s_%s.csv' %(specie, training, iteration)\n",
    "# Background file depends on pseudoabsence strategy label\n",
    "background_file_name = '%s_background_%s_%s.csv' %(specie, pseudoabsence, training)\n",
    "# Name for the fully-merged training table saved for reproducibility\n",
    "train_input_data_name = '%s_model-train_input-data_%s_%s_%s_%s_%s.csv' %(model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "\n",
    "# Load presence/background CSVs and convert lon/lat into a GeoDataFrame in WGS84\n",
    "# Expect input columns 'lon' and 'lat' in decimal degrees\n",
    "presence_csv = pd.read_csv(os.path.join(input_path, 'train', presence_file_name))\n",
    "geometry = gpd.points_from_xy(presence_csv['lon'], presence_csv['lat'])\n",
    "presence_gdf = gpd.GeoDataFrame(geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "# Background points used as pseudo-absences for MaxEnt\n",
    "background_csv = pd.read_csv(os.path.join(input_path, 'train', background_file_name))\n",
    "geometry = gpd.points_from_xy(background_csv['lon'], background_csv['lat'])\n",
    "background_gdf = gpd.GeoDataFrame(geometry=geometry, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1cc7b4-b97d-45cc-9843-a0c2025572ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raster values at presence locations; drop observations that fall on nodata\n",
    "# Returns a GeoDataFrame with columns for each predictor at each point\n",
    "presence_train = ela.annotate(\n",
    "    presence_gdf.geometry,\n",
    "    raster_paths=raster_paths, \n",
    "    labels=labels, \n",
    "    drop_na=True,\n",
    ")\n",
    "\n",
    "# Compute sample weights for presence via distance-based weighting\n",
    "# n_neighbors=-1 uses all other points to estimate local density (global KDE-like)\n",
    "# Higher density => lower weight; isolated points => higher weight\n",
    "presence_train['SampleWeight'] = ela.distance_weights(presence_train, n_neighbors=-1)\n",
    "# Quick diagnostic plot of weights\n",
    "presence_train.plot(column='SampleWeight', legend=True, cmap='YlOrBr')\n",
    "\n",
    "# Extract raster values at background locations (pseudo-absences)\n",
    "background_train = ela.annotate(\n",
    "    background_gdf,  # supports different biasing strategies upstream\n",
    "    raster_paths=raster_paths, \n",
    "    labels=labels, \n",
    "    drop_na=True,\n",
    ")\n",
    "\n",
    "# Background weights based on nearest-neighbor density (n_neighbors=1)\n",
    "background_train['SampleWeight'] = ela.distance_weights(background_train, n_neighbors=1)\n",
    "\n",
    "# Combine presence and background with a binary class label (1=presence, 0=background)\n",
    "# Resulting table contains predictors, class label, geometry, and SampleWeight\n",
    "train = ela.stack_geodataframes(\n",
    "    presence_train,\n",
    "    background_train,\n",
    "    add_class_label=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc0b43-7772-427f-9ac5-54d542a03b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66728b4-87b4-4ce7-acd8-79cac73690d3",
   "metadata": {},
   "source": [
    "### 1.3 run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0a717-0fa9-4520-9d29-bcd8b49d973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1.3 Run Model\")\n",
    "# Build experiment/run identifiers to keep outputs organized and reproducible\n",
    "experiment_name = 'exp_%s_%s_%s_%s_%s' %(model_prefix, pseudoabsence, training, topo, ndvi)\n",
    "run_name = '%s_model-train_%s_%s_%s_%s_%s.ela' %(model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "# NetCDF and raster file names for gridded predictions\n",
    "nc_name = '%s_model-train_%s_%s_%s_%s_%s.nc' %(model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "raster_name = '%s_model-train_%s_%s_%s_%s_%s.tif' %(model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "\n",
    "# Make sure the experiment output directory exists\n",
    "exp_path = os.path.join(output_path, experiment_name)\n",
    "if not os.path.exists(exp_path):\n",
    "    os.makedirs(exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188e2b0-981b-477d-a7a3-200842f5cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the fully-prepared training table for traceability/reproducibility\n",
    "# Includes predictors, labels, geometry, and computed SampleWeight\n",
    "train.to_csv(os.path.join(exp_path, train_input_data_name))\n",
    "\n",
    "# Split features/labels/weights; drop non-feature columns\n",
    "# x_train contains only predictor columns used by the model\n",
    "x_train = train.drop(columns=['class', 'SampleWeight', 'geometry' ])\n",
    "# y_train is the binary target: 1 for presence, 0 for background\n",
    "y_train = train['class']\n",
    "# sample_weight per observation passed to the learner to correct bias/imbalance\n",
    "sample_weight_train = train['SampleWeight']\n",
    "\n",
    "# Configure and fit weighted MaxEnt (logistic output)\n",
    "# beta_multiplier controls regularization strength (higher => smoother model)\n",
    "model_train = ela.MaxentModel(transform='logistic', beta_multiplier=1.5,)\n",
    "print(model_train.get_params())\n",
    "model_train.fit(x_train, y_train, sample_weight=sample_weight_train)\n",
    "\n",
    "# Serialize trained model to disk for later reuse\n",
    "ela.save_object(model_train, os.path.join(exp_path, run_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba25d5-500f-4ae3-9f10-6cf41f92ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the model\n",
    "print(\"predict the model\")\n",
    "\n",
    "# Point-wise predictions for the training table (used for metrics/diagnostics)\n",
    "y_train_predict = model_train.predict(x_train)\n",
    "# y_train_predict = np.nan_to_num(y_train_predict, nan=0.5)\n",
    "\n",
    "# Apply the model across the full raster stack to produce a map\n",
    "# Convert xarray Dataset to a 3D numpy array [band, y, x]\n",
    "array = training_output.isel(band=0).to_array().values\n",
    "# Define nodata value and mask for safe application over missing areas\n",
    "nodata = np.nan\n",
    "nodata_idx = np.isnan(array)\n",
    "# Evaluate the model over the raster cube\n",
    "rop = ela.geo.apply_model_to_array(model_train, array, nodata, nodata_idx)\n",
    "\n",
    "# Attach the predicted probability map to the dataset under variable 'rop'\n",
    "training_output['rop'] = (('band', 'y', 'x'), rop)\n",
    "training_output['rop'].attrs['long_name'] = \"relative occurrence probability\"\n",
    "\n",
    "# write model output to netcdf\n",
    "training_output.to_netcdf(os.path.join(exp_path, nc_name))\n",
    "\n",
    "# write model predictions to raster\n",
    "ela.apply_model_to_rasters(model_train, raster_paths, os.path.join(exp_path, raster_name), quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4693e-7362-425b-bcdd-8ffa309932a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training performance metrics\n",
    "# - ROC-AUC (unweighted and weighted by sample weights)\n",
    "# - PR-AUC (unweighted and weighted)\n",
    "\n",
    "# ROC-curve inputs (kept commented; AUC via direct score API below)\n",
    "# fpr_train, tpr_train, thresholds = metrics.roc_curve(y_train, y_train_predict)\n",
    "\n",
    "auc_train = metrics.roc_auc_score(y_train, y_train_predict)\n",
    "auc_train_weighted = metrics.roc_auc_score(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "\n",
    "# PR-curve\n",
    "precision_train, recall_train, _= metrics.precision_recall_curve(y_train, y_train_predict) \n",
    "pr_auc_train = metrics.auc(recall_train, precision_train)\n",
    "precision_train_w, recall_train_w, _= metrics.precision_recall_curve(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "pr_auc_train_weighted = metrics.auc(recall_train, precision_train)\n",
    "\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score  : {auc_train_weighted:0.3f}\")\n",
    "print(f\"PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810f6d4-723c-45db-b906-ee32e705b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Permutation_importace_Plot\")\n",
    "# model_train.permutation_importance_plot(x_train, y_train, sample_weight=sample_weight_train, n_repeats=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993984c-f068-4953-8a73-841a09f30b72",
   "metadata": {},
   "source": [
    "## 2. Predict model for testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674151b7-99a5-43e3-9220-ff76066070cf",
   "metadata": {},
   "source": [
    "### 2.1 load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c5392-3677-4518-8cba-4375bb9e36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.1 load test data\")\n",
    "\n",
    "# Rebuild predictor stack for the test/interest region\n",
    "rasters, labels = (\n",
    "    (['srtm_%s.tif' % training], ['srtm']) if topo else ([], [])\n",
    ")\n",
    "rasters, labels = (\n",
    "    rasters + (['ndvi_%s.tif' % training] if ndvi else []),\n",
    "    labels  + (['ndvi'] if ndvi else [])\n",
    ")\n",
    "\n",
    "# Templates for interest region or future runs are kept for reference above\n",
    "\n",
    "# Add selected bioclim variables for the test prediction stack\n",
    "for no in bioclim:\n",
    "    rasters.append('%s_bio_%s_%s.tif' %(model_prefix, no, training))\n",
    "    labels.append('%s_bioclim_%02d' %(model_prefix, no))\n",
    "\n",
    "# Full paths to geotiffs to be opened\n",
    "raster_paths = [os.path.join(input_path, raster) for raster in rasters]\n",
    "\n",
    "# initialise dataset for model output\n",
    "# Each opened raster becomes a variable in the dataset keyed by label\n",
    "test_output = xr.Dataset()\n",
    "for raster, label in zip(raster_paths, labels):\n",
    "    da = rioxr.open_rasterio(raster, masked=True)\n",
    "    test_output[label] = da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89635dc6-33a8-4211-9b49-355940d49921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build filenames for test presence/background and export name\n",
    "presence_file_name = '%s_presence_%s_%s.csv' %(specie, interest, iteration)\n",
    "background_file_name = '%s_background_%s_%s.csv' %(specie, pseudoabsence, interest)\n",
    "# Export name for merged test table\n",
    "test_input_data_name = '%s_model-test_input-data_%s_%s_%s_%s_%s.csv' %(model_prefix, specie, pseudoabsence, interest, bio, iteration)\n",
    "\n",
    "#####----------------------------\n",
    "# if Future:\n",
    "#     presence_csv = pd.read_csv(os.path.join(input_path, 'test', presence_file_name))\n",
    "#     geometry = gpd.points_from_xy(presence_csv['lon'], presence_csv['lat'])\n",
    "#     presence_gdf = gpd.GeoDataFrame(geometry=geometry, crs='EPSG:4326')\n",
    "#     print(presence_gdf.geometry)\n",
    "#     print(presence_gdf)\n",
    "#     presence_gdf = presence_gdf.drop(presence_gdf.index)\n",
    "    \n",
    "# else:\n",
    "# Load point data for the interest region and convert to GeoDataFrames\n",
    "presence_csv = pd.read_csv(os.path.join(input_path, 'test', presence_file_name))\n",
    "geometry = gpd.points_from_xy(presence_csv['lon'], presence_csv['lat'])\n",
    "presence_gdf = gpd.GeoDataFrame(geometry=geometry, crs='EPSG:4326')\n",
    "        \n",
    "background_csv = pd.read_csv(os.path.join(input_path, 'test', background_file_name))\n",
    "geometry = gpd.points_from_xy(background_csv['lon'], background_csv['lat'])\n",
    "background_gdf = gpd.GeoDataFrame(geometry=geometry, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d79056-5e1e-4ac8-b2ad-94748ef3be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle edge-case: no presence points in interest region\n",
    "# Downstream code expects at least one point, so insert a dummy coordinate\n",
    "if len(presence_gdf) == 0:\n",
    "    print('There are no occurrences of this specie in this region!')\n",
    "    # chose arbitrary coordinate to make the code run (lon=115, lat=0)\n",
    "    presence_gdf.geometry = gpd.points_from_xy([115], [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefb62e-ee35-45b2-bcd6-086b9b8afbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate predictors at test presence points\n",
    "presence_test = ela.annotate(\n",
    "    presence_gdf.geometry,\n",
    "    raster_paths=raster_paths, \n",
    "    labels=labels, \n",
    "    drop_na=True,\n",
    ")\n",
    "\n",
    "# Distance-based sample weights for presence (use all neighbors)\n",
    "presence_test['SampleWeight'] = ela.distance_weights(presence_test, n_neighbors=-1)\n",
    "# Quick visual check of presence weights\n",
    "presence_test.plot(column='SampleWeight', legend=True, cmap='YlOrBr')\n",
    "\n",
    "# Annotate predictors at test background points\n",
    "background_test = ela.annotate(\n",
    "    background_gdf,  # supports different biasing strategies upstream\n",
    "    raster_paths=raster_paths, \n",
    "    labels=labels, \n",
    "    drop_na=True,\n",
    ")\n",
    "\n",
    "# Nearest-neighbor based weights for background\n",
    "background_test['SampleWeight'] = ela.distance_weights(background_test, n_neighbors=1)\n",
    "\n",
    "# Merge presence and background with binary class label\n",
    "test = ela.stack_geodataframes(\n",
    "    presence_test,\n",
    "    background_test,\n",
    "    add_class_label=True,\n",
    ")\n",
    "\n",
    "# Save merged test table for reproducibility\n",
    "__ = test.to_csv(os.path.join(exp_path, test_input_data_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f67ff-84d7-4e34-8b2d-b7740f6f49b8",
   "metadata": {},
   "source": [
    "### 2.2 model predict for region of interest/ test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b889a-5e5b-4a94-9b91-d26f10453687",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_name = '%s_model-test_%s_%s_%s_%s_%s.nc' %(model_prefix, specie, pseudoabsence, interest, bio, iteration)\n",
    "raster_name = '%s_model-test_%s_%s_%s_%s_%s.tif' %(model_prefix, specie, pseudoabsence, interest, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd517e-431b-42ce-8bdd-d84205318c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.2 model predict for region of interest/ test data\")\n",
    "\n",
    "# Split features/labels/weights for test set\n",
    "# Remove non-feature columns and extract target + weights\n",
    "x_test = test.drop(columns=['class', 'SampleWeight', 'geometry'])\n",
    "y_test = test['class']\n",
    "sample_weight_test = test['SampleWeight']\n",
    "\n",
    "# Predict probabilities for test samples (tabular points)\n",
    "y_test_predict = model_train.predict(x_test)\n",
    "# y_test_predict = np.nan_to_num(y_test_predict, nan=0.5)\n",
    "\n",
    "# Apply trained model across raster stack to produce a probability map\n",
    "# Convert the predictor stack to 3D array [band, y, x]\n",
    "array = test_output.isel(band=0).to_array().values\n",
    "nodata = np.nan\n",
    "nodata_idx = np.isnan(array)\n",
    "rop = ela.geo.apply_model_to_array(model_train, array, nodata, nodata_idx)\n",
    "\n",
    "# Store map into xarray and annotate\n",
    "_test_da = (('band', 'y', 'x'), rop)\n",
    "test_output['rop'] = _test_da\n",
    "test_output['rop'].attrs['long_name'] = \"relative occurrence probability\"\n",
    "\n",
    "# Persist outputs\n",
    "# NetCDF: multi-variable dataset, GeoTIFF: single-band map\n",
    "test_output.to_netcdf(os.path.join(exp_path, nc_name))\n",
    "ela.apply_model_to_rasters(model_train, raster_paths, os.path.join(exp_path, raster_name), quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9a5b5-2e20-495c-b74c-a43cd01dcf61",
   "metadata": {},
   "source": [
    "### 2.3 test data performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b6561-543d-46cf-8167-878dd0f0bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.3 test data performance\")\n",
    "# ROC-curve\n",
    "# fpr_test, tpr_test, _ = metrics.roc_curve(y_test, y_test_predict)\n",
    "auc_test = metrics.roc_auc_score(y_test, y_test_predict)\n",
    "auc_test_weighted = metrics.roc_auc_score(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "\n",
    "# PR-curve\n",
    "precision_test, recall_test, _= metrics.precision_recall_curve(y_test, y_test_predict) \n",
    "pr_auc_test = metrics.auc(recall_test, precision_test)\n",
    "precision_test_w, recall_test_w, _= metrics.precision_recall_curve(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "pr_auc_test_weighted = metrics.auc(recall_test_w, precision_test_w)\n",
    "\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score: {auc_train_weighted:0.3f}\")\n",
    "print(f\"Training PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"Training PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")\n",
    "\n",
    "print(f\"Test ROC-AUC score: {auc_test:0.3f}\")\n",
    "print(f\"Test ROC-AUC Weighted score: {auc_test_weighted:0.3f}\")\n",
    "print(f\"Test PR-AUC Score: {pr_auc_test:0.3f}\")\n",
    "print(f\"Test PR-AUC Weighted Score: {pr_auc_test_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1e0dc-05c9-4a65-809f-42b1055ff5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=False\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 8), constrained_layout=True) #dpi=100\n",
    "\n",
    "cmap = plt.cm.GnBu #'GnBu'\n",
    "bounds = np.linspace(0, 1, 11)\n",
    "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# pcol = train_out.plot(ax=ax[0], vmin=0, vmax=1, cmap='GnBu', add_colorbar=False)\n",
    "gdf_countries.plot(ax=ax[0], facecolor='lightgray', edgecolor='k')\n",
    "pcol = training_output.rop.plot(ax=ax[0], vmin=0, vmax=1, norm=norm, cmap=cmap, add_colorbar=False)\n",
    "presence_train.plot(ax=ax[0], color='tab:red', marker='*', label='presence-train')\n",
    "ax[0].text(0.98, 0.91, 'presence points: %s\\n pseudo-absence points: %s\\n AUC: %.3f' %(len(presence_train), len(background_train), auc_train_weighted), fontsize=12, \n",
    "           horizontalalignment='right', verticalalignment='top', transform=ax[0].transAxes)\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[0].set_title('')\n",
    "#ax[0].set_ylim([15, 60])\n",
    "\n",
    "gdf_countries.plot(ax=ax[1], facecolor='lightgray', edgecolor='k')\n",
    "pcol = test_output.rop.plot(ax=ax[1], vmin=0, vmax=1, norm=norm, cmap=cmap, add_colorbar=False)\n",
    "presence_test.plot(ax=ax[1], color='tab:red', marker='*', label='presence-test')\n",
    "ax[1].text(0.98, 0.92, 'presence points: %s\\n pseudo-absence points: %s\\n AUC: %.3f' %(len(presence_test), len(background_test), auc_test_weighted), fontsize=12, \n",
    "           horizontalalignment='right', verticalalignment='top', transform=ax[1].transAxes)\n",
    "ax[1].legend()\n",
    "ax[1].set_title('')\n",
    "\n",
    "cbar = fig.colorbar(pcol, ax=ax, aspect=50, pad=0.05, label=\"relative occurrence probability\", orientation='horizontal', fraction=0.03)\n",
    "\n",
    "if doc:\n",
    "    ax[0].axis('off')\n",
    "    ax[1].axis('off')\n",
    "\n",
    "if savefig:\n",
    "    fig.savefig(os.path.join(docs_path, '05_rel-occ-prob-%s_%s_%s_%s_%s.png' %(specie, training, bio, model_prefix, iteration)), transparent=True, bbox_inches='tight')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e7fd3-bb04-4dcc-87e1-77826ef8fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if savefig:\n",
    "    # If a model is specified, add it to the filename\n",
    "    file_path = os.path.join(figs_path, '05_rel-occ-prob-%s_%s_%s_%s_%s.png' %(specie, training, bio, model_prefix, iteration))\n",
    "    fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef88b46-5d31-498a-b827-8ae979468c36",
   "metadata": {},
   "source": [
    "### 2.4 load Future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d184fd-5f95-47c4-92ff-b3bdc6ee0bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.4 Future Data\")\n",
    "\n",
    "rasters, labels = (\n",
    "    (['srtm_%s.tif' % training], ['srtm']) if topo else ([], [])\n",
    ")\n",
    "rasters, labels = (\n",
    "    rasters + (['ndvi_%s.tif' % training] if ndvi else []),\n",
    "    labels  + (['ndvi'] if ndvi else [])\n",
    ")\n",
    "# rasters, labels = (['srtm_%s.tif' %interest], ['srtm']) if topo else ([], []) # 'ndvi_east-asia.tif', 'ndvi' # \n",
    "\n",
    "for no in bioclim:\n",
    "    rasters.append('%s_bio_%s_%s_future.tif' %(model_prefix, no, training))\n",
    "    labels.append('%s_bioclim_%02d' %(model_prefix, no))\n",
    "    \n",
    "raster_paths = [os.path.join(input_path, raster) for raster in rasters]\n",
    "\n",
    "# initialise dataset for model output\n",
    "# future_output = xr.Dataset()\n",
    "# for raster, label in zip(raster_paths, labels):\n",
    "#     da = rioxr.open_rasterio(raster, masked=True).squeeze(drop=True)  \n",
    "#     future_output[label] = da\n",
    "\n",
    "\n",
    "arrays = []\n",
    "for raster in raster_paths:\n",
    "    da = rioxr.open_rasterio(raster, masked=True)\n",
    "    arr = da.squeeze().values  # pastikan 2D\n",
    "    arrays.append(arr)\n",
    "\n",
    "\n",
    "nc_name = '%s_model-future_%s_%s_%s_%s_%s.nc' %(model_prefix, specie, pseudoabsence, interest, bio, iteration)\n",
    "raster_name = '%s_model-future_%s_%s_%s_%s_%s.tif' %(model_prefix, specie, pseudoabsence, interest, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71201d62-c6d7-454b-bd66-8374d5f29b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.stack(arrays, axis=-1)  # shape: (rows, cols, features)\n",
    "nrow, ncol, nfeat = stacked.shape\n",
    "\n",
    "X_future = stacked.reshape(-1, nfeat)\n",
    "\n",
    "y_pred = model_train.predict(X_future)  # probabilitas presence\n",
    "# y_pred = np.nan_to_num(y_pred, nan=0.5)\n",
    "\n",
    "# Reshape raster 2D\n",
    "y_map = y_pred.reshape(nrow, ncol)\n",
    "\n",
    "future_output = xr.Dataset()\n",
    "for raster, label in zip(raster_paths, labels):\n",
    "    da = rioxr.open_rasterio(raster, masked=True)\n",
    "    future_output[label] = da\n",
    "\n",
    "future_output['rop'] = (('band', 'y', 'x'), y_map[np.newaxis, :, :])\n",
    "future_output['rop'].attrs['long_name'] = \"relative occurrence probability\"\n",
    "\n",
    "# --- 7. Simpan ke NetCDF ---\n",
    "future_output.to_netcdf(os.path.join(exp_path, nc_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237f913-c147-4208-85ad-6aedfedfd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=False\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(24, 8), constrained_layout=True) #dpi=100\n",
    "\n",
    "cmap = plt.cm.GnBu #'GnBu'\n",
    "bounds = np.linspace(0, 1, 11)\n",
    "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# pcol = train_out.plot(ax=ax[0], vmin=0, vmax=1, cmap='GnBu', add_colorbar=False)\n",
    "gdf_countries.plot(ax=ax[0], facecolor='lightgray', edgecolor='k')\n",
    "pcol = training_output.rop.plot(ax=ax[0], vmin=0, vmax=1, norm=norm, cmap=cmap, add_colorbar=False)\n",
    "presence_train.plot(ax=ax[0], color='tab:red', marker='*', label='presence-train')\n",
    "ax[0].text(0.98, 0.91, 'presence points: %s\\n pseudo-absence points: %s\\n AUC: %.3f' %(len(presence_train), len(background_train), auc_train_weighted), fontsize=12, \n",
    "           horizontalalignment='right', verticalalignment='top', transform=ax[0].transAxes)\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[0].set_title('')\n",
    "#ax[0].set_ylim([15, 60])\n",
    "\n",
    "gdf_countries.plot(ax=ax[1], facecolor='lightgray', edgecolor='k')\n",
    "pcol = test_output.rop.plot(ax=ax[1], vmin=0, vmax=1, norm=norm, cmap=cmap, add_colorbar=False)\n",
    "presence_test.plot(ax=ax[1], color='tab:red', marker='*', label='presence-test')\n",
    "ax[1].text(0.98, 0.92, 'presence points: %s\\n pseudo-absence points: %s\\n AUC: %.3f' %(len(presence_test), len(background_test), auc_test_weighted), fontsize=12, \n",
    "           horizontalalignment='right', verticalalignment='top', transform=ax[1].transAxes)\n",
    "ax[1].legend()\n",
    "ax[1].set_title('')\n",
    "\n",
    "gdf_countries.plot(ax=ax[2], facecolor='lightgray', edgecolor='k')\n",
    "pcol = future_output.rop.plot(ax=ax[2], vmin=0, vmax=1, norm=norm, cmap=cmap, add_colorbar=False)\n",
    "ax[2].set_title('Future prediction')\n",
    "\n",
    "cbar = fig.colorbar(pcol, ax=ax, aspect=50, pad=0.05, label=\"relative occurrence probability\", orientation='horizontal', fraction=0.03)\n",
    "\n",
    "if doc:\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "\n",
    "if savefig:\n",
    "    fig.savefig(os.path.join(docs_path, '05_rel-occ-prob-%s_%s_%s_%s_%s_future.png' %(specie, training, bio, model_prefix, iteration)), transparent=True, bbox_inches='tight')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761df818-12ae-4b6f-94f7-e9fcff96b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "if savefig:\n",
    "    # If a model is specified, add it to the filename\n",
    "    file_path = os.path.join(figs_path, '05_rel-occ-prob-%s_%s_%s_%s_%s_future.png' %(specie, training, bio, model_prefix, iteration))\n",
    "    fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47edf884-fbce-4d1c-82cf-f446e03030b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# --- ASSUMPTIONS: These variables already exist from previous code ---\n",
    "# training_output.rop: xarray DataArray for training results\n",
    "# future_output.rop: xarray DataArray for future/test results\n",
    "# gdf_countries: GeoDataFrame for the background map\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "## 1. Calculate the Difference\n",
    "# We cannot directly subtract two xarray objects if their grids/coordinates differ.\n",
    "# Use .reindex_like() to align 'future_output' to 'training_output'.\n",
    "# Areas outside the coverage of 'future_output' will become NaN.\n",
    "# future_aligned = future_output.rop.reindex_like(training_output.rop)\n",
    "\n",
    "# Now we can safely subtract them\n",
    "# Formula: Future Prediction - Historical/Training Prediction\n",
    "difference = future_output.rop - training_output.rop\n",
    "\n",
    "## 2. Prepare Visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8), constrained_layout=True)\n",
    "\n",
    "# Choose a diverging colormap (centered around zero).\n",
    "# 'coolwarm' (Blue-White-Red) or 'RdBu_r' works well.\n",
    "# Blue = negative values, Red = positive values, White = zero.\n",
    "cmap_diff = plt.cm.coolwarm\n",
    "\n",
    "# Create a normalization centered at zero.\n",
    "# This ensures that value 0 appears as white (neutral).\n",
    "norm_diff = mcolors.CenteredNorm()\n",
    "\n",
    "## 3. Plot the Difference\n",
    "# Plot all countries as background\n",
    "gdf_countries.plot(ax=ax, facecolor='lightgray', edgecolor='k')\n",
    "\n",
    "# Plot raster difference on top\n",
    "# xarray will automatically ignore NaN values\n",
    "diff_plot = difference.plot(ax=ax, cmap=cmap_diff, norm=norm_diff, vmin=-1, vmax=1, add_colorbar=False)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(diff_plot, ax=ax, orientation='vertical', label=\"Relative Occurrence Probability (Future - Historical)\")\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Change in Potentially Suitable Areas', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51838f39-9f3b-493c-8534-ed625f4df697",
   "metadata": {},
   "outputs": [],
   "source": [
    "if savefig:\n",
    "    # If a model is specified, add it to the filename\n",
    "    file_path = os.path.join(figs_path, '05_rel-occ-dif-%s_%s_%s_%s_%s.png' %(specie, training, bio, model_prefix, iteration))\n",
    "    fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf9622-87c1-4363-922d-04939e2028f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output.close()\n",
    "future_output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aciar",
   "language": "python",
   "name": "aciar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
