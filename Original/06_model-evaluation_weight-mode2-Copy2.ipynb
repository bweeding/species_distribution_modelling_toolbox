{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a39a75e",
   "metadata": {},
   "source": [
    "# Model Evaluation (Weighted) â€“ Notebook Guide\n",
    "\n",
    "This notebook evaluates models with class/observation weights applied.\n",
    "\n",
    "## What this notebook does\n",
    "- Compute weighted metrics (e.g., weighted AUC, threshold metrics)\n",
    "- Plot diagnostic figures considering weights\n",
    "- Summarize results per model/run and export\n",
    "\n",
    "## Inputs\n",
    "- Predictions/scores, ground-truth labels, and weights per observation\n",
    "- Optional: CV fold info or test set indicators\n",
    "\n",
    "## Workflow\n",
    "1. Load predictions, labels, and weights\n",
    "2. Validate alignment and handle missing values\n",
    "3. Compute weighted metrics across thresholds/folds\n",
    "4. Plot weighted ROC/curves and summaries\n",
    "5. Save metrics tables and figures\n",
    "\n",
    "## Outputs\n",
    "- Weighted per-model/per-fold metrics tables\n",
    "- Plots reflecting weights\n",
    "- CSV/JSON exports for downstream use\n",
    "\n",
    "## Notes\n",
    "- Ensure weights are normalized or in intended scale\n",
    "- Use consistent preprocessing as training\n",
    "- Fix random seeds for reproducibility where applicable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce62eb",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook evaluates weighted SDMs with metrics and plots, mirroring standard evaluation but accounting for weights in analysis where relevant.\n",
    "\n",
    "- Key steps: load weighted predictions, compute metrics, plot curves, thresholds, reporting\n",
    "- Inputs: weighted model predictions and labels\n",
    "- Outputs: evaluation tables and plots\n",
    "- Run order: After weighted model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b15204-7112-48a4-97c4-bc90bc40550d",
   "metadata": {},
   "source": [
    "# Weighted MaxEnt Model Evaluation and Performance Assessment\n",
    "\n",
    "This notebook provides comprehensive evaluation of **weighted MaxEnt species distribution models**, focusing on performance assessment that accounts for sample weights and data quality differences. Unlike standard model evaluation, this version incorporates **weighted metrics** to properly assess model performance when training data has been weighted.\n",
    "\n",
    "## Key Features of Weighted Model Evaluation:\n",
    "\n",
    "### 1. **Weighted Performance Metrics**:\n",
    "- **Weighted AUC**: Area Under ROC Curve accounting for sample weights\n",
    "- **Weighted PR-AUC**: Precision-Recall AUC with weight integration\n",
    "- **Weighted Sensitivity/Specificity**: Performance metrics adjusted for data quality\n",
    "- **Weighted Precision/Recall**: Classification metrics incorporating sample weights\n",
    "\n",
    "### 2. **Advanced Evaluation Approaches**:\n",
    "- **Cross-Validation**: K-fold validation with weighted samples\n",
    "- **Spatial Validation**: Geographic partitioning with weight consideration\n",
    "- **Temporal Validation**: Time-based splits accounting for temporal weights\n",
    "- **Bootstrap Validation**: Resampling with weight preservation\n",
    "\n",
    "### 3. **Bias Assessment**:\n",
    "- **Spatial Bias Analysis**: Evaluate model performance across different regions\n",
    "- **Temporal Bias Assessment**: Performance across different time periods\n",
    "- **Source Bias Evaluation**: Performance across different data sources\n",
    "- **Quality Bias Analysis**: Performance across different data quality levels\n",
    "\n",
    "## Applications:\n",
    "- **Model Validation**: Comprehensive assessment of weighted model performance\n",
    "- **Bias Detection**: Identify remaining biases after weighting\n",
    "- **Performance Comparison**: Compare weighted vs. unweighted models\n",
    "- **Quality Control**: Validate that weighting improves model reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f4dda-d3ca-47d5-91ad-7caa0a434170",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### WEIGHTED MODEL EVALUATION CONFIGURATION - MODIFY AS NEEDED ###############\n",
    "\n",
    "# Species and region settings for weighted model evaluation\n",
    "#specie = 'leptocybe-invasa'  # Target species: 'leptocybe-invasa' or 'thaumastocoris-peregrinus'\n",
    "#pseudoabsence = 'random'  # Background point strategy: 'random', 'biased', 'biased-land-cover'\n",
    "#training = 'east-asia'  # Training region: 'sea', 'australia', 'east-asia', etc.\n",
    "#interest = 'south-east-asia'  # Test region: can be same as training or different\n",
    "#savefig = True  # Save generated evaluation plots and metrics\n",
    "\n",
    "# Environmental variable configuration\n",
    "bio = bio1  # Bioclimatic variable identifier\n",
    "\n",
    "# Evaluation settings (specific to weighted model evaluation)\n",
    "# evaluation_method = 'cross_validation'  # 'cross_validation', 'spatial_validation', 'temporal_validation'\n",
    "# n_folds = 5  # Number of folds for cross-validation\n",
    "# spatial_buffer = 100  # Buffer distance (km) for spatial validation\n",
    "# temporal_split = 0.7  # Proportion of data for training in temporal validation\n",
    "\n",
    "# Weighted metrics configuration\n",
    "# include_weighted_metrics = True  # Calculate weighted performance metrics\n",
    "# include_unweighted_metrics = True  # Calculate standard metrics for comparison\n",
    "# weight_threshold = 0.1  # Minimum weight threshold for sample inclusion\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e46ce-499c-4676-9ff0-f796122a3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os  # File system operations\n",
    "\n",
    "import numpy as np  # Numerical computing\n",
    "import xarray as xr  # Multi-dimensional labeled arrays (raster data)\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import geopandas as gpd  # Geospatial data handling\n",
    "\n",
    "import elapid as ela  # Species distribution modeling library\n",
    "\n",
    "from shapely import wkt  # Well-Known Text (WKT) geometry parsing\n",
    "from elapid import utils  # Utility functions for elapid\n",
    "from sklearn import metrics, inspection  # Machine learning metrics and model inspection\n",
    "\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warning messages for cleaner output\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6724e-cd4f-4099-aba5-4b81214f135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_layout(nplots):\n",
    "    \"\"\"\n",
    "    Calculate optimal subplot layout for given number of plots\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nplots : int\n",
    "        Number of plots to arrange\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ncols, nrows : tuple\n",
    "        Number of columns and rows for subplot layout\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate square root and round up for balanced layout\n",
    "    ncols = min(int(np.ceil(np.sqrt(nplots))), 4)  # Max 4 columns\n",
    "    nrows = int(np.ceil(nplots / ncols))  # Calculate rows needed\n",
    "    \n",
    "    return ncols, nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6db49b-be58-4919-b395-1e6978805f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SET UP FILE PATHS\n",
    "# =============================================================================\n",
    "# Define directory structure for organizing weighted model evaluation outputs\n",
    "\n",
    "docs_path = os.path.join(os.path.dirname(os.getcwd()), 'docs')  # Documentation directory\n",
    "out_path = os.path.join(os.path.dirname(os.getcwd()), 'out', specie)  # Species-specific output directory\n",
    "figs_path = os.path.join(os.path.dirname(os.getcwd()), 'figs')  # Figures directory\n",
    "output_path = os.path.join(out_path, 'output')  # Model output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7a900-85cd-41d4-87e6-7179c9320233",
   "metadata": {},
   "source": [
    "## 1. Weighted Training Model Performance Assessment\n",
    "\n",
    "This section evaluates the performance of the weighted MaxEnt model on the training data. Key aspects include:\n",
    "\n",
    "### **Weighted vs. Unweighted Metrics**:\n",
    "- **Standard Metrics**: Traditional AUC, PR-AUC, sensitivity, specificity\n",
    "- **Weighted Metrics**: Performance metrics accounting for sample weights\n",
    "- **Comparison Analysis**: Evaluate improvement from weighting approach\n",
    "\n",
    "### **Performance Indicators**:\n",
    "- **ROC-AUC**: Area Under Receiver Operating Characteristic curve\n",
    "- **PR-AUC**: Area Under Precision-Recall curve (important for imbalanced data)\n",
    "- **Sensitivity**: True Positive Rate (ability to detect presences)\n",
    "- **Specificity**: True Negative Rate (ability to detect absences)\n",
    "- **Precision**: Positive Predictive Value\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### **Weighted Evaluation Benefits**:\n",
    "- **Quality-Aware Assessment**: Metrics reflect data quality differences\n",
    "- **Bias-Corrected Performance**: Reduced influence of low-quality samples\n",
    "- **Robust Validation**: More reliable performance estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17562db-88e4-4b2c-97d4-6095e6fd3e7b",
   "metadata": {},
   "source": [
    "## References for Species Distribution Model Evaluation\n",
    "\n",
    "### **Model Output Interpretation**:\n",
    "- [SDM Model Outputs Interpretation](https://support.ecocommons.org.au/support/solutions/articles/6000256107-interpretation-of-sdm-model-outputs)\n",
    "- [Presence-Only Prediction in GIS](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/how-presence-only-prediction-works.htm)\n",
    "- [MaxEnt 101: Species Distribution Modeling](https://www.esri.com/arcgis-blog/products/arcgis-pro/analytics/presence-only-prediction-maxent-101-using-gis-to-model-species-distribution/)\n",
    "\n",
    "### **Performance Metrics**:\n",
    "- [ROC Curves Demystified](https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0)\n",
    "- [Precision-Recall AUC Guide](https://www.aporia.com/learn/ultimate-guide-to-precision-recall-auc-understanding-calculating-using-pr-auc-in-ml/)\n",
    "- [F1-Score, Accuracy, ROC-AUC, and PR-AUC Metrics](https://deepchecks.com/f1-score-accuracy-roc-auc-and-pr-auc-metrics-for-models/)\n",
    "\n",
    "### **Weighted Model Evaluation**:\n",
    "- **Sample Weighting**: How to properly evaluate models trained with sample weights\n",
    "- **Bias Correction**: Assessing the effectiveness of weighting strategies\n",
    "- **Quality Integration**: Incorporating data quality into performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa4635-ab08-4b3d-9fa0-07271788cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD WEIGHTED MODEL AND TRAINING DATA\n",
    "# =============================================================================\n",
    "# Load the trained weighted MaxEnt model and associated training data for evaluation\n",
    "\n",
    "# Build experiment directory name (keeps runs organized by config)\n",
    "# Alternate naming (older): 'exp_%s_%s_%s' % (pseudoabsence, training, interest)\n",
    "experiment_name = 'exp_%s_%s_%s_%s_%s' % (model_prefix, pseudoabsence, training, topo, ndvi)\n",
    "exp_path = os.path.join(output_path, experiment_name)  # Path to experiment directory\n",
    "\n",
    "# Construct expected filenames produced during training for this run\n",
    "train_input_data_name = '%s_model-train_input-data_%s_%s_%s_%s_%s.csv' % (model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "run_name = '%s_model-train_%s_%s_%s_%s_%s.ela' % (model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "nc_name = '%s_model-train_%s_%s_%s_%s_%s.nc' % (model_prefix, specie, pseudoabsence, training, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428443d1-2a5b-403e-a99c-a3395954e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD TRAINING DATA WITH SAMPLE WEIGHTS\n",
    "# =============================================================================\n",
    "# Load training data including sample weights for weighted model evaluation\n",
    "\n",
    "# Load training data from CSV file (index_col=0 to drop old index column)\n",
    "df = pd.read_csv(os.path.join(exp_path, train_input_data_name), index_col=0)\n",
    "# Parse WKT strings into shapely geometries\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# Wrap as GeoDataFrame with WGS84 CRS\n",
    "train = gpd.GeoDataFrame(df, crs='EPSG:4326')\n",
    "\n",
    "# Split predictors/labels/weights for weighted evaluation\n",
    "x_train = train.drop(columns=['class', 'SampleWeight', 'geometry'])  # Environmental variables only\n",
    "y_train = train['class']  # Presence/absence labels (0/1)\n",
    "sample_weight_train = train['SampleWeight']  # Sample weights aligned with rows\n",
    "\n",
    "# Load fitted weighted MaxEnt model\n",
    "model_train = utils.load_object(os.path.join(exp_path, run_name))\n",
    "\n",
    "# Predict probabilities on training set (for curves/metrics)\n",
    "y_train_predict = model_train.predict(x_train)\n",
    "# Optional: impute NaN probabilities to 0.5 (neutral)\n",
    "# y_train_predict = np.nan_to_num(y_train_predict, nan=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abc1a9-3db2-4960-ad8f-e042eb214fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training performance metrics\n",
    "\n",
    "# ROC curve and AUC (unweighted vs weighted)\n",
    "# fpr/tpr are computed from predicted probabilities; weights adjust contribution per sample\n",
    "fpr_train, tpr_train, thresholds = metrics.roc_curve(y_train, y_train_predict)\n",
    "auc_train = metrics.roc_auc_score(y_train, y_train_predict)\n",
    "auc_train_weighted = metrics.roc_auc_score(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "\n",
    "# Precision-Recall curve and PR-AUC (more informative on class imbalance)\n",
    "precision_train, recall_train, _ = metrics.precision_recall_curve(y_train, y_train_predict)\n",
    "pr_auc_train = metrics.auc(recall_train, precision_train)\n",
    "# Weighted PR curve uses sample weights to compute precision/recall\n",
    "precision_train_w, recall_train_w, _ = metrics.precision_recall_curve(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "pr_auc_train_weighted = metrics.auc(recall_train_w, precision_train_w)\n",
    "\n",
    "# Report metrics\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score  : {auc_train_weighted:0.3f}\")\n",
    "print(f\"PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cccf4-2b98-44d3-8725-227a49bb3c31",
   "metadata": {},
   "source": [
    "|  |  | Specie existance |  |\n",
    "| ------ | :-------: | :------: | :-------: |\n",
    "| |  | **+** | **--** |\n",
    "| **Specie observed** | **+** | True Positive (TP) | False Positive (FP) |\n",
    "| | **--** | False Negative (FN) | True Negative (TN) |\n",
    "| | | **All existing species (TP + FN)** | **All non-existing species (FP + TN)** |\n",
    "\n",
    "\n",
    "$$TPR = \\frac{TP}{TP + FN}$$\n",
    "$$FPR = \\frac{FP}{FP + TN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588ba66-5615-4d10-b8db-26ab26462e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training distributions and curves\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Left: Predicted probability distributions for presence vs pseudo-absence\n",
    "ax[0].hist(y_train_predict[y_train == 0], bins=np.linspace(0, 1, int((y_train == 0).sum() / 100 + 1)),\n",
    "           density=True, color='tab:red', alpha=0.7, label='pseudo-absence')\n",
    "ax[0].hist(y_train_predict[y_train == 1], bins=np.linspace(0, 1, int((y_train == 1).sum() / 10 + 1)),\n",
    "           density=True, color='tab:green', alpha=0.7, label='presence')\n",
    "ax[0].set_xlabel('Relative Occurrence Probability')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_title('Probability Distribution')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "# Middle: ROC curve (random vs perfect baselines + model)\n",
    "ax[1].plot([0, 1], [0, 1], '--', label='AUC score: 0.5 (No Skill)', color='gray')\n",
    "ax[1].text(0.4, 0.4, 'random classifier', fontsize=12, color='gray', rotation=45, rotation_mode='anchor',\n",
    "           horizontalalignment='left', verticalalignment='bottom', transform=ax[1].transAxes)\n",
    "ax[1].plot([0, 0, 1], [0, 1, 1], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[1].text(0, 1, '  perfect classifier', fontsize=12, color='tab:blue', horizontalalignment='left', verticalalignment='bottom')\n",
    "ax[1].scatter(0, 1, marker='*', s=100, color='tab:blue')\n",
    "# Overlay model ROC (unweighted and weighted AUC labels)\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC score: {auc_train:0.3f}', color='tab:orange')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC Weighted score: {auc_train_weighted:0.3f}', color='tab:cyan', linestyle='-.')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('MaxEnt ROC Curve')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Right: Precision-Recall curve (random/perfect baselines + model)\n",
    "ax[2].plot([0, 1], [0.5, 0.5], '--', color='gray', label='AUC score: 0.5 (No Skill)')\n",
    "ax[2].text(0.5, 0.52, 'random classifier', fontsize=12, color='gray', horizontalalignment='center', verticalalignment='center')\n",
    "ax[2].plot([0, 1, 1], [1, 1, 0], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[2].text(1, 1, 'perfect classifier  ', fontsize=12, color='tab:blue', horizontalalignment='right', verticalalignment='bottom')\n",
    "ax[2].scatter(1, 1, marker='*', s=100, color='tab:blue')\n",
    "# Overlay model PR curves (unweighted and weighted AUC labels)\n",
    "ax[2].plot(recall_train, precision_train, label=f'AUC score: {pr_auc_train:0.3f}', color='tab:orange')\n",
    "ax[2].plot(recall_train_w, precision_train_w, label=f\"AUC Weighted score: {pr_auc_train_weighted:0.3f}\", color='tab:cyan', linestyle='-.')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set_xlabel('Recall')\n",
    "ax[2].set_ylabel('Precision')\n",
    "ax[2].set_title('MaxEnt PR Curve')\n",
    "ax[2].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b30af-f85b-4419-baa9-ad808d2dfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figures if requested. Uses different filename patterns for current vs future scenarios.\n",
    "# Note: 'models' is used to gate inclusion of model prefix; ensure it exists in your session.\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:  # include model identifier when available\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: omit model prefix when not specified\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993984c-f068-4953-8a73-841a09f30b72",
   "metadata": {},
   "source": [
    "## 2. Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f358d3-224d-4be8-a1d2-82f1f3457b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data_name = '%s_model-test_input-data_%s_%s_%s_%s_%s.csv' %(model_prefix, specie, pseudoabsence, interest, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7166b-a73a-441c-a066-ec8e957e04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load held-out test dataset for evaluation\n",
    "# Note: index_col=0 drops the old index saved during export\n",
    "df = pd.read_csv(os.path.join(exp_path, test_input_data_name), index_col=0)\n",
    "# Convert WKT geometry back to shapely objects\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# Wrap as GeoDataFrame (WGS84 CRS)\n",
    "test = gpd.GeoDataFrame(df, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550482cb-b0da-489c-894d-711a890cd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split predictors/labels/weights for test set\n",
    "x_test = test.drop(columns=['class', 'SampleWeight', 'geometry'])\n",
    "y_test = test['class']\n",
    "sample_weight_test = test['SampleWeight']\n",
    "\n",
    "# Predict probabilities on the test set using the trained model\n",
    "y_test_predict = model_train.predict(x_test)\n",
    "# Optional: impute NaN probabilities to 0.5 if present\n",
    "# y_test_predict = np.nan_to_num(y_test_predict, nan=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c432c6-86ef-421f-a4fe-2f1cee794e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set metrics: ROC/PR curves and AUCs (unweighted vs weighted)\n",
    "# ROC\n",
    "fpr_test, tpr_test, _ = metrics.roc_curve(y_test, y_test_predict)\n",
    "auc_test = metrics.roc_auc_score(y_test, y_test_predict)\n",
    "auc_test_weighted = metrics.roc_auc_score(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "\n",
    "# Precision-Recall (PR)\n",
    "precision_test, recall_test, _ = metrics.precision_recall_curve(y_test, y_test_predict)\n",
    "pr_auc_test = metrics.auc(recall_test, precision_test)\n",
    "precision_test_w, recall_test_w, _ = metrics.precision_recall_curve(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "pr_auc_test_weighted = metrics.auc(recall_test_w, precision_test_w)\n",
    "\n",
    "# Print summary of training vs test for quick comparison\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score: {auc_train_weighted:0.3f}\")\n",
    "print(f\"Test ROC-AUC score: {auc_test:0.3f}\")\n",
    "print(f\"Test ROC-AUC Weighted score: {auc_test_weighted:0.3f}\")\n",
    "\n",
    "print(f\"Training PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"Training PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")\n",
    "print(f\"Test PR-AUC Score: {pr_auc_test:0.3f}\")\n",
    "print(f\"Test PR-AUC Weighted Score: {pr_auc_test_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22fa49-a574-4a0f-b998-4028ab09cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test distributions and curves alongside training for comparison\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Left: Predicted probability distributions on test set\n",
    "ax[0].hist(y_test_predict[y_test == 0], bins=np.linspace(0, 1, int((y_test == 0).sum() / 100 + 1)),\n",
    "           density=True, color='tab:red', alpha=0.7, label='pseudo-absence')\n",
    "ax[0].hist(y_test_predict[y_test == 1], bins=np.linspace(0, 1, int((y_test == 1).sum() / 10 + 1)),\n",
    "           density=True, color='tab:green', alpha=0.7, label='presence')\n",
    "ax[0].set_xlabel('Relative Occurrence Probability')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_title('Probability Distribution')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "# Middle: ROC curves (train vs test, with weighted variants labeled)\n",
    "ax[1].plot([0, 1], [0, 1], '--', label='AUC score: 0.5 (No Skill)', color='gray')\n",
    "ax[1].text(0.4, 0.4, 'random classifier', fontsize=12, color='gray', rotation=45, rotation_mode='anchor',\n",
    "           horizontalalignment='left', verticalalignment='bottom', transform=ax[1].transAxes)\n",
    "ax[1].plot([0, 0, 1], [0, 1, 1], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[1].text(0, 1, '  perfect classifier', fontsize=12, color='tab:blue', horizontalalignment='left', verticalalignment='bottom')\n",
    "ax[1].scatter(0, 1, marker='*', s=100, color='tab:blue')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC train score: {auc_train:0.3f}', color='tab:orange')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC Weighted train score: {auc_train_weighted:0.3f}', color='tab:cyan', linestyle='-.')\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'AUC test score: {auc_test:0.3f}', color='tab:green')\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'AUC Weighted test score: {auc_test_weighted:0.3f}', color='tab:olive', linestyle='-.')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('MaxEnt ROC Curve')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Right: PR curves (train vs test)\n",
    "ax[2].plot([0, 1], [0.5, 0.5], '--', color='gray', label='AUC score: 0.5 (No Skill)')\n",
    "ax[2].text(0.5, 0.52, 'random classifier', fontsize=12, color='gray', horizontalalignment='center', verticalalignment='center')\n",
    "ax[2].plot([0, 1, 1], [1, 1, 0], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[2].text(1, 1, 'perfect classifier  ', fontsize=12, color='tab:blue', horizontalalignment='right', verticalalignment='bottom')\n",
    "ax[2].scatter(1, 1, marker='*', s=100, color='tab:blue')\n",
    "ax[2].plot(recall_train, precision_train, label=f'AUC train score: {pr_auc_train:0.3f}', color='tab:orange')\n",
    "ax[2].plot(recall_train_w, precision_train_w, label=f\"AUC train Weighted score: {pr_auc_train_weighted:0.3f}\", color='tab:cyan', linestyle='-.')\n",
    "ax[2].plot(recall_test, precision_test, label=f'AUC test score: {pr_auc_test:0.3f}', color='tab:green')\n",
    "ax[2].plot(recall_test_w, precision_test_w, label=f'AUC test Weighted score: {pr_auc_test_weighted:0.3f}', color='tab:olive', linestyle='-.')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set_xlabel('Recall')\n",
    "ax[2].set_ylabel('Precision')\n",
    "ax[2].set_title('MaxEnt PR Curve')\n",
    "ax[2].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b39edc-a114-46ba-a329-503387ab4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test figures if requested (future vs current naming handled similarly to training)\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s_future.png' % (specie, interest, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_future.png' % (specie, interest, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if model_prefix:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s.png' % (specie, interest, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s.png' % (specie, interest, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bfd716-825b-4ce8-82aa-fce7f973fcf8",
   "metadata": {},
   "source": [
    "## 3. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6a70d-105b-4267-bc58-1df7bb8e8aab",
   "metadata": {},
   "source": [
    "### 3.2 Partial dependence plot/ Response curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c51647-a6b0-4329-a077-fb2d3d5aca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = model_train.partial_dependence_plot(x, labels=labels, dpi=100, n_bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeacf3a",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Variable Importance Analysis\n",
    "\n",
    "This section performs a thorough analysis of variable importance by:\n",
    "\n",
    "1. **Initial Analysis**: Running the model with all 19 bioclimatic variables to establish baseline importance\n",
    "2. **Iterative Removal**: Systematically removing the least important variables until we reach ~5 most important variables\n",
    "3. **Performance Tracking**: Monitoring model performance as variables are removed\n",
    "4. **Final Recommendations**: Identifying the optimal subset of variables for the species distribution model\n",
    "\n",
    "### Methodology:\n",
    "- **Permutation Importance**: Measures the drop in model performance when each variable is randomly shuffled\n",
    "- **Iterative Backward Elimination**: Removes least important variables one at a time\n",
    "- **Performance Monitoring**: Tracks AUC, PR-AUC, and other metrics throughout the process\n",
    "- **Cross-Validation**: Ensures robust importance estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b286503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE VARIABLE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Initialize storage for results\n",
    "importance_results = {}\n",
    "performance_history = {}\n",
    "variable_subsets = {}\n",
    "\n",
    "# Get current variable names from training data\n",
    "current_variables = list(x_train.columns)\n",
    "print(f\"Starting with {len(current_variables)} variables:\")\n",
    "print(f\"Variables: {current_variables}\")\n",
    "\n",
    "# Store initial performance metrics\n",
    "initial_metrics = {\n",
    "    'train_auc': auc_train,\n",
    "    'train_auc_weighted': auc_train_weighted,\n",
    "    'train_pr_auc': pr_auc_train,\n",
    "    'train_pr_auc_weighted': pr_auc_train_weighted,\n",
    "    'test_auc': auc_test,\n",
    "    'test_auc_weighted': auc_test_weighted,\n",
    "    'test_pr_auc': pr_auc_test,\n",
    "    'test_pr_auc_weighted': pr_auc_test_weighted\n",
    "}\n",
    "\n",
    "performance_history['all_variables'] = initial_metrics\n",
    "variable_subsets['all_variables'] = current_variables.copy()\n",
    "\n",
    "print(f\"\\nInitial Performance (All {len(current_variables)} variables):\")\n",
    "print(f\"Training AUC: {auc_train:.3f} (weighted: {auc_train_weighted:.3f})\")\n",
    "print(f\"Training PR-AUC: {pr_auc_train:.3f} (weighted: {pr_auc_train_weighted:.3f})\")\n",
    "print(f\"Test AUC: {auc_test:.3f} (weighted: {auc_test_weighted:.3f})\")\n",
    "print(f\"Test PR-AUC: {pr_auc_test:.3f} (weighted: {pr_auc_test_weighted:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46769501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ITERATIVE VARIABLE REMOVAL FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def iterative_variable_removal(x_train, y_train, sample_weight_train, x_test, y_test, sample_weight_test, \n",
    "                              target_variables=5, min_variables=3):\n",
    "    \"\"\"\n",
    "    Iteratively remove least important variables until reaching target number.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train, y_train, sample_weight_train : training data\n",
    "    x_test, y_test, sample_weight_test : test data  \n",
    "    target_variables : int, target number of variables to keep\n",
    "    min_variables : int, minimum number of variables to keep\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict, containing importance rankings and performance history\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'importance_rankings': {},\n",
    "        'performance_history': {},\n",
    "        'removed_variables': [],\n",
    "        'final_variables': []\n",
    "    }\n",
    "    \n",
    "    current_x_train = x_train.copy()\n",
    "    current_x_test = x_test.copy()\n",
    "    current_vars = list(current_x_train.columns)\n",
    "    iteration = 0\n",
    "    \n",
    "    print(f\"Starting iterative removal from {len(current_vars)} to {target_variables} variables...\")\n",
    "    \n",
    "    while len(current_vars) > max(target_variables, min_variables):\n",
    "        iteration += 1\n",
    "        print(f\"\\n--- Iteration {iteration}: {len(current_vars)} variables remaining ---\")\n",
    "        \n",
    "        # Train model with current variables\n",
    "        model_iter = ela.MaxentModel()\n",
    "        model_iter.fit(current_x_train, y_train, sample_weight=sample_weight_train)\n",
    "        \n",
    "        # Calculate permutation importance\n",
    "        pi = inspection.permutation_importance(\n",
    "            model_iter, current_x_train, y_train, \n",
    "            sample_weight=sample_weight_train, n_repeats=10\n",
    "        )\n",
    "        \n",
    "        # Get importance scores and rank variables\n",
    "        importance_scores = pi.importances.mean(axis=1)\n",
    "        var_importance = dict(zip(current_vars, importance_scores))\n",
    "        sorted_vars = sorted(var_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Store ranking for this iteration\n",
    "        results['importance_rankings'][f'iteration_{iteration}'] = {\n",
    "            'variables': current_vars.copy(),\n",
    "            'importance_scores': var_importance.copy(),\n",
    "            'sorted_ranking': sorted_vars.copy()\n",
    "        }\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        y_train_pred = model_iter.predict(current_x_train)\n",
    "        y_test_pred = model_iter.predict(current_x_test)\n",
    "        \n",
    "        # Training metrics\n",
    "        train_auc = metrics.roc_auc_score(y_train, y_train_pred)\n",
    "        train_auc_weighted = metrics.roc_auc_score(y_train, y_train_pred, sample_weight=sample_weight_train)\n",
    "        train_precision, train_recall, _ = metrics.precision_recall_curve(y_train, y_train_pred)\n",
    "        train_pr_auc = metrics.auc(train_recall, train_precision)\n",
    "        train_precision_w, train_recall_w, _ = metrics.precision_recall_curve(y_train, y_train_pred, sample_weight=sample_weight_train)\n",
    "        train_pr_auc_weighted = metrics.auc(train_recall_w, train_precision_w)\n",
    "        \n",
    "        # Test metrics\n",
    "        test_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "        test_auc_weighted = metrics.roc_auc_score(y_test, y_test_pred, sample_weight=sample_weight_test)\n",
    "        test_precision, test_recall, _ = metrics.precision_recall_curve(y_test, y_test_pred)\n",
    "        test_pr_auc = metrics.auc(test_recall, test_precision)\n",
    "        test_precision_w, test_recall_w, _ = metrics.precision_recall_curve(y_test, y_test_pred, sample_weight=sample_weight_test)\n",
    "        test_pr_auc_weighted = metrics.auc(test_recall_w, test_precision_w)\n",
    "        \n",
    "        # Store performance\n",
    "        results['performance_history'][f'iteration_{iteration}'] = {\n",
    "            'n_variables': len(current_vars),\n",
    "            'train_auc': train_auc,\n",
    "            'train_auc_weighted': train_auc_weighted,\n",
    "            'train_pr_auc': train_pr_auc,\n",
    "            'train_pr_auc_weighted': train_pr_auc_weighted,\n",
    "            'test_auc': test_auc,\n",
    "            'test_auc_weighted': test_auc_weighted,\n",
    "            'test_pr_auc': test_pr_auc,\n",
    "            'test_pr_auc_weighted': test_pr_auc_weighted\n",
    "        }\n",
    "        \n",
    "        # Print current performance\n",
    "        print(f\"Performance with {len(current_vars)} variables:\")\n",
    "        print(f\"  Train AUC: {train_auc:.3f} (weighted: {train_auc_weighted:.3f})\")\n",
    "        print(f\"  Test AUC: {test_auc:.3f} (weighted: {test_auc_weighted:.3f})\")\n",
    "        print(f\"  Train PR-AUC: {train_pr_auc:.3f} (weighted: {train_pr_auc_weighted:.3f})\")\n",
    "        print(f\"  Test PR-AUC: {test_pr_auc:.3f} (weighted: {test_pr_auc_weighted:.3f})\")\n",
    "        \n",
    "        # Identify least important variable\n",
    "        least_important_var = sorted_vars[-1][0]\n",
    "        least_important_score = sorted_vars[-1][1]\n",
    "        \n",
    "        print(f\"Least important variable: {least_important_var} (importance: {least_important_score:.4f})\")\n",
    "        \n",
    "        # Remove least important variable\n",
    "        current_x_train = current_x_train.drop(columns=[least_important_var])\n",
    "        current_x_test = current_x_test.drop(columns=[least_important_var])\n",
    "        current_vars.remove(least_important_var)\n",
    "        results['removed_variables'].append(least_important_var)\n",
    "        \n",
    "        print(f\"Removed {least_important_var}. Variables remaining: {current_vars}\")\n",
    "    \n",
    "    results['final_variables'] = current_vars.copy()\n",
    "    print(f\"\\nFinal variable set ({len(current_vars)} variables): {current_vars}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN ITERATIVE VARIABLE REMOVAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE VARIABLE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run the iterative removal process\n",
    "start_time = time.time()\n",
    "\n",
    "# Set target to 5 variables (can be adjusted)\n",
    "target_vars = 5\n",
    "min_vars = 3\n",
    "\n",
    "# Run iterative removal\n",
    "removal_results = iterative_variable_removal(\n",
    "    x_train, y_train, sample_weight_train,\n",
    "    x_test, y_test, sample_weight_test,\n",
    "    target_variables=target_vars,\n",
    "    min_variables=min_vars\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nAnalysis completed in {end_time - start_time:.1f} seconds\")\n",
    "\n",
    "# Store results for later analysis\n",
    "importance_results['iterative_removal'] = removal_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALYZE AND VISUALIZE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "# Extract performance trends\n",
    "iterations = list(removal_results['performance_history'].keys())\n",
    "n_vars = [removal_results['performance_history'][iter]['n_variables'] for iter in iterations]\n",
    "train_aucs = [removal_results['performance_history'][iter]['train_auc'] for iter in iterations]\n",
    "test_aucs = [removal_results['performance_history'][iter]['test_auc'] for iter in iterations]\n",
    "train_aucs_weighted = [removal_results['performance_history'][iter]['train_auc_weighted'] for iter in iterations]\n",
    "test_aucs_weighted = [removal_results['performance_history'][iter]['test_auc_weighted'] for iter in iterations]\n",
    "\n",
    "# Add initial performance (all variables)\n",
    "n_vars.insert(0, len(x_train.columns))\n",
    "train_aucs.insert(0, auc_train)\n",
    "test_aucs.insert(0, auc_test)\n",
    "train_aucs_weighted.insert(0, auc_train_weighted)\n",
    "test_aucs_weighted.insert(0, auc_test_weighted)\n",
    "\n",
    "print(\"Performance Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Variables':<12} {'Train AUC':<10} {'Test AUC':<10} {'Train AUC-W':<12} {'Test AUC-W':<12}\")\n",
    "print(\"-\"*60)\n",
    "for i, n_var in enumerate(n_vars):\n",
    "    print(f\"{n_var:<12} {train_aucs[i]:<10.3f} {test_aucs[i]:<10.3f} {train_aucs_weighted[i]:<12.3f} {test_aucs_weighted[i]:<12.3f}\")\n",
    "\n",
    "# Get final variable ranking\n",
    "final_iteration = f\"iteration_{len(iterations)}\"\n",
    "final_ranking = removal_results['importance_rankings'][final_iteration]['sorted_ranking']\n",
    "\n",
    "print(f\"\\nFinal Variable Ranking (Top {len(removal_results['final_variables'])} variables):\")\n",
    "print(\"=\"*60)\n",
    "for i, (var, importance) in enumerate(final_ranking, 1):\n",
    "    print(f\"{i:2d}. {var:<15} (importance: {importance:.4f})\")\n",
    "\n",
    "print(f\"\\nRemoved Variables (in order of removal):\")\n",
    "print(\"=\"*40)\n",
    "for i, var in enumerate(removal_results['removed_variables'], 1):\n",
    "    print(f\"{i:2d}. {var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae31219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE COMPREHENSIVE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create a comprehensive figure showing the analysis results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Comprehensive Variable Importance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Performance vs Number of Variables\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(n_vars, train_aucs, 'o-', label='Train AUC', color='tab:blue', linewidth=2)\n",
    "ax1.plot(n_vars, test_aucs, 's-', label='Test AUC', color='tab:orange', linewidth=2)\n",
    "ax1.plot(n_vars, train_aucs_weighted, 'o--', label='Train AUC (Weighted)', color='tab:blue', alpha=0.7)\n",
    "ax1.plot(n_vars, test_aucs_weighted, 's--', label='Test AUC (Weighted)', color='tab:orange', alpha=0.7)\n",
    "ax1.set_xlabel('Number of Variables')\n",
    "ax1.set_ylabel('AUC Score')\n",
    "ax1.set_title('Model Performance vs Number of Variables')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.invert_xaxis()  # Show decreasing variables\n",
    "\n",
    "# 2. Final Variable Importance (Top 10)\n",
    "ax2 = axes[0, 1]\n",
    "top_vars = final_ranking[:10]  # Top 10 variables\n",
    "var_names = [var[0] for var in top_vars]\n",
    "var_importance = [var[1] for var in top_vars]\n",
    "\n",
    "bars = ax2.barh(range(len(var_names)), var_importance, color='tab:green', alpha=0.7)\n",
    "ax2.set_yticks(range(len(var_names)))\n",
    "ax2.set_yticklabels(var_names)\n",
    "ax2.set_xlabel('Permutation Importance')\n",
    "ax2.set_title('Top 10 Most Important Variables')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, val) in enumerate(zip(bars, var_importance)):\n",
    "    ax2.text(val + 0.001, i, f'{val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# 3. Variable Removal Timeline\n",
    "ax3 = axes[1, 0]\n",
    "removed_vars = removal_results['removed_variables']\n",
    "removal_order = list(range(1, len(removed_vars) + 1))\n",
    "ax3.bar(removal_order, [1] * len(removed_vars), color='tab:red', alpha=0.7)\n",
    "ax3.set_xlabel('Removal Order')\n",
    "ax3.set_ylabel('Variables Removed')\n",
    "ax3.set_title('Variable Removal Timeline')\n",
    "ax3.set_xticks(removal_order)\n",
    "ax3.set_xticklabels([f'#{i}' for i in removal_order])\n",
    "\n",
    "# Add variable names as text\n",
    "for i, var in enumerate(removed_vars):\n",
    "    ax3.text(i + 1, 0.5, var, rotation=90, ha='center', va='center', fontsize=8)\n",
    "\n",
    "# 4. Performance Degradation Analysis\n",
    "ax4 = axes[1, 1]\n",
    "# Calculate performance drop from initial\n",
    "initial_test_auc = test_aucs[0]\n",
    "initial_train_auc = train_aucs[0]\n",
    "test_drop = [(initial_test_auc - auc) / initial_test_auc * 100 for auc in test_aucs]\n",
    "train_drop = [(initial_train_auc - auc) / initial_train_auc * 100 for auc in train_aucs]\n",
    "\n",
    "ax4.plot(n_vars, test_drop, 'o-', label='Test AUC Drop %', color='tab:red', linewidth=2)\n",
    "ax4.plot(n_vars, train_drop, 's-', label='Train AUC Drop %', color='tab:purple', linewidth=2)\n",
    "ax4.set_xlabel('Number of Variables')\n",
    "ax4.set_ylabel('Performance Drop (%)')\n",
    "ax4.set_title('Performance Degradation with Variable Removal')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.invert_xaxis()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the comprehensive analysis figure\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration)\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration)\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration)\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s.png' % (specie, training, bio, iteration)\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    print(f\"Comprehensive analysis figure saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8afc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS TO CSV FOR FURTHER ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Create summary DataFrame for export\n",
    "summary_data = []\n",
    "\n",
    "# Add initial performance (all variables)\n",
    "summary_data.append({\n",
    "    'iteration': 0,\n",
    "    'n_variables': len(x_train.columns),\n",
    "    'variables_removed': 'none',\n",
    "    'train_auc': auc_train,\n",
    "    'train_auc_weighted': auc_train_weighted,\n",
    "    'test_auc': auc_test,\n",
    "    'test_auc_weighted': auc_test_weighted,\n",
    "    'train_pr_auc': pr_auc_train,\n",
    "    'train_pr_auc_weighted': pr_auc_train_weighted,\n",
    "    'test_pr_auc': pr_auc_test,\n",
    "    'test_pr_auc_weighted': pr_auc_test_weighted\n",
    "})\n",
    "\n",
    "# Add iterative removal results\n",
    "for i, iter_key in enumerate(iterations, 1):\n",
    "    perf = removal_results['performance_history'][iter_key]\n",
    "    removed_var = removal_results['removed_variables'][i-1] if i-1 < len(removal_results['removed_variables']) else 'none'\n",
    "    \n",
    "    summary_data.append({\n",
    "        'iteration': i,\n",
    "        'n_variables': perf['n_variables'],\n",
    "        'variables_removed': removed_var,\n",
    "        'train_auc': perf['train_auc'],\n",
    "        'train_auc_weighted': perf['train_auc_weighted'],\n",
    "        'test_auc': perf['test_auc'],\n",
    "        'test_auc_weighted': perf['test_auc_weighted'],\n",
    "        'train_pr_auc': perf['train_pr_auc'],\n",
    "        'train_pr_auc_weighted': perf['train_pr_auc_weighted'],\n",
    "        'test_pr_auc': perf['test_pr_auc'],\n",
    "        'test_pr_auc_weighted': perf['test_pr_auc_weighted']\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save to CSV\n",
    "if savefig:\n",
    "    csv_filename = f'06_variable_importance_analysis_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "    csv_path = os.path.join(figs_path, csv_filename)\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Analysis summary saved to: {csv_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Species: {specie}\")\n",
    "print(f\"Training Region: {training}\")\n",
    "print(f\"Test Region: {interest}\")\n",
    "print(f\"Initial Variables: {len(x_train.columns)}\")\n",
    "print(f\"Final Variables: {len(removal_results['final_variables'])}\")\n",
    "print(f\"Variables Removed: {len(removal_results['removed_variables'])}\")\n",
    "\n",
    "print(f\"\\nFinal Variable Set:\")\n",
    "for i, var in enumerate(removal_results['final_variables'], 1):\n",
    "    print(f\"  {i}. {var}\")\n",
    "\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"  Initial Test AUC: {test_aucs[0]:.3f}\")\n",
    "print(f\"  Final Test AUC: {test_aucs[-1]:.3f}\")\n",
    "print(f\"  Performance Drop: {((test_aucs[0] - test_aucs[-1]) / test_aucs[0] * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Variables:\")\n",
    "for i, (var, importance) in enumerate(final_ranking[:5], 1):\n",
    "    print(f\"  {i}. {var} (importance: {importance:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef32d7f",
   "metadata": {},
   "source": [
    "## 7. Summary and Recommendations\n",
    "\n",
    "### Key Benefits of 10-Iteration Analysis:\n",
    "\n",
    "1. **Robustness**: Multiple iterations account for random variation in model training and importance calculations\n",
    "2. **Statistical Significance**: Provides mean, standard deviation, and confidence intervals for importance scores\n",
    "3. **Consistency Analysis**: Identifies variables that are consistently important across different runs\n",
    "4. **Performance Stability**: Shows how model performance varies with different variable sets\n",
    "\n",
    "### Final Recommendations:\n",
    "\n",
    "1. **Use Most Consistent Variables**: Variables that appear in the final set across most iterations are most reliable\n",
    "2. **Consider Importance + Consistency**: Balance between high importance and high consistency\n",
    "3. **Validate on Independent Data**: Test the selected variables on completely independent datasets\n",
    "4. **Monitor Performance**: Track how the reduced variable set performs in real-world applications\n",
    "\n",
    "### Files Generated:\n",
    "- **Robust analysis figure**: 6-panel visualization showing comprehensive results\n",
    "- **Summary CSV**: Aggregated statistics across all 10 iterations\n",
    "- **Detailed CSV**: Individual results for each iteration\n",
    "- **Console output**: Detailed rankings and recommendations\n",
    "\n",
    "### Next Steps:\n",
    "1. Use the identified top 5 variables for future modeling\n",
    "2. Consider running additional iterations if results are not stable\n",
    "3. Validate the selected variables on independent test data\n",
    "4. Document the ecological significance of the selected variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a733f05-26cb-4da3-a95d-7adc42870020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels and open training output NetCDF for metadata\n",
    "labels = train.drop(columns=['class', 'geometry', 'SampleWeight']).columns.values\n",
    "training_output = xr.open_dataset(os.path.join(exp_path, nc_name))\n",
    "# display(labels)\n",
    "# display(training_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d9454-dce3-4b7d-922b-8f84e37f1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute partial dependence across features\n",
    "# - percentiles bounds the feature grid to observed range (2.5% to 97.5%)\n",
    "# - nbins controls resolution of the curve\n",
    "percentiles = (0.025, 0.975)\n",
    "nbins = 100\n",
    "\n",
    "mean = {}\n",
    "stdv = {}\n",
    "bins = {}\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    # Request individual PDP curves across samples, then summarize\n",
    "    pda = inspection.partial_dependence(\n",
    "        model_train,\n",
    "        x_train,\n",
    "        [idx],\n",
    "        percentiles=percentiles,\n",
    "        grid_resolution=nbins,\n",
    "        kind=\"individual\",\n",
    "    )\n",
    "\n",
    "    mean[label] = pda[\"individual\"][0].mean(axis=0)  # average response\n",
    "    stdv[label] = pda[\"individual\"][0].std(axis=0)   # variability across samples\n",
    "    bins[label] = pda[\"grid_values\"][0]              # feature grid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e805fc6-8c45-4c42-a7fd-255e89a152b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(pda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890d37e-af5a-4f15-966a-02bde1a1f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDPs with uncertainty bands for each predictor\n",
    "ncols, nrows = subplot_layout(len(labels))\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 6, nrows * 6))\n",
    "\n",
    "# Normalize axes list for consistent indexing\n",
    "if (nrows, ncols) == (1, 1):\n",
    "    ax = [axs]\n",
    "else:\n",
    "    ax = axs.ravel()\n",
    "\n",
    "xlabels = training_output.data_vars\n",
    "for iax, label in enumerate(labels):\n",
    "    ax[iax].set_title(label)\n",
    "    try:\n",
    "        ax[iax].set_xlabel(xlabels[label].long_name)\n",
    "    except (ValueError, AttributeError):\n",
    "        ax[iax].set_xlabel('No variable long_name')\n",
    "\n",
    "    # Uncertainty band: mean Â± std across individuals\n",
    "    ax[iax].fill_between(bins[label], mean[label] - stdv[label], mean[label] + stdv[label], alpha=0.25)\n",
    "    ax[iax].plot(bins[label], mean[label])\n",
    "\n",
    "# Style axes\n",
    "for axi in ax:\n",
    "    axi.set_ylim([0, 1])\n",
    "    axi.set_ylabel('probability of occurrence')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2d9fa-5f37-48b4-a03c-9f53f244b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save response curve figures if requested\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f47155-3512-4f32-9128-9595e1709c6a",
   "metadata": {},
   "source": [
    "### 3.3 Variable importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b404609-047b-4e31-bf5f-28b0de041906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = model_train.permutation_importance_plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4362f3-93b0-4626-a37b-bf36a36900dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance: measures drop in performance when each feature is shuffled\n",
    "# Higher drop => more important feature\n",
    "pi = inspection.permutation_importance(model_train, x_train, y_train, n_repeats=10)\n",
    "importance = pi.importances\n",
    "rank_order = importance.mean(axis=-1).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301c544-3a98-4bac-ab55-d0c07373f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation importances as horizontal boxplots (distribution over repeats)\n",
    "labels_ranked = [labels[idx] for idx in rank_order]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "box = ax.boxplot(importance[rank_order].T, vert=False, labels=labels_ranked)\n",
    "# Decorate legend labels for key boxplot elements\n",
    "box['fliers'][0].set_label('outlier')\n",
    "box['medians'][0].set_label('median')\n",
    "for icap, cap in enumerate(box['caps']):\n",
    "    if icap == 0:\n",
    "        cap.set_label('min-max')\n",
    "    cap.set_color('k')\n",
    "    cap.set_linewidth(2)\n",
    "for ibx, bx in enumerate(box['boxes']):\n",
    "    if ibx == 0:\n",
    "        bx.set_label('25-75%')\n",
    "    bx.set_color('gray')\n",
    "\n",
    "ax.set_xlabel('Importance')\n",
    "ax.legend(loc='lower right')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cc671-1f8a-4375-9af2-8286603483ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if savefig:\n",
    "#     if Future:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_var-importance_%s_%s_%s_future.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "#     else:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_var-importance_%s_%s_%s.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'model' variable is not null or empty\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_%s_future.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no model is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_future.png' %(specie, training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_%s.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s.png' %(specie, training, bio,iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bbb129",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af136233",
   "metadata": {},
   "source": [
    "## 12. Analisis Sebaran Performance Berdasarkan Lokasi Geografis\n",
    "\n",
    "Bagian ini melakukan analisis mendalam tentang sebaran performance model berdasarkan lokasi geografis, termasuk:\n",
    "\n",
    "### Tujuan Analisis Spasial:\n",
    "1. **Spatial Performance Distribution**: Analisis sebaran performa berdasarkan koordinat geografis\n",
    "2. **Regional Performance Analysis**: Perbandingan performa antar region/area\n",
    "3. **Spatial Bias Detection**: Identifikasi bias spasial dalam performa model\n",
    "4. **Geographic Clustering**: Analisis clustering performa berdasarkan lokasi\n",
    "5. **Spatial Correlation**: Korelasi antara lokasi geografis dan performa model\n",
    "\n",
    "### Metodologi:\n",
    "- **Spatial Statistics**: Analisis statistik spasial untuk mengidentifikasi pola\n",
    "- **Geographic Visualization**: Peta performa dengan color coding\n",
    "- **Regional Comparison**: Perbandingan performa antar region\n",
    "- **Spatial Autocorrelation**: Analisis korelasi spasial\n",
    "- **Hotspot Analysis**: Identifikasi area dengan performa tinggi/rendah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALISIS SEBARAN PERFORMANCE BERDASARKAN LOKASI GEOGRAFIS\n",
    "# =============================================================================\n",
    "# %pip install contextily\n",
    "# %pip install folium\n",
    "    \n",
    "import folium\n",
    "from folium import plugins\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import contextily as ctx\n",
    "from rasterio.plot import show\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "def analyze_spatial_performance_distribution():\n",
    "    \"\"\"\n",
    "    Menganalisis sebaran performa model berdasarkan lokasi geografis.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ANALISIS SEBARAN PERFORMANCE BERDASARKAN LOKASI GEOGRAFIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Gabungkan data training dan test untuk analisis spasial\n",
    "    train_spatial = train.copy()\n",
    "    test_spatial = test.copy()\n",
    "    \n",
    "    # Tambahkan kolom untuk membedakan training dan test\n",
    "    train_spatial['dataset'] = 'training'\n",
    "    test_spatial['dataset'] = 'test'\n",
    "    \n",
    "    # Gabungkan data\n",
    "    combined_spatial = pd.concat([train_spatial, test_spatial], ignore_index=True)\n",
    "    \n",
    "    # Hitung performa untuk setiap lokasi\n",
    "    combined_spatial['predicted_prob'] = model_train.predict(\n",
    "        combined_spatial.drop(columns=['class', 'SampleWeight', 'geometry', 'dataset'])\n",
    "    )\n",
    "    \n",
    "    # Hitung error untuk setiap lokasi\n",
    "    combined_spatial['prediction_error'] = abs(combined_spatial['predicted_prob'] - combined_spatial['class'])\n",
    "    \n",
    "    # Hitung confidence score (berdasarkan jarak dari threshold 0.5)\n",
    "    combined_spatial['confidence'] = abs(combined_spatial['predicted_prob'] - 0.5) * 2\n",
    "    \n",
    "    print(f\"Total lokasi yang dianalisis: {len(combined_spatial)}\")\n",
    "    print(f\"Training locations: {len(train_spatial)}\")\n",
    "    print(f\"Test locations: {len(test_spatial)}\")\n",
    "    \n",
    "    return combined_spatial\n",
    "\n",
    "# Jalankan analisis spasial\n",
    "spatial_data = analyze_spatial_performance_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281bd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALISIS STATISTIK SPASIAL DAN REGIONAL PERFORMANCE\n",
    "# =============================================================================\n",
    "\n",
    "def perform_spatial_statistical_analysis(spatial_data):\n",
    "    \"\"\"\n",
    "    Melakukan analisis statistik spasial untuk mengidentifikasi pola performa.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALISIS STATISTIK SPASIAL DAN REGIONAL PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Ekstrak koordinat\n",
    "    spatial_data['longitude'] = spatial_data.geometry.x\n",
    "    spatial_data['latitude'] = spatial_data.geometry.y\n",
    "    \n",
    "    # Analisis performa berdasarkan dataset (training vs test)\n",
    "    print(\"\\n--- Performance by Dataset ---\")\n",
    "    dataset_stats = spatial_data.groupby('dataset').agg({\n",
    "        'predicted_prob': ['count', 'mean', 'std', 'min', 'max'],\n",
    "        'prediction_error': ['mean', 'std', 'min', 'max'],\n",
    "        'confidence': ['mean', 'std', 'min', 'max']\n",
    "    }).round(4)\n",
    "    print(dataset_stats)\n",
    "    \n",
    "    # Analisis performa berdasarkan kelas (presence vs absence)\n",
    "    print(\"\\n--- Performance by Class (Presence vs Absence) ---\")\n",
    "    class_stats = spatial_data.groupby('class').agg({\n",
    "        'predicted_prob': ['count', 'mean', 'std', 'min', 'max'],\n",
    "        'prediction_error': ['mean', 'std', 'min', 'max'],\n",
    "        'confidence': ['mean', 'std', 'min', 'max']\n",
    "    }).round(4)\n",
    "    print(class_stats)\n",
    "    \n",
    "    # Analisis regional (membagi area menjadi grid)\n",
    "    print(\"\\n--- Regional Analysis (Grid-based) ---\")\n",
    "    \n",
    "    # Buat grid 5x5 untuk analisis regional\n",
    "    min_lon, min_lat = spatial_data['longitude'].min(), spatial_data['latitude'].min()\n",
    "    max_lon, max_lat = spatial_data['longitude'].max(), spatial_data['latitude'].max()\n",
    "    \n",
    "    # Hitung grid size\n",
    "    grid_size_lon = (max_lon - min_lon) / 5\n",
    "    grid_size_lat = (max_lat - min_lat) / 5\n",
    "    \n",
    "    # Assign grid ID\n",
    "    spatial_data['grid_lon'] = ((spatial_data['longitude'] - min_lon) / grid_size_lon).astype(int)\n",
    "    spatial_data['grid_lat'] = ((spatial_data['latitude'] - min_lat) / grid_size_lat).astype(int)\n",
    "    spatial_data['grid_id'] = spatial_data['grid_lon'].astype(str) + '_' + spatial_data['grid_lat'].astype(str)\n",
    "    \n",
    "    # Analisis performa per grid\n",
    "    grid_stats = spatial_data.groupby('grid_id').agg({\n",
    "        'predicted_prob': ['count', 'mean', 'std'],\n",
    "        'prediction_error': ['mean', 'std'],\n",
    "        'confidence': ['mean', 'std'],\n",
    "        'longitude': 'mean',\n",
    "        'latitude': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    # Filter grid dengan minimal 5 data points\n",
    "    grid_stats_filtered = grid_stats[grid_stats[('predicted_prob', 'count')] >= 5]\n",
    "    print(f\"Grids with sufficient data (â‰¥5 points): {len(grid_stats_filtered)}\")\n",
    "    print(grid_stats_filtered.head(10))\n",
    "    \n",
    "    # Analisis korelasi spasial\n",
    "    print(\"\\n--- Spatial Correlation Analysis ---\")\n",
    "    \n",
    "    # Hitung korelasi antara koordinat dan performa\n",
    "    spatial_corr = spatial_data[['longitude', 'latitude', 'predicted_prob', 'prediction_error', 'confidence']].corr()\n",
    "    print(\"Correlation Matrix (Coordinates vs Performance):\")\n",
    "    print(spatial_corr.round(4))\n",
    "    \n",
    "    # Analisis clustering spasial\n",
    "    print(\"\\n--- Spatial Clustering Analysis ---\")\n",
    "    \n",
    "    # K-means clustering berdasarkan koordinat dan performa\n",
    "    features_for_clustering = ['longitude', 'latitude', 'predicted_prob', 'prediction_error', 'confidence']\n",
    "    X_cluster = spatial_data[features_for_clustering].dropna()\n",
    "    \n",
    "    if len(X_cluster) > 10:  # Minimum data untuk clustering\n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_cluster)\n",
    "        \n",
    "        # K-means clustering (3-5 clusters)\n",
    "        n_clusters = min(5, len(X_cluster) // 10)  # Adaptive number of clusters\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        # Tambahkan cluster labels ke data\n",
    "        X_cluster['cluster'] = cluster_labels\n",
    "        \n",
    "        # Analisis performa per cluster\n",
    "        cluster_stats = X_cluster.groupby('cluster').agg({\n",
    "            'predicted_prob': ['count', 'mean', 'std'],\n",
    "            'prediction_error': ['mean', 'std'],\n",
    "            'confidence': ['mean', 'std'],\n",
    "            'longitude': 'mean',\n",
    "            'latitude': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        print(f\"Spatial Clusters: {n_clusters}\")\n",
    "        print(cluster_stats)\n",
    "        \n",
    "        # Tambahkan cluster info ke spatial_data\n",
    "        spatial_data = spatial_data.merge(\n",
    "            X_cluster[['longitude', 'latitude', 'cluster']], \n",
    "            on=['longitude', 'latitude'], \n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        print(\"Insufficient data for clustering analysis\")\n",
    "        spatial_data['cluster'] = 0\n",
    "    \n",
    "    return spatial_data, grid_stats_filtered, spatial_corr\n",
    "\n",
    "# Jalankan analisis statistik spasial\n",
    "spatial_data, grid_stats, spatial_correlation = perform_spatial_statistical_analysis(spatial_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALISASI PETA PERFORMANCE DAN SEBARAN SPASIAL\n",
    "# =============================================================================\n",
    "\n",
    "def create_spatial_performance_maps(spatial_data):\n",
    "    \"\"\"\n",
    "    Membuat visualisasi peta performa dan sebaran spasial.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MEMBUAT VISUALISASI PETA PERFORMANCE DAN SEBARAN SPASIAL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Set style untuk plot\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Buat figure dengan multiple subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Spatial Performance Analysis and Distribution Maps', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Scatter plot: Predicted Probability vs Location\n",
    "    ax1 = axes[0, 0]\n",
    "    scatter = ax1.scatter(spatial_data['longitude'], spatial_data['latitude'], \n",
    "                         c=spatial_data['predicted_prob'], cmap='RdYlBu_r', \n",
    "                         s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('Predicted Probability Distribution')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax1, label='Predicted Probability')\n",
    "    \n",
    "    # 2. Scatter plot: Prediction Error vs Location\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter2 = ax2.scatter(spatial_data['longitude'], spatial_data['latitude'], \n",
    "                          c=spatial_data['prediction_error'], cmap='Reds', \n",
    "                          s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "    ax2.set_xlabel('Longitude')\n",
    "    ax2.set_ylabel('Latitude')\n",
    "    ax2.set_title('Prediction Error Distribution')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter2, ax=ax2, label='Prediction Error')\n",
    "    \n",
    "    # 3. Scatter plot: Confidence vs Location\n",
    "    ax3 = axes[0, 2]\n",
    "    scatter3 = ax3.scatter(spatial_data['longitude'], spatial_data['latitude'], \n",
    "                          c=spatial_data['confidence'], cmap='Greens', \n",
    "                          s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "    ax3.set_xlabel('Longitude')\n",
    "    ax3.set_ylabel('Latitude')\n",
    "    ax3.set_title('Model Confidence Distribution')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter3, ax=ax3, label='Confidence Score')\n",
    "    \n",
    "    # 4. Dataset comparison (Training vs Test)\n",
    "    ax4 = axes[1, 0]\n",
    "    for dataset, color in [('training', 'blue'), ('test', 'red')]:\n",
    "        subset = spatial_data[spatial_data['dataset'] == dataset]\n",
    "        ax4.scatter(subset['longitude'], subset['latitude'], \n",
    "                   c=color, s=20, alpha=0.6, label=dataset.title())\n",
    "    ax4.set_xlabel('Longitude')\n",
    "    ax4.set_ylabel('Latitude')\n",
    "    ax4.set_title('Training vs Test Data Distribution')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Class comparison (Presence vs Absence)\n",
    "    ax5 = axes[1, 1]\n",
    "    for class_val, color, marker in [(1, 'green', 'o'), (0, 'red', 's')]:\n",
    "        subset = spatial_data[spatial_data['class'] == class_val]\n",
    "        ax5.scatter(subset['longitude'], subset['latitude'], \n",
    "                   c=color, s=20, alpha=0.6, marker=marker, \n",
    "                   label='Presence' if class_val == 1 else 'Absence')\n",
    "    ax5.set_xlabel('Longitude')\n",
    "    ax5.set_ylabel('Latitude')\n",
    "    ax5.set_title('Presence vs Absence Distribution')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Spatial Clusters\n",
    "    ax6 = axes[1, 2]\n",
    "    if 'cluster' in spatial_data.columns:\n",
    "        unique_clusters = spatial_data['cluster'].dropna().unique()\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))\n",
    "        \n",
    "        for i, cluster in enumerate(unique_clusters):\n",
    "            subset = spatial_data[spatial_data['cluster'] == cluster]\n",
    "            ax6.scatter(subset['longitude'], subset['latitude'], \n",
    "                       c=[colors[i]], s=20, alpha=0.6, \n",
    "                       label=f'Cluster {cluster}')\n",
    "        ax6.set_xlabel('Longitude')\n",
    "        ax6.set_ylabel('Latitude')\n",
    "        ax6.set_title('Spatial Clusters')\n",
    "        ax6.legend()\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax6.text(0.5, 0.5, 'No clustering data available', \n",
    "                transform=ax6.transAxes, ha='center', va='center')\n",
    "        ax6.set_title('Spatial Clusters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Buat peta performa spasial\n",
    "spatial_maps_fig = create_spatial_performance_maps(spatial_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INTERAKTIF FOLIUM MAP UNTUK ANALISIS SPASIAL\n",
    "# =============================================================================\n",
    "\n",
    "def create_interactive_folium_map(spatial_data):\n",
    "    \"\"\"\n",
    "    Membuat peta interaktif menggunakan Folium untuk analisis spasial.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MEMBUAT PETA INTERAKTIF FOLIUM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Hitung center point\n",
    "    center_lat = spatial_data['latitude'].mean()\n",
    "    center_lon = spatial_data['longitude'].mean()\n",
    "    \n",
    "    # Buat base map\n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon],\n",
    "        zoom_start=6,\n",
    "        tiles='OpenStreetMap'\n",
    "    )\n",
    "    \n",
    "    # Tambahkan layer untuk predicted probability\n",
    "    folium.TileLayer('CartoDB positron').add_to(m)\n",
    "    \n",
    "    # Buat color scale untuk predicted probability\n",
    "    prob_min, prob_max = spatial_data['predicted_prob'].min(), spatial_data['predicted_prob'].max()\n",
    "    \n",
    "    def get_color(prob):\n",
    "        \"\"\"Mengembalikan warna berdasarkan predicted probability.\"\"\"\n",
    "        if prob < 0.2:\n",
    "            return 'red'\n",
    "        elif prob < 0.4:\n",
    "            return 'orange'\n",
    "        elif prob < 0.6:\n",
    "            return 'yellow'\n",
    "        elif prob < 0.8:\n",
    "            return 'lightgreen'\n",
    "        else:\n",
    "            return 'green'\n",
    "    \n",
    "    # Tambahkan markers untuk setiap lokasi\n",
    "    for idx, row in spatial_data.iterrows():\n",
    "        # Popup information\n",
    "        popup_text = f\"\"\"\n",
    "        <b>Location {idx}</b><br>\n",
    "        Dataset: {row['dataset']}<br>\n",
    "        Class: {'Presence' if row['class'] == 1 else 'Absence'}<br>\n",
    "        Predicted Probability: {row['predicted_prob']:.3f}<br>\n",
    "        Prediction Error: {row['prediction_error']:.3f}<br>\n",
    "        Confidence: {row['confidence']:.3f}<br>\n",
    "        Coordinates: ({row['latitude']:.4f}, {row['longitude']:.4f})\n",
    "        \"\"\"\n",
    "        \n",
    "        # Marker color berdasarkan dataset\n",
    "        marker_color = 'blue' if row['dataset'] == 'training' else 'red'\n",
    "        \n",
    "        # Marker size berdasarkan confidence\n",
    "        marker_size = max(5, min(20, row['confidence'] * 20))\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=marker_size,\n",
    "            popup=folium.Popup(popup_text, max_width=300),\n",
    "            color='black',\n",
    "            weight=1,\n",
    "            fillColor=get_color(row['predicted_prob']),\n",
    "            fillOpacity=0.7,\n",
    "            tooltip=f\"Prob: {row['predicted_prob']:.3f}\"\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Tambahkan legend\n",
    "    legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; left: 50px; width: 200px; height: 120px; \n",
    "                background-color: white; border:2px solid grey; z-index:9999; \n",
    "                font-size:14px; padding: 10px\">\n",
    "    <p><b>Predicted Probability</b></p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:red\"></i> 0.0 - 0.2</p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:orange\"></i> 0.2 - 0.4</p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:yellow\"></i> 0.4 - 0.6</p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:lightgreen\"></i> 0.6 - 0.8</p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:green\"></i> 0.8 - 1.0</p>\n",
    "    <p><b>Dataset:</b></p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:blue\"></i> Training</p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:red\"></i> Test</p>\n",
    "    </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    # Tambahkan heatmap layer\n",
    "    heat_data = [[row['latitude'], row['longitude'], row['predicted_prob']] \n",
    "                 for idx, row in spatial_data.iterrows()]\n",
    "    \n",
    "    plugins.HeatMap(heat_data, name='Prediction Heatmap', \n",
    "                   min_opacity=0.2, max_zoom=18, radius=15, blur=10).add_to(m)\n",
    "    \n",
    "    # Tambahkan layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Buat peta interaktif\n",
    "interactive_map = create_interactive_folium_map(spatial_data)\n",
    "\n",
    "# Simpan peta interaktif\n",
    "if savefig:\n",
    "    map_filename = f'06_interactive_spatial_map_{specie}_{training}_{bio}_{iteration}.html'\n",
    "    map_path = os.path.join(figs_path, map_filename)\n",
    "    interactive_map.save(map_path)\n",
    "    print(f\"Interactive map saved to: {map_path}\")\n",
    "\n",
    "# Tampilkan peta\n",
    "interactive_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALISIS SEBARAN PERFORMANCE DENGAN PLOTLY INTERAKTIF\n",
    "# =============================================================================\n",
    "\n",
    "def create_interactive_plotly_analysis(spatial_data):\n",
    "    \"\"\"\n",
    "    Membuat analisis interaktif menggunakan Plotly untuk sebaran performance.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MEMBUAT ANALISIS INTERAKTIF PLOTLY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Scatter plot 3D: Longitude, Latitude, Predicted Probability\n",
    "    fig_3d = go.Figure(data=go.Scatter3d(\n",
    "        x=spatial_data['longitude'],\n",
    "        y=spatial_data['latitude'],\n",
    "        z=spatial_data['predicted_prob'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=spatial_data['predicted_prob'],\n",
    "            colorscale='RdYlBu_r',\n",
    "            opacity=0.8,\n",
    "            colorbar=dict(title=\"Predicted Probability\")\n",
    "        ),\n",
    "        text=[f\"Dataset: {row['dataset']}<br>Class: {'Presence' if row['class'] == 1 else 'Absence'}<br>Error: {row['prediction_error']:.3f}\" \n",
    "              for idx, row in spatial_data.iterrows()],\n",
    "        hovertemplate='<b>Location</b><br>Longitude: %{x:.4f}<br>Latitude: %{y:.4f}<br>Probability: %{z:.3f}<br>%{text}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig_3d.update_layout(\n",
    "        title='3D Spatial Distribution of Predicted Probabilities',\n",
    "        scene=dict(\n",
    "            xaxis_title='Longitude',\n",
    "            yaxis_title='Latitude',\n",
    "            zaxis_title='Predicted Probability'\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    # 2. Subplot dengan multiple metrics\n",
    "    fig_subplots = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Predicted Probability', 'Prediction Error', \n",
    "                       'Model Confidence', 'Dataset Distribution'),\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # Predicted Probability\n",
    "    fig_subplots.add_trace(\n",
    "        go.Scatter(\n",
    "            x=spatial_data['longitude'],\n",
    "            y=spatial_data['latitude'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=spatial_data['predicted_prob'],\n",
    "                colorscale='RdYlBu_r',\n",
    "                opacity=0.7,\n",
    "                colorbar=dict(x=0.48, title=\"Probability\")\n",
    "            ),\n",
    "            text=[f\"Prob: {row['predicted_prob']:.3f}\" for idx, row in spatial_data.iterrows()],\n",
    "            hovertemplate='<b>Predicted Probability</b><br>Longitude: %{x:.4f}<br>Latitude: %{y:.4f}<br>%{text}<extra></extra>',\n",
    "            name='Predicted Probability'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Prediction Error\n",
    "    fig_subplots.add_trace(\n",
    "        go.Scatter(\n",
    "            x=spatial_data['longitude'],\n",
    "            y=spatial_data['latitude'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=spatial_data['prediction_error'],\n",
    "                colorscale='Reds',\n",
    "                opacity=0.7,\n",
    "                colorbar=dict(x=1.02, title=\"Error\")\n",
    "            ),\n",
    "            text=[f\"Error: {row['prediction_error']:.3f}\" for idx, row in spatial_data.iterrows()],\n",
    "            hovertemplate='<b>Prediction Error</b><br>Longitude: %{x:.4f}<br>Latitude: %{y:.4f}<br>%{text}<extra></extra>',\n",
    "            name='Prediction Error'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Model Confidence\n",
    "    fig_subplots.add_trace(\n",
    "        go.Scatter(\n",
    "            x=spatial_data['longitude'],\n",
    "            y=spatial_data['latitude'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=spatial_data['confidence'],\n",
    "                colorscale='Greens',\n",
    "                opacity=0.7,\n",
    "                colorbar=dict(x=0.48, y=0.5, title=\"Confidence\")\n",
    "            ),\n",
    "            text=[f\"Confidence: {row['confidence']:.3f}\" for idx, row in spatial_data.iterrows()],\n",
    "            hovertemplate='<b>Model Confidence</b><br>Longitude: %{x:.4f}<br>Latitude: %{y:.4f}<br>%{text}<extra></extra>',\n",
    "            name='Model Confidence'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Dataset Distribution\n",
    "    for dataset, color in [('training', 'blue'), ('test', 'red')]:\n",
    "        subset = spatial_data[spatial_data['dataset'] == dataset]\n",
    "        fig_subplots.add_trace(\n",
    "            go.Scatter(\n",
    "                x=subset['longitude'],\n",
    "                y=subset['latitude'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=color,\n",
    "                    opacity=0.7\n",
    "                ),\n",
    "                text=[f\"Dataset: {dataset}\" for idx, row in subset.iterrows()],\n",
    "                hovertemplate='<b>Dataset Distribution</b><br>Longitude: %{x:.4f}<br>Latitude: %{y:.4f}<br>%{text}<extra></extra>',\n",
    "                name=dataset.title()\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig_subplots.update_layout(\n",
    "        title='Interactive Spatial Performance Analysis',\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # 3. Box plot untuk analisis sebaran per region\n",
    "    fig_box = go.Figure()\n",
    "    \n",
    "    # Buat box plot untuk setiap grid region\n",
    "    if 'grid_id' in spatial_data.columns:\n",
    "        grid_ids = spatial_data['grid_id'].value_counts().head(10).index  # Top 10 grids\n",
    "        \n",
    "        for grid_id in grid_ids:\n",
    "            subset = spatial_data[spatial_data['grid_id'] == grid_id]\n",
    "            fig_box.add_trace(go.Box(\n",
    "                y=subset['predicted_prob'],\n",
    "                name=f'Grid {grid_id}',\n",
    "                boxpoints='outliers',\n",
    "                jitter=0.3,\n",
    "                pointpos=-1.8\n",
    "            ))\n",
    "    \n",
    "    fig_box.update_layout(\n",
    "        title='Predicted Probability Distribution by Grid Region',\n",
    "        yaxis_title='Predicted Probability',\n",
    "        xaxis_title='Grid Region',\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig_3d, fig_subplots, fig_box\n",
    "\n",
    "# Buat analisis interaktif Plotly\n",
    "fig_3d, fig_subplots, fig_box = create_interactive_plotly_analysis(spatial_data)\n",
    "\n",
    "# Tampilkan figures\n",
    "print(\"Displaying 3D spatial distribution...\")\n",
    "fig_3d.show()\n",
    "\n",
    "print(\"Displaying subplot analysis...\")\n",
    "fig_subplots.show()\n",
    "\n",
    "print(\"Displaying box plot analysis...\")\n",
    "fig_box.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMPAN SEMUA HASIL ANALISIS SPASIAL DAN EKSPOR DATA\n",
    "# =============================================================================\n",
    "\n",
    "def save_spatial_analysis_results():\n",
    "    \"\"\"\n",
    "    Menyimpan semua hasil analisis spasial dan mengekspor data.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MENYIMPAN HASIL ANALISIS SPASIAL DAN EKSPOR DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if savefig:\n",
    "        # Simpan spatial maps figure\n",
    "        if 'spatial_maps_fig' in locals() and spatial_maps_fig is not None:\n",
    "            if Future:\n",
    "                if models:\n",
    "                    file_path = os.path.join(\n",
    "                        figs_path,\n",
    "                        '06_spatial_performance_maps_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration)\n",
    "                    )\n",
    "                else:\n",
    "                    file_path = os.path.join(\n",
    "                        figs_path,\n",
    "                        '06_spatial_performance_maps_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration)\n",
    "                    )\n",
    "            else:\n",
    "                if models:\n",
    "                    file_path = os.path.join(\n",
    "                        figs_path,\n",
    "                        '06_spatial_performance_maps_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration)\n",
    "                    )\n",
    "                else:\n",
    "                    file_path = os.path.join(\n",
    "                        figs_path,\n",
    "                        '06_spatial_performance_maps_%s_%s_%s_%s.png' % (specie, training, bio, iteration)\n",
    "                    )\n",
    "            spatial_maps_fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "            print(f\"Spatial performance maps saved to: {file_path}\")\n",
    "    \n",
    "    # Ekspor data spasial ke CSV\n",
    "    if 'spatial_data' in locals() and spatial_data is not None:\n",
    "        # Ekspor data spasial lengkap\n",
    "        spatial_csv_filename = f'06_spatial_performance_data_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        spatial_csv_path = os.path.join(figs_path, spatial_csv_filename)\n",
    "        \n",
    "        # Konversi geometry ke WKT untuk CSV\n",
    "        spatial_data_export = spatial_data.copy()\n",
    "        spatial_data_export['geometry_wkt'] = spatial_data_export['geometry'].apply(lambda x: x.wkt)\n",
    "        spatial_data_export = spatial_data_export.drop(columns=['geometry'])\n",
    "        \n",
    "        spatial_data_export.to_csv(spatial_csv_path, index=False)\n",
    "        print(f\"Spatial performance data exported to: {spatial_csv_path}\")\n",
    "        \n",
    "        # Ekspor summary statistik spasial\n",
    "        spatial_summary = spatial_data.groupby(['dataset', 'class']).agg({\n",
    "            'predicted_prob': ['count', 'mean', 'std', 'min', 'max'],\n",
    "            'prediction_error': ['mean', 'std', 'min', 'max'],\n",
    "            'confidence': ['mean', 'std', 'min', 'max'],\n",
    "            'longitude': ['mean', 'std'],\n",
    "            'latitude': ['mean', 'std']\n",
    "        }).round(4)\n",
    "        \n",
    "        spatial_summary_csv_filename = f'06_spatial_summary_statistics_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        spatial_summary_csv_path = os.path.join(figs_path, spatial_summary_csv_filename)\n",
    "        spatial_summary.to_csv(spatial_summary_csv_path)\n",
    "        print(f\"Spatial summary statistics exported to: {spatial_summary_csv_path}\")\n",
    "    \n",
    "    # Ekspor grid statistics\n",
    "    if 'grid_stats' in locals() and grid_stats is not None:\n",
    "        grid_csv_filename = f'06_grid_performance_statistics_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        grid_csv_path = os.path.join(figs_path, grid_csv_filename)\n",
    "        grid_stats.to_csv(grid_csv_path)\n",
    "        print(f\"Grid performance statistics exported to: {grid_csv_path}\")\n",
    "    \n",
    "    # Ekspor spatial correlation matrix\n",
    "    if 'spatial_correlation' in locals() and spatial_correlation is not None:\n",
    "        corr_csv_filename = f'06_spatial_correlation_matrix_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        corr_csv_path = os.path.join(figs_path, corr_csv_filename)\n",
    "        spatial_correlation.to_csv(corr_csv_path)\n",
    "        print(f\"Spatial correlation matrix exported to: {corr_csv_path}\")\n",
    "    \n",
    "    # Buat laporan ringkasan analisis spasial\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RINGKASAN ANALISIS SPASIAL PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if 'spatial_data' in locals() and spatial_data is not None:\n",
    "        print(f\"Total lokasi yang dianalisis: {len(spatial_data)}\")\n",
    "        print(f\"Training locations: {len(spatial_data[spatial_data['dataset'] == 'training'])}\")\n",
    "        print(f\"Test locations: {len(spatial_data[spatial_data['dataset'] == 'test'])}\")\n",
    "        print(f\"Presence locations: {len(spatial_data[spatial_data['class'] == 1])}\")\n",
    "        print(f\"Absence locations: {len(spatial_data[spatial_data['class'] == 0])}\")\n",
    "        \n",
    "        print(f\"\\nPerformance Statistics:\")\n",
    "        print(f\"Mean predicted probability: {spatial_data['predicted_prob'].mean():.4f}\")\n",
    "        print(f\"Mean prediction error: {spatial_data['prediction_error'].mean():.4f}\")\n",
    "        print(f\"Mean confidence: {spatial_data['confidence'].mean():.4f}\")\n",
    "        \n",
    "        print(f\"\\nGeographic Range:\")\n",
    "        print(f\"Longitude: {spatial_data['longitude'].min():.4f} to {spatial_data['longitude'].max():.4f}\")\n",
    "        print(f\"Latitude: {spatial_data['latitude'].min():.4f} to {spatial_data['latitude'].max():.4f}\")\n",
    "        \n",
    "        if 'grid_stats' in locals() and grid_stats is not None:\n",
    "            print(f\"\\nGrid Analysis:\")\n",
    "            print(f\"Total grids with sufficient data: {len(grid_stats)}\")\n",
    "            print(f\"Best performing grid: {grid_stats[('predicted_prob', 'mean')].idxmax()}\")\n",
    "            print(f\"Worst performing grid: {grid_stats[('predicted_prob', 'mean')].idxmin()}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ All spatial analysis results have been saved successfully!\")\n",
    "    print(f\"âœ“ Interactive maps and visualizations are available for exploration\")\n",
    "\n",
    "# Jalankan fungsi penyimpanan\n",
    "save_spatial_analysis_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4cc17",
   "metadata": {},
   "source": [
    "## 13. Kesimpulan dan Rekomendasi Analisis Sebaran Performance vs Number of Variables\n",
    "\n",
    "### Ringkasan Analisis Komprehensif:\n",
    "\n",
    "Analisis sebaran performance vs number of variables telah berhasil dilakukan dengan pendekatan multi-dimensional yang mencakup:\n",
    "\n",
    "#### 1. **Analisis Sebaran Performance (Distribution Analysis)**:\n",
    "- **Box Plot & Violin Plot**: Menunjukkan distribusi quartile dan density untuk setiap jumlah variabel\n",
    "- **Histogram Analysis**: Analisis frekuensi distribusi dengan mean dan median\n",
    "- **Statistical Tests**: Uji normalitas (Shapiro-Wilk, D'Agostino, Kolmogorov-Smirnov)\n",
    "- **Performance Stability**: Analisis Coefficient of Variation (CV) untuk mengukur stabilitas\n",
    "\n",
    "#### 2. **Analisis Spasial Performance (Spatial Analysis)**:\n",
    "- **Geographic Distribution**: Analisis sebaran performa berdasarkan koordinat geografis\n",
    "- **Regional Performance**: Perbandingan performa antar region/area menggunakan grid analysis\n",
    "- **Spatial Clustering**: K-means clustering berdasarkan koordinat dan performa\n",
    "- **Spatial Correlation**: Korelasi antara lokasi geografis dan performa model\n",
    "- **Interactive Mapping**: Peta interaktif dengan Folium dan Plotly\n",
    "\n",
    "#### 3. **Visualisasi Komprehensif**:\n",
    "- **Static Maps**: Scatter plots dengan color coding untuk berbagai metrik\n",
    "- **Interactive Maps**: Peta interaktif dengan popup information dan heatmap\n",
    "- **3D Visualization**: Scatter plot 3D untuk analisis spasial mendalam\n",
    "- **Box Plot Analysis**: Analisis sebaran per region dengan outlier detection\n",
    "- **Correlation Heatmaps**: Matriks korelasi antar variabel dan performa\n",
    "\n",
    "#### 4. **Analisis Statistik Mendalam**:\n",
    "- **Descriptive Statistics**: Mean, std, min, max, median, quartiles untuk setiap kelompok\n",
    "- **Correlation Analysis**: Pearson dan Spearman correlation\n",
    "- **Linear Regression**: Analisis tren dengan RÂ² dan p-value\n",
    "- **ANOVA & Kruskal-Wallis**: Uji perbedaan signifikan antar kelompok\n",
    "- **Normality Tests**: Uji distribusi normal untuk setiap metrik\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "#### **Distribusi Performance**:\n",
    "1. **Variabilitas Performance**: Sebaran performa bervariasi signifikan untuk setiap jumlah variabel\n",
    "2. **Outlier Detection**: Beberapa konfigurasi menunjukkan outlier yang perlu diperhatikan\n",
    "3. **Distribution Patterns**: Beberapa metrik menunjukkan distribusi normal, lainnya tidak\n",
    "4. **Stability Analysis**: CV analysis mengidentifikasi konfigurasi yang paling stabil\n",
    "\n",
    "#### **Spatial Patterns**:\n",
    "1. **Geographic Bias**: Teridentifikasi pola spasial dalam performa model\n",
    "2. **Regional Differences**: Perbedaan performa yang signifikan antar region\n",
    "3. **Clustering Patterns**: Lokasi dengan performa serupa cenderung berkelompok\n",
    "4. **Spatial Correlation**: Korelasi antara koordinat geografis dan performa model\n",
    "\n",
    "#### **Optimal Configuration**:\n",
    "1. **Efficiency Score**: Jumlah variabel optimal berdasarkan mean - std\n",
    "2. **Stability Priority**: Konfigurasi dengan CV terendah untuk stabilitas\n",
    "3. **Performance Range**: Analisis konsistensi performa dalam aplikasi nyata\n",
    "4. **Trade-off Analysis**: Balance antara performa tinggi dan stabilitas\n",
    "\n",
    "### Rekomendasi Praktis:\n",
    "\n",
    "#### **1. Pemilihan Jumlah Variabel Optimal**:\n",
    "- **Gunakan analisis efficiency score** untuk menentukan jumlah variabel optimal\n",
    "- **Pertimbangkan stabilitas** dengan memilih konfigurasi CV terendah\n",
    "- **Validasi dengan independent data** untuk memastikan generalisasi\n",
    "- **Monitor performance range** untuk memastikan konsistensi\n",
    "\n",
    "#### **2. Analisis Spasial**:\n",
    "- **Identifikasi bias spasial** dan pertimbangkan spatial weighting\n",
    "- **Gunakan regional analysis** untuk memahami perbedaan performa\n",
    "- **Implementasi spatial validation** untuk model yang lebih robust\n",
    "- **Pertimbangkan spatial clustering** dalam strategi sampling\n",
    "\n",
    "#### **3. Monitoring dan Validasi**:\n",
    "- **Implementasi continuous monitoring** untuk performa model\n",
    "- **Validasi berkala** dengan data baru\n",
    "- **Dokumentasi pola spasial** untuk referensi future\n",
    "- **Update model** berdasarkan analisis sebaran terbaru\n",
    "\n",
    "### File Output yang Dihasilkan:\n",
    "\n",
    "#### **Visualisasi**:\n",
    "- **Static Figures**: 6 figure komprehensif (distribution, histogram, comprehensive, spatial maps)\n",
    "- **Interactive Maps**: HTML maps dengan Folium dan Plotly\n",
    "- **3D Visualizations**: Scatter plot 3D untuk analisis spasial\n",
    "\n",
    "#### **Data Export**:\n",
    "- **Performance Data**: CSV dengan data performa lengkap\n",
    "- **Spatial Data**: CSV dengan data spasial dan koordinat\n",
    "- **Statistical Results**: CSV dengan hasil analisis statistik\n",
    "- **Grid Statistics**: CSV dengan statistik per grid region\n",
    "- **Correlation Matrices**: CSV dengan matriks korelasi\n",
    "\n",
    "#### **Summary Reports**:\n",
    "- **Console Output**: Ringkasan statistik dan interpretasi\n",
    "- **Performance Summary**: Perbandingan performa antar konfigurasi\n",
    "- **Spatial Summary**: Ringkasan analisis spasial dan regional\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Implementasi Rekomendasi**: Gunakan jumlah variabel optimal yang teridentifikasi\n",
    "2. **Spatial Validation**: Implementasi validasi spasial untuk model yang lebih robust\n",
    "3. **Continuous Monitoring**: Setup monitoring sistem untuk performa model\n",
    "4. **Documentation**: Dokumentasi lengkap untuk referensi future\n",
    "5. **Publication**: Siapkan hasil untuk publikasi atau presentasi\n",
    "\n",
    "Analisis ini memberikan pemahaman mendalam dan komprehensif tentang bagaimana performa model berubah seiring dengan perubahan jumlah variabel, dengan fokus khusus pada sebaran dan pola spasial yang dapat digunakan untuk optimasi model dan strategi validasi yang lebih baik.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aciar",
   "language": "python",
   "name": "aciar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
