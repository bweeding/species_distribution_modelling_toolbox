{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a39a75e",
   "metadata": {},
   "source": [
    "# Model Evaluation (Weighted) â€“ Notebook Guide\n",
    "\n",
    "This notebook evaluates models with class/observation weights applied.\n",
    "\n",
    "## What this notebook does\n",
    "- Compute weighted metrics (e.g., weighted AUC, threshold metrics)\n",
    "- Plot diagnostic figures considering weights\n",
    "- Summarize results per model/run and export\n",
    "\n",
    "## Inputs\n",
    "- Predictions/scores, ground-truth labels, and weights per observation\n",
    "- Optional: CV fold info or test set indicators\n",
    "\n",
    "## Workflow\n",
    "1. Load predictions, labels, and weights\n",
    "2. Validate alignment and handle missing values\n",
    "3. Compute weighted metrics across thresholds/folds\n",
    "4. Plot weighted ROC/curves and summaries\n",
    "5. Save metrics tables and figures\n",
    "\n",
    "## Outputs\n",
    "- Weighted per-model/per-fold metrics tables\n",
    "- Plots reflecting weights\n",
    "- CSV/JSON exports for downstream use\n",
    "\n",
    "## Notes\n",
    "- Ensure weights are normalized or in intended scale\n",
    "- Use consistent preprocessing as training\n",
    "- Fix random seeds for reproducibility where applicable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce62eb",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook evaluates weighted SDMs with metrics and plots, mirroring standard evaluation but accounting for weights in analysis where relevant.\n",
    "\n",
    "- Key steps: load weighted predictions, compute metrics, plot curves, thresholds, reporting\n",
    "- Inputs: weighted model predictions and labels\n",
    "- Outputs: evaluation tables and plots\n",
    "- Run order: After weighted model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b15204-7112-48a4-97c4-bc90bc40550d",
   "metadata": {},
   "source": [
    "# Weighted MaxEnt Model Evaluation and Performance Assessment\n",
    "\n",
    "This notebook provides comprehensive evaluation of **weighted MaxEnt species distribution models**, focusing on performance assessment that accounts for sample weights and data quality differences. Unlike standard model evaluation, this version incorporates **weighted metrics** to properly assess model performance when training data has been weighted.\n",
    "\n",
    "## Key Features of Weighted Model Evaluation:\n",
    "\n",
    "### 1. **Weighted Performance Metrics**:\n",
    "- **Weighted AUC**: Area Under ROC Curve accounting for sample weights\n",
    "- **Weighted PR-AUC**: Precision-Recall AUC with weight integration\n",
    "- **Weighted Sensitivity/Specificity**: Performance metrics adjusted for data quality\n",
    "- **Weighted Precision/Recall**: Classification metrics incorporating sample weights\n",
    "\n",
    "### 2. **Advanced Evaluation Approaches**:\n",
    "- **Cross-Validation**: K-fold validation with weighted samples\n",
    "- **Spatial Validation**: Geographic partitioning with weight consideration\n",
    "- **Temporal Validation**: Time-based splits accounting for temporal weights\n",
    "- **Bootstrap Validation**: Resampling with weight preservation\n",
    "\n",
    "### 3. **Bias Assessment**:\n",
    "- **Spatial Bias Analysis**: Evaluate model performance across different regions\n",
    "- **Temporal Bias Assessment**: Performance across different time periods\n",
    "- **Source Bias Evaluation**: Performance across different data sources\n",
    "- **Quality Bias Analysis**: Performance across different data quality levels\n",
    "\n",
    "## Applications:\n",
    "- **Model Validation**: Comprehensive assessment of weighted model performance\n",
    "- **Bias Detection**: Identify remaining biases after weighting\n",
    "- **Performance Comparison**: Compare weighted vs. unweighted models\n",
    "- **Quality Control**: Validate that weighting improves model reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f4dda-d3ca-47d5-91ad-7caa0a434170",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### WEIGHTED MODEL EVALUATION CONFIGURATION - MODIFY AS NEEDED ###############\n",
    "\n",
    "# Species and region settings for weighted model evaluation\n",
    "#specie = 'leptocybe-invasa'  # Target species: 'leptocybe-invasa' or 'thaumastocoris-peregrinus'\n",
    "#pseudoabsence = 'random'  # Background point strategy: 'random', 'biased', 'biased-land-cover'\n",
    "#training = 'east-asia'  # Training region: 'sea', 'australia', 'east-asia', etc.\n",
    "#interest = 'south-east-asia'  # Test region: can be same as training or different\n",
    "#savefig = True  # Save generated evaluation plots and metrics\n",
    "\n",
    "# Environmental variable configuration\n",
    "bio = bio1  # Bioclimatic variable identifier\n",
    "\n",
    "# Evaluation settings (specific to weighted model evaluation)\n",
    "# evaluation_method = 'cross_validation'  # 'cross_validation', 'spatial_validation', 'temporal_validation'\n",
    "# n_folds = 5  # Number of folds for cross-validation\n",
    "# spatial_buffer = 100  # Buffer distance (km) for spatial validation\n",
    "# temporal_split = 0.7  # Proportion of data for training in temporal validation\n",
    "\n",
    "# Weighted metrics configuration\n",
    "# include_weighted_metrics = True  # Calculate weighted performance metrics\n",
    "# include_unweighted_metrics = True  # Calculate standard metrics for comparison\n",
    "# weight_threshold = 0.1  # Minimum weight threshold for sample inclusion\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e46ce-499c-4676-9ff0-f796122a3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os  # File system operations\n",
    "\n",
    "import numpy as np  # Numerical computing\n",
    "import xarray as xr  # Multi-dimensional labeled arrays (raster data)\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import geopandas as gpd  # Geospatial data handling\n",
    "\n",
    "import elapid as ela  # Species distribution modeling library\n",
    "\n",
    "from shapely import wkt  # Well-Known Text (WKT) geometry parsing\n",
    "from elapid import utils  # Utility functions for elapid\n",
    "from sklearn import metrics, inspection  # Machine learning metrics and model inspection\n",
    "\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warning messages for cleaner output\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6724e-cd4f-4099-aba5-4b81214f135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_layout(nplots):\n",
    "    \"\"\"\n",
    "    Calculate optimal subplot layout for given number of plots\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nplots : int\n",
    "        Number of plots to arrange\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ncols, nrows : tuple\n",
    "        Number of columns and rows for subplot layout\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate square root and round up for balanced layout\n",
    "    ncols = min(int(np.ceil(np.sqrt(nplots))), 4)  # Max 4 columns\n",
    "    nrows = int(np.ceil(nplots / ncols))  # Calculate rows needed\n",
    "    \n",
    "    return ncols, nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6db49b-be58-4919-b395-1e6978805f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SET UP FILE PATHS\n",
    "# =============================================================================\n",
    "# Define directory structure for organizing weighted model evaluation outputs\n",
    "\n",
    "docs_path = os.path.join(os.path.dirname(os.getcwd()), 'docs')  # Documentation directory\n",
    "out_path = os.path.join(os.path.dirname(os.getcwd()), 'out', specie)  # Species-specific output directory\n",
    "figs_path = os.path.join(os.path.dirname(os.getcwd()), 'figs')  # Figures directory\n",
    "output_path = os.path.join(out_path, 'output')  # Model output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7a900-85cd-41d4-87e6-7179c9320233",
   "metadata": {},
   "source": [
    "## 1. Weighted Training Model Performance Assessment\n",
    "\n",
    "This section evaluates the performance of the weighted MaxEnt model on the training data. Key aspects include:\n",
    "\n",
    "### **Weighted vs. Unweighted Metrics**:\n",
    "- **Standard Metrics**: Traditional AUC, PR-AUC, sensitivity, specificity\n",
    "- **Weighted Metrics**: Performance metrics accounting for sample weights\n",
    "- **Comparison Analysis**: Evaluate improvement from weighting approach\n",
    "\n",
    "### **Performance Indicators**:\n",
    "- **ROC-AUC**: Area Under Receiver Operating Characteristic curve\n",
    "- **PR-AUC**: Area Under Precision-Recall curve (important for imbalanced data)\n",
    "- **Sensitivity**: True Positive Rate (ability to detect presences)\n",
    "- **Specificity**: True Negative Rate (ability to detect absences)\n",
    "- **Precision**: Positive Predictive Value\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### **Weighted Evaluation Benefits**:\n",
    "- **Quality-Aware Assessment**: Metrics reflect data quality differences\n",
    "- **Bias-Corrected Performance**: Reduced influence of low-quality samples\n",
    "- **Robust Validation**: More reliable performance estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17562db-88e4-4b2c-97d4-6095e6fd3e7b",
   "metadata": {},
   "source": [
    "## References for Species Distribution Model Evaluation\n",
    "\n",
    "### **Model Output Interpretation**:\n",
    "- [SDM Model Outputs Interpretation](https://support.ecocommons.org.au/support/solutions/articles/6000256107-interpretation-of-sdm-model-outputs)\n",
    "- [Presence-Only Prediction in GIS](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/how-presence-only-prediction-works.htm)\n",
    "- [MaxEnt 101: Species Distribution Modeling](https://www.esri.com/arcgis-blog/products/arcgis-pro/analytics/presence-only-prediction-maxent-101-using-gis-to-model-species-distribution/)\n",
    "\n",
    "### **Performance Metrics**:\n",
    "- [ROC Curves Demystified](https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0)\n",
    "- [Precision-Recall AUC Guide](https://www.aporia.com/learn/ultimate-guide-to-precision-recall-auc-understanding-calculating-using-pr-auc-in-ml/)\n",
    "- [F1-Score, Accuracy, ROC-AUC, and PR-AUC Metrics](https://deepchecks.com/f1-score-accuracy-roc-auc-and-pr-auc-metrics-for-models/)\n",
    "\n",
    "### **Weighted Model Evaluation**:\n",
    "- **Sample Weighting**: How to properly evaluate models trained with sample weights\n",
    "- **Bias Correction**: Assessing the effectiveness of weighting strategies\n",
    "- **Quality Integration**: Incorporating data quality into performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa4635-ab08-4b3d-9fa0-07271788cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD WEIGHTED MODEL AND TRAINING DATA\n",
    "# =============================================================================\n",
    "# Load the trained weighted MaxEnt model and associated training data for evaluation\n",
    "\n",
    "# Build experiment directory name (keeps runs organized by config)\n",
    "# Alternate naming (older): 'exp_%s_%s_%s' % (pseudoabsence, training, interest)\n",
    "experiment_name = 'exp_%s_%s_%s_%s_%s' % (model_prefix, pseudoabsence, training, topo, ndvi)\n",
    "exp_path = os.path.join(output_path, experiment_name)  # Path to experiment directory\n",
    "\n",
    "# Construct expected filenames produced during training for this run\n",
    "train_input_data_name = '%s_model-train_input-data_%s_%s_%s_%s_%s.csv' % (model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "run_name = '%s_model-train_%s_%s_%s_%s_%s.ela' % (model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "nc_name = '%s_model-train_%s_%s_%s_%s_%s.nc' % (model_prefix, specie, pseudoabsence, training, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428443d1-2a5b-403e-a99c-a3395954e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD TRAINING DATA WITH SAMPLE WEIGHTS\n",
    "# =============================================================================\n",
    "# Load training data including sample weights for weighted model evaluation\n",
    "\n",
    "# Load training data from CSV file (index_col=0 to drop old index column)\n",
    "df = pd.read_csv(os.path.join(exp_path, train_input_data_name), index_col=0)\n",
    "# Parse WKT strings into shapely geometries\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# Wrap as GeoDataFrame with WGS84 CRS\n",
    "train = gpd.GeoDataFrame(df, crs='EPSG:4326')\n",
    "\n",
    "# Split predictors/labels/weights for weighted evaluation\n",
    "x_train = train.drop(columns=['class', 'SampleWeight', 'geometry'])  # Environmental variables only\n",
    "y_train = train['class']  # Presence/absence labels (0/1)\n",
    "sample_weight_train = train['SampleWeight']  # Sample weights aligned with rows\n",
    "\n",
    "# Load fitted weighted MaxEnt model\n",
    "model_train = utils.load_object(os.path.join(exp_path, run_name))\n",
    "\n",
    "# Predict probabilities on training set (for curves/metrics)\n",
    "y_train_predict = model_train.predict(x_train)\n",
    "# Optional: impute NaN probabilities to 0.5 (neutral)\n",
    "# y_train_predict = np.nan_to_num(y_train_predict, nan=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abc1a9-3db2-4960-ad8f-e042eb214fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training performance metrics\n",
    "\n",
    "# ROC curve and AUC (unweighted vs weighted)\n",
    "# fpr/tpr are computed from predicted probabilities; weights adjust contribution per sample\n",
    "fpr_train, tpr_train, thresholds = metrics.roc_curve(y_train, y_train_predict)\n",
    "auc_train = metrics.roc_auc_score(y_train, y_train_predict)\n",
    "auc_train_weighted = metrics.roc_auc_score(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "\n",
    "# Precision-Recall curve and PR-AUC (more informative on class imbalance)\n",
    "precision_train, recall_train, _ = metrics.precision_recall_curve(y_train, y_train_predict)\n",
    "pr_auc_train = metrics.auc(recall_train, precision_train)\n",
    "# Weighted PR curve uses sample weights to compute precision/recall\n",
    "precision_train_w, recall_train_w, _ = metrics.precision_recall_curve(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "pr_auc_train_weighted = metrics.auc(recall_train_w, precision_train_w)\n",
    "\n",
    "# Report metrics\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score  : {auc_train_weighted:0.3f}\")\n",
    "print(f\"PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cccf4-2b98-44d3-8725-227a49bb3c31",
   "metadata": {},
   "source": [
    "|  |  | Specie existance |  |\n",
    "| ------ | :-------: | :------: | :-------: |\n",
    "| |  | **+** | **--** |\n",
    "| **Specie observed** | **+** | True Positive (TP) | False Positive (FP) |\n",
    "| | **--** | False Negative (FN) | True Negative (TN) |\n",
    "| | | **All existing species (TP + FN)** | **All non-existing species (FP + TN)** |\n",
    "\n",
    "\n",
    "$$TPR = \\frac{TP}{TP + FN}$$\n",
    "$$FPR = \\frac{FP}{FP + TN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588ba66-5615-4d10-b8db-26ab26462e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training distributions and curves\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Left: Predicted probability distributions for presence vs pseudo-absence\n",
    "ax[0].hist(y_train_predict[y_train == 0], bins=np.linspace(0, 1, int((y_train == 0).sum() / 100 + 1)),\n",
    "           density=True, color='tab:red', alpha=0.7, label='pseudo-absence')\n",
    "ax[0].hist(y_train_predict[y_train == 1], bins=np.linspace(0, 1, int((y_train == 1).sum() / 10 + 1)),\n",
    "           density=True, color='tab:green', alpha=0.7, label='presence')\n",
    "ax[0].set_xlabel('Relative Occurrence Probability')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_title('Probability Distribution')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "# Middle: ROC curve (random vs perfect baselines + model)\n",
    "ax[1].plot([0, 1], [0, 1], '--', label='AUC score: 0.5 (No Skill)', color='gray')\n",
    "ax[1].text(0.4, 0.4, 'random classifier', fontsize=12, color='gray', rotation=45, rotation_mode='anchor',\n",
    "           horizontalalignment='left', verticalalignment='bottom', transform=ax[1].transAxes)\n",
    "ax[1].plot([0, 0, 1], [0, 1, 1], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[1].text(0, 1, '  perfect classifier', fontsize=12, color='tab:blue', horizontalalignment='left', verticalalignment='bottom')\n",
    "ax[1].scatter(0, 1, marker='*', s=100, color='tab:blue')\n",
    "# Overlay model ROC (unweighted and weighted AUC labels)\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC score: {auc_train:0.3f}', color='tab:orange')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC Weighted score: {auc_train_weighted:0.3f}', color='tab:cyan', linestyle='-.')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('MaxEnt ROC Curve')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Right: Precision-Recall curve (random/perfect baselines + model)\n",
    "ax[2].plot([0, 1], [0.5, 0.5], '--', color='gray', label='AUC score: 0.5 (No Skill)')\n",
    "ax[2].text(0.5, 0.52, 'random classifier', fontsize=12, color='gray', horizontalalignment='center', verticalalignment='center')\n",
    "ax[2].plot([0, 1, 1], [1, 1, 0], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[2].text(1, 1, 'perfect classifier  ', fontsize=12, color='tab:blue', horizontalalignment='right', verticalalignment='bottom')\n",
    "ax[2].scatter(1, 1, marker='*', s=100, color='tab:blue')\n",
    "# Overlay model PR curves (unweighted and weighted AUC labels)\n",
    "ax[2].plot(recall_train, precision_train, label=f'AUC score: {pr_auc_train:0.3f}', color='tab:orange')\n",
    "ax[2].plot(recall_train_w, precision_train_w, label=f\"AUC Weighted score: {pr_auc_train_weighted:0.3f}\", color='tab:cyan', linestyle='-.')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set_xlabel('Recall')\n",
    "ax[2].set_ylabel('Precision')\n",
    "ax[2].set_title('MaxEnt PR Curve')\n",
    "ax[2].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b30af-f85b-4419-baa9-ad808d2dfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figures if requested. Uses different filename patterns for current vs future scenarios.\n",
    "# Note: 'models' is used to gate inclusion of model prefix; ensure it exists in your session.\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:  # include model identifier when available\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: omit model prefix when not specified\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993984c-f068-4953-8a73-841a09f30b72",
   "metadata": {},
   "source": [
    "## 2. Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f358d3-224d-4be8-a1d2-82f1f3457b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data_name = '%s_model-test_input-data_%s_%s_%s_%s_%s.csv' %(model_prefix, specie, pseudoabsence, interest, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7166b-a73a-441c-a066-ec8e957e04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load held-out test dataset for evaluation\n",
    "# Note: index_col=0 drops the old index saved during export\n",
    "df = pd.read_csv(os.path.join(exp_path, test_input_data_name), index_col=0)\n",
    "# Convert WKT geometry back to shapely objects\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# Wrap as GeoDataFrame (WGS84 CRS)\n",
    "test = gpd.GeoDataFrame(df, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550482cb-b0da-489c-894d-711a890cd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split predictors/labels/weights for test set\n",
    "x_test = test.drop(columns=['class', 'SampleWeight', 'geometry'])\n",
    "y_test = test['class']\n",
    "sample_weight_test = test['SampleWeight']\n",
    "\n",
    "# Predict probabilities on the test set using the trained model\n",
    "y_test_predict = model_train.predict(x_test)\n",
    "# Optional: impute NaN probabilities to 0.5 if present\n",
    "# y_test_predict = np.nan_to_num(y_test_predict, nan=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c432c6-86ef-421f-a4fe-2f1cee794e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set metrics: ROC/PR curves and AUCs (unweighted vs weighted)\n",
    "# ROC\n",
    "fpr_test, tpr_test, _ = metrics.roc_curve(y_test, y_test_predict)\n",
    "auc_test = metrics.roc_auc_score(y_test, y_test_predict)\n",
    "auc_test_weighted = metrics.roc_auc_score(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "\n",
    "# Precision-Recall (PR)\n",
    "precision_test, recall_test, _ = metrics.precision_recall_curve(y_test, y_test_predict)\n",
    "pr_auc_test = metrics.auc(recall_test, precision_test)\n",
    "precision_test_w, recall_test_w, _ = metrics.precision_recall_curve(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "pr_auc_test_weighted = metrics.auc(recall_test_w, precision_test_w)\n",
    "\n",
    "# Print summary of training vs test for quick comparison\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score: {auc_train_weighted:0.3f}\")\n",
    "print(f\"Test ROC-AUC score: {auc_test:0.3f}\")\n",
    "print(f\"Test ROC-AUC Weighted score: {auc_test_weighted:0.3f}\")\n",
    "\n",
    "print(f\"Training PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"Training PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")\n",
    "print(f\"Test PR-AUC Score: {pr_auc_test:0.3f}\")\n",
    "print(f\"Test PR-AUC Weighted Score: {pr_auc_test_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22fa49-a574-4a0f-b998-4028ab09cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test distributions and curves alongside training for comparison\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Left: Predicted probability distributions on test set\n",
    "ax[0].hist(y_test_predict[y_test == 0], bins=np.linspace(0, 1, int((y_test == 0).sum() / 100 + 1)),\n",
    "           density=True, color='tab:red', alpha=0.7, label='pseudo-absence')\n",
    "ax[0].hist(y_test_predict[y_test == 1], bins=np.linspace(0, 1, int((y_test == 1).sum() / 10 + 1)),\n",
    "           density=True, color='tab:green', alpha=0.7, label='presence')\n",
    "ax[0].set_xlabel('Relative Occurrence Probability')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_title('Probability Distribution')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "# Middle: ROC curves (train vs test, with weighted variants labeled)\n",
    "ax[1].plot([0, 1], [0, 1], '--', label='AUC score: 0.5 (No Skill)', color='gray')\n",
    "ax[1].text(0.4, 0.4, 'random classifier', fontsize=12, color='gray', rotation=45, rotation_mode='anchor',\n",
    "           horizontalalignment='left', verticalalignment='bottom', transform=ax[1].transAxes)\n",
    "ax[1].plot([0, 0, 1], [0, 1, 1], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[1].text(0, 1, '  perfect classifier', fontsize=12, color='tab:blue', horizontalalignment='left', verticalalignment='bottom')\n",
    "ax[1].scatter(0, 1, marker='*', s=100, color='tab:blue')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC train score: {auc_train:0.3f}', color='tab:orange')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC Weighted train score: {auc_train_weighted:0.3f}', color='tab:cyan', linestyle='-.')\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'AUC test score: {auc_test:0.3f}', color='tab:green')\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'AUC Weighted test score: {auc_test_weighted:0.3f}', color='tab:olive', linestyle='-.')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('MaxEnt ROC Curve')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Right: PR curves (train vs test)\n",
    "ax[2].plot([0, 1], [0.5, 0.5], '--', color='gray', label='AUC score: 0.5 (No Skill)')\n",
    "ax[2].text(0.5, 0.52, 'random classifier', fontsize=12, color='gray', horizontalalignment='center', verticalalignment='center')\n",
    "ax[2].plot([0, 1, 1], [1, 1, 0], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[2].text(1, 1, 'perfect classifier  ', fontsize=12, color='tab:blue', horizontalalignment='right', verticalalignment='bottom')\n",
    "ax[2].scatter(1, 1, marker='*', s=100, color='tab:blue')\n",
    "ax[2].plot(recall_train, precision_train, label=f'AUC train score: {pr_auc_train:0.3f}', color='tab:orange')\n",
    "ax[2].plot(recall_train_w, precision_train_w, label=f\"AUC train Weighted score: {pr_auc_train_weighted:0.3f}\", color='tab:cyan', linestyle='-.')\n",
    "ax[2].plot(recall_test, precision_test, label=f'AUC test score: {pr_auc_test:0.3f}', color='tab:green')\n",
    "ax[2].plot(recall_test_w, precision_test_w, label=f'AUC test Weighted score: {pr_auc_test_weighted:0.3f}', color='tab:olive', linestyle='-.')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set_xlabel('Recall')\n",
    "ax[2].set_ylabel('Precision')\n",
    "ax[2].set_title('MaxEnt PR Curve')\n",
    "ax[2].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b39edc-a114-46ba-a329-503387ab4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test figures if requested (future vs current naming handled similarly to training)\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s_future.png' % (specie, interest, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_future.png' % (specie, interest, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if model_prefix:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s.png' % (specie, interest, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s.png' % (specie, interest, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bfd716-825b-4ce8-82aa-fce7f973fcf8",
   "metadata": {},
   "source": [
    "## 3. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6a70d-105b-4267-bc58-1df7bb8e8aab",
   "metadata": {},
   "source": [
    "### 3.2 Partial dependence plot/ Response curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c51647-a6b0-4329-a077-fb2d3d5aca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = model_train.partial_dependence_plot(x, labels=labels, dpi=100, n_bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeacf3a",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Variable Importance Analysis\n",
    "\n",
    "This section performs a thorough analysis of variable importance by:\n",
    "\n",
    "1. **Initial Analysis**: Running the model with all 19 bioclimatic variables to establish baseline importance\n",
    "2. **Iterative Removal**: Systematically removing the least important variables until we reach ~5 most important variables\n",
    "3. **Performance Tracking**: Monitoring model performance as variables are removed\n",
    "4. **Final Recommendations**: Identifying the optimal subset of variables for the species distribution model\n",
    "\n",
    "### Methodology:\n",
    "- **Permutation Importance**: Measures the drop in model performance when each variable is randomly shuffled\n",
    "- **Iterative Backward Elimination**: Removes least important variables one at a time\n",
    "- **Performance Monitoring**: Tracks AUC, PR-AUC, and other metrics throughout the process\n",
    "- **Cross-Validation**: Ensures robust importance estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b286503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE VARIABLE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Initialize storage for results\n",
    "importance_results = {}\n",
    "performance_history = {}\n",
    "variable_subsets = {}\n",
    "\n",
    "# Get current variable names from training data\n",
    "current_variables = list(x_train.columns)\n",
    "print(f\"Starting with {len(current_variables)} variables:\")\n",
    "print(f\"Variables: {current_variables}\")\n",
    "\n",
    "# Store initial performance metrics\n",
    "initial_metrics = {\n",
    "    'train_auc': auc_train,\n",
    "    'train_auc_weighted': auc_train_weighted,\n",
    "    'train_pr_auc': pr_auc_train,\n",
    "    'train_pr_auc_weighted': pr_auc_train_weighted,\n",
    "    'test_auc': auc_test,\n",
    "    'test_auc_weighted': auc_test_weighted,\n",
    "    'test_pr_auc': pr_auc_test,\n",
    "    'test_pr_auc_weighted': pr_auc_test_weighted\n",
    "}\n",
    "\n",
    "performance_history['all_variables'] = initial_metrics\n",
    "variable_subsets['all_variables'] = current_variables.copy()\n",
    "\n",
    "print(f\"\\nInitial Performance (All {len(current_variables)} variables):\")\n",
    "print(f\"Training AUC: {auc_train:.3f} (weighted: {auc_train_weighted:.3f})\")\n",
    "print(f\"Training PR-AUC: {pr_auc_train:.3f} (weighted: {pr_auc_train_weighted:.3f})\")\n",
    "print(f\"Test AUC: {auc_test:.3f} (weighted: {auc_test_weighted:.3f})\")\n",
    "print(f\"Test PR-AUC: {pr_auc_test:.3f} (weighted: {pr_auc_test_weighted:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46769501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ITERATIVE VARIABLE REMOVAL FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def iterative_variable_removal(x_train, y_train, sample_weight_train, x_test, y_test, sample_weight_test, \n",
    "                              target_variables=5, min_variables=3):\n",
    "    \"\"\"\n",
    "    Iteratively remove least important variables until reaching target number.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train, y_train, sample_weight_train : training data\n",
    "    x_test, y_test, sample_weight_test : test data  \n",
    "    target_variables : int, target number of variables to keep\n",
    "    min_variables : int, minimum number of variables to keep\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict, containing importance rankings and performance history\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'importance_rankings': {},\n",
    "        'performance_history': {},\n",
    "        'removed_variables': [],\n",
    "        'final_variables': []\n",
    "    }\n",
    "    \n",
    "    current_x_train = x_train.copy()\n",
    "    current_x_test = x_test.copy()\n",
    "    current_vars = list(current_x_train.columns)\n",
    "    iteration = 0\n",
    "    \n",
    "    print(f\"Starting iterative removal from {len(current_vars)} to {target_variables} variables...\")\n",
    "    \n",
    "    while len(current_vars) > max(target_variables, min_variables):\n",
    "        iteration += 1\n",
    "        print(f\"\\n--- Iteration {iteration}: {len(current_vars)} variables remaining ---\")\n",
    "        \n",
    "        # Train model with current variables\n",
    "        model_iter = ela.MaxentModel()\n",
    "        model_iter.fit(current_x_train, y_train, sample_weight=sample_weight_train)\n",
    "        \n",
    "        # Calculate permutation importance\n",
    "        pi = inspection.permutation_importance(\n",
    "            model_iter, current_x_train, y_train, \n",
    "            sample_weight=sample_weight_train, n_repeats=10\n",
    "        )\n",
    "        \n",
    "        # Get importance scores and rank variables\n",
    "        importance_scores = pi.importances.mean(axis=1)\n",
    "        var_importance = dict(zip(current_vars, importance_scores))\n",
    "        sorted_vars = sorted(var_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Store ranking for this iteration\n",
    "        results['importance_rankings'][f'iteration_{iteration}'] = {\n",
    "            'variables': current_vars.copy(),\n",
    "            'importance_scores': var_importance.copy(),\n",
    "            'sorted_ranking': sorted_vars.copy()\n",
    "        }\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        y_train_pred = model_iter.predict(current_x_train)\n",
    "        y_test_pred = model_iter.predict(current_x_test)\n",
    "        \n",
    "        # Training metrics\n",
    "        train_auc = metrics.roc_auc_score(y_train, y_train_pred)\n",
    "        train_auc_weighted = metrics.roc_auc_score(y_train, y_train_pred, sample_weight=sample_weight_train)\n",
    "        train_precision, train_recall, _ = metrics.precision_recall_curve(y_train, y_train_pred)\n",
    "        train_pr_auc = metrics.auc(train_recall, train_precision)\n",
    "        train_precision_w, train_recall_w, _ = metrics.precision_recall_curve(y_train, y_train_pred, sample_weight=sample_weight_train)\n",
    "        train_pr_auc_weighted = metrics.auc(train_recall_w, train_precision_w)\n",
    "        \n",
    "        # Test metrics\n",
    "        test_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "        test_auc_weighted = metrics.roc_auc_score(y_test, y_test_pred, sample_weight=sample_weight_test)\n",
    "        test_precision, test_recall, _ = metrics.precision_recall_curve(y_test, y_test_pred)\n",
    "        test_pr_auc = metrics.auc(test_recall, test_precision)\n",
    "        test_precision_w, test_recall_w, _ = metrics.precision_recall_curve(y_test, y_test_pred, sample_weight=sample_weight_test)\n",
    "        test_pr_auc_weighted = metrics.auc(test_recall_w, test_precision_w)\n",
    "        \n",
    "        # Store performance\n",
    "        results['performance_history'][f'iteration_{iteration}'] = {\n",
    "            'n_variables': len(current_vars),\n",
    "            'train_auc': train_auc,\n",
    "            'train_auc_weighted': train_auc_weighted,\n",
    "            'train_pr_auc': train_pr_auc,\n",
    "            'train_pr_auc_weighted': train_pr_auc_weighted,\n",
    "            'test_auc': test_auc,\n",
    "            'test_auc_weighted': test_auc_weighted,\n",
    "            'test_pr_auc': test_pr_auc,\n",
    "            'test_pr_auc_weighted': test_pr_auc_weighted\n",
    "        }\n",
    "        \n",
    "        # Print current performance\n",
    "        print(f\"Performance with {len(current_vars)} variables:\")\n",
    "        print(f\"  Train AUC: {train_auc:.3f} (weighted: {train_auc_weighted:.3f})\")\n",
    "        print(f\"  Test AUC: {test_auc:.3f} (weighted: {test_auc_weighted:.3f})\")\n",
    "        print(f\"  Train PR-AUC: {train_pr_auc:.3f} (weighted: {train_pr_auc_weighted:.3f})\")\n",
    "        print(f\"  Test PR-AUC: {test_pr_auc:.3f} (weighted: {test_pr_auc_weighted:.3f})\")\n",
    "        \n",
    "        # Identify least important variable\n",
    "        least_important_var = sorted_vars[-1][0]\n",
    "        least_important_score = sorted_vars[-1][1]\n",
    "        \n",
    "        print(f\"Least important variable: {least_important_var} (importance: {least_important_score:.4f})\")\n",
    "        \n",
    "        # Remove least important variable\n",
    "        current_x_train = current_x_train.drop(columns=[least_important_var])\n",
    "        current_x_test = current_x_test.drop(columns=[least_important_var])\n",
    "        current_vars.remove(least_important_var)\n",
    "        results['removed_variables'].append(least_important_var)\n",
    "        \n",
    "        print(f\"Removed {least_important_var}. Variables remaining: {current_vars}\")\n",
    "    \n",
    "    results['final_variables'] = current_vars.copy()\n",
    "    print(f\"\\nFinal variable set ({len(current_vars)} variables): {current_vars}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN ITERATIVE VARIABLE REMOVAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE VARIABLE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run the iterative removal process\n",
    "start_time = time.time()\n",
    "\n",
    "# Set target to 5 variables (can be adjusted)\n",
    "target_vars = 5\n",
    "min_vars = 3\n",
    "\n",
    "# Run iterative removal\n",
    "removal_results = iterative_variable_removal(\n",
    "    x_train, y_train, sample_weight_train,\n",
    "    x_test, y_test, sample_weight_test,\n",
    "    target_variables=target_vars,\n",
    "    min_variables=min_vars\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nAnalysis completed in {end_time - start_time:.1f} seconds\")\n",
    "\n",
    "# Store results for later analysis\n",
    "importance_results['iterative_removal'] = removal_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALYZE AND VISUALIZE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "# Extract performance trends\n",
    "iterations = list(removal_results['performance_history'].keys())\n",
    "n_vars = [removal_results['performance_history'][iter]['n_variables'] for iter in iterations]\n",
    "train_aucs = [removal_results['performance_history'][iter]['train_auc'] for iter in iterations]\n",
    "test_aucs = [removal_results['performance_history'][iter]['test_auc'] for iter in iterations]\n",
    "train_aucs_weighted = [removal_results['performance_history'][iter]['train_auc_weighted'] for iter in iterations]\n",
    "test_aucs_weighted = [removal_results['performance_history'][iter]['test_auc_weighted'] for iter in iterations]\n",
    "\n",
    "# Add initial performance (all variables)\n",
    "n_vars.insert(0, len(x_train.columns))\n",
    "train_aucs.insert(0, auc_train)\n",
    "test_aucs.insert(0, auc_test)\n",
    "train_aucs_weighted.insert(0, auc_train_weighted)\n",
    "test_aucs_weighted.insert(0, auc_test_weighted)\n",
    "\n",
    "print(\"Performance Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Variables':<12} {'Train AUC':<10} {'Test AUC':<10} {'Train AUC-W':<12} {'Test AUC-W':<12}\")\n",
    "print(\"-\"*60)\n",
    "for i, n_var in enumerate(n_vars):\n",
    "    print(f\"{n_var:<12} {train_aucs[i]:<10.3f} {test_aucs[i]:<10.3f} {train_aucs_weighted[i]:<12.3f} {test_aucs_weighted[i]:<12.3f}\")\n",
    "\n",
    "# Get final variable ranking\n",
    "final_iteration = f\"iteration_{len(iterations)}\"\n",
    "final_ranking = removal_results['importance_rankings'][final_iteration]['sorted_ranking']\n",
    "\n",
    "print(f\"\\nFinal Variable Ranking (Top {len(removal_results['final_variables'])} variables):\")\n",
    "print(\"=\"*60)\n",
    "for i, (var, importance) in enumerate(final_ranking, 1):\n",
    "    print(f\"{i:2d}. {var:<15} (importance: {importance:.4f})\")\n",
    "\n",
    "print(f\"\\nRemoved Variables (in order of removal):\")\n",
    "print(\"=\"*40)\n",
    "for i, var in enumerate(removal_results['removed_variables'], 1):\n",
    "    print(f\"{i:2d}. {var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae31219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE COMPREHENSIVE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create a comprehensive figure showing the analysis results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Comprehensive Variable Importance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Performance vs Number of Variables\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(n_vars, train_aucs, 'o-', label='Train AUC', color='tab:blue', linewidth=2)\n",
    "ax1.plot(n_vars, test_aucs, 's-', label='Test AUC', color='tab:orange', linewidth=2)\n",
    "ax1.plot(n_vars, train_aucs_weighted, 'o--', label='Train AUC (Weighted)', color='tab:blue', alpha=0.7)\n",
    "ax1.plot(n_vars, test_aucs_weighted, 's--', label='Test AUC (Weighted)', color='tab:orange', alpha=0.7)\n",
    "ax1.set_xlabel('Number of Variables')\n",
    "ax1.set_ylabel('AUC Score')\n",
    "ax1.set_title('Model Performance vs Number of Variables')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.invert_xaxis()  # Show decreasing variables\n",
    "\n",
    "# 2. Final Variable Importance (Top 10)\n",
    "ax2 = axes[0, 1]\n",
    "top_vars = final_ranking[:10]  # Top 10 variables\n",
    "var_names = [var[0] for var in top_vars]\n",
    "var_importance = [var[1] for var in top_vars]\n",
    "\n",
    "bars = ax2.barh(range(len(var_names)), var_importance, color='tab:green', alpha=0.7)\n",
    "ax2.set_yticks(range(len(var_names)))\n",
    "ax2.set_yticklabels(var_names)\n",
    "ax2.set_xlabel('Permutation Importance')\n",
    "ax2.set_title('Top 10 Most Important Variables')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, val) in enumerate(zip(bars, var_importance)):\n",
    "    ax2.text(val + 0.001, i, f'{val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# 3. Variable Removal Timeline\n",
    "ax3 = axes[1, 0]\n",
    "removed_vars = removal_results['removed_variables']\n",
    "removal_order = list(range(1, len(removed_vars) + 1))\n",
    "ax3.bar(removal_order, [1] * len(removed_vars), color='tab:red', alpha=0.7)\n",
    "ax3.set_xlabel('Removal Order')\n",
    "ax3.set_ylabel('Variables Removed')\n",
    "ax3.set_title('Variable Removal Timeline')\n",
    "ax3.set_xticks(removal_order)\n",
    "ax3.set_xticklabels([f'#{i}' for i in removal_order])\n",
    "\n",
    "# Add variable names as text\n",
    "for i, var in enumerate(removed_vars):\n",
    "    ax3.text(i + 1, 0.5, var, rotation=90, ha='center', va='center', fontsize=8)\n",
    "\n",
    "# 4. Performance Degradation Analysis\n",
    "ax4 = axes[1, 1]\n",
    "# Calculate performance drop from initial\n",
    "initial_test_auc = test_aucs[0]\n",
    "initial_train_auc = train_aucs[0]\n",
    "test_drop = [(initial_test_auc - auc) / initial_test_auc * 100 for auc in test_aucs]\n",
    "train_drop = [(initial_train_auc - auc) / initial_train_auc * 100 for auc in train_aucs]\n",
    "\n",
    "ax4.plot(n_vars, test_drop, 'o-', label='Test AUC Drop %', color='tab:red', linewidth=2)\n",
    "ax4.plot(n_vars, train_drop, 's-', label='Train AUC Drop %', color='tab:purple', linewidth=2)\n",
    "ax4.set_xlabel('Number of Variables')\n",
    "ax4.set_ylabel('Performance Drop (%)')\n",
    "ax4.set_title('Performance Degradation with Variable Removal')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.invert_xaxis()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the comprehensive analysis figure\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration)\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration)\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration)\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s.png' % (specie, training, bio, iteration)\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    print(f\"Comprehensive analysis figure saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8afc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS TO CSV FOR FURTHER ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Create summary DataFrame for export\n",
    "summary_data = []\n",
    "\n",
    "# Add initial performance (all variables)\n",
    "summary_data.append({\n",
    "    'iteration': 0,\n",
    "    'n_variables': len(x_train.columns),\n",
    "    'variables_removed': 'none',\n",
    "    'train_auc': auc_train,\n",
    "    'train_auc_weighted': auc_train_weighted,\n",
    "    'test_auc': auc_test,\n",
    "    'test_auc_weighted': auc_test_weighted,\n",
    "    'train_pr_auc': pr_auc_train,\n",
    "    'train_pr_auc_weighted': pr_auc_train_weighted,\n",
    "    'test_pr_auc': pr_auc_test,\n",
    "    'test_pr_auc_weighted': pr_auc_test_weighted\n",
    "})\n",
    "\n",
    "# Add iterative removal results\n",
    "for i, iter_key in enumerate(iterations, 1):\n",
    "    perf = removal_results['performance_history'][iter_key]\n",
    "    removed_var = removal_results['removed_variables'][i-1] if i-1 < len(removal_results['removed_variables']) else 'none'\n",
    "    \n",
    "    summary_data.append({\n",
    "        'iteration': i,\n",
    "        'n_variables': perf['n_variables'],\n",
    "        'variables_removed': removed_var,\n",
    "        'train_auc': perf['train_auc'],\n",
    "        'train_auc_weighted': perf['train_auc_weighted'],\n",
    "        'test_auc': perf['test_auc'],\n",
    "        'test_auc_weighted': perf['test_auc_weighted'],\n",
    "        'train_pr_auc': perf['train_pr_auc'],\n",
    "        'train_pr_auc_weighted': perf['train_pr_auc_weighted'],\n",
    "        'test_pr_auc': perf['test_pr_auc'],\n",
    "        'test_pr_auc_weighted': perf['test_pr_auc_weighted']\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save to CSV\n",
    "if savefig:\n",
    "    csv_filename = f'06_variable_importance_analysis_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "    csv_path = os.path.join(figs_path, csv_filename)\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Analysis summary saved to: {csv_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Species: {specie}\")\n",
    "print(f\"Training Region: {training}\")\n",
    "print(f\"Test Region: {interest}\")\n",
    "print(f\"Initial Variables: {len(x_train.columns)}\")\n",
    "print(f\"Final Variables: {len(removal_results['final_variables'])}\")\n",
    "print(f\"Variables Removed: {len(removal_results['removed_variables'])}\")\n",
    "\n",
    "print(f\"\\nFinal Variable Set:\")\n",
    "for i, var in enumerate(removal_results['final_variables'], 1):\n",
    "    print(f\"  {i}. {var}\")\n",
    "\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"  Initial Test AUC: {test_aucs[0]:.3f}\")\n",
    "print(f\"  Final Test AUC: {test_aucs[-1]:.3f}\")\n",
    "print(f\"  Performance Drop: {((test_aucs[0] - test_aucs[-1]) / test_aucs[0] * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Variables:\")\n",
    "for i, (var, importance) in enumerate(final_ranking[:5], 1):\n",
    "    print(f\"  {i}. {var} (importance: {importance:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef32d7f",
   "metadata": {},
   "source": [
    "## 7. Summary and Recommendations\n",
    "\n",
    "### Key Benefits of 10-Iteration Analysis:\n",
    "\n",
    "1. **Robustness**: Multiple iterations account for random variation in model training and importance calculations\n",
    "2. **Statistical Significance**: Provides mean, standard deviation, and confidence intervals for importance scores\n",
    "3. **Consistency Analysis**: Identifies variables that are consistently important across different runs\n",
    "4. **Performance Stability**: Shows how model performance varies with different variable sets\n",
    "\n",
    "### Final Recommendations:\n",
    "\n",
    "1. **Use Most Consistent Variables**: Variables that appear in the final set across most iterations are most reliable\n",
    "2. **Consider Importance + Consistency**: Balance between high importance and high consistency\n",
    "3. **Validate on Independent Data**: Test the selected variables on completely independent datasets\n",
    "4. **Monitor Performance**: Track how the reduced variable set performs in real-world applications\n",
    "\n",
    "### Files Generated:\n",
    "- **Robust analysis figure**: 6-panel visualization showing comprehensive results\n",
    "- **Summary CSV**: Aggregated statistics across all 10 iterations\n",
    "- **Detailed CSV**: Individual results for each iteration\n",
    "- **Console output**: Detailed rankings and recommendations\n",
    "\n",
    "### Next Steps:\n",
    "1. Use the identified top 5 variables for future modeling\n",
    "2. Consider running additional iterations if results are not stable\n",
    "3. Validate the selected variables on independent test data\n",
    "4. Document the ecological significance of the selected variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a733f05-26cb-4da3-a95d-7adc42870020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels and open training output NetCDF for metadata\n",
    "labels = train.drop(columns=['class', 'geometry', 'SampleWeight']).columns.values\n",
    "training_output = xr.open_dataset(os.path.join(exp_path, nc_name))\n",
    "# display(labels)\n",
    "# display(training_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d9454-dce3-4b7d-922b-8f84e37f1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute partial dependence across features\n",
    "# - percentiles bounds the feature grid to observed range (2.5% to 97.5%)\n",
    "# - nbins controls resolution of the curve\n",
    "percentiles = (0.025, 0.975)\n",
    "nbins = 100\n",
    "\n",
    "mean = {}\n",
    "stdv = {}\n",
    "bins = {}\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    # Request individual PDP curves across samples, then summarize\n",
    "    pda = inspection.partial_dependence(\n",
    "        model_train,\n",
    "        x_train,\n",
    "        [idx],\n",
    "        percentiles=percentiles,\n",
    "        grid_resolution=nbins,\n",
    "        kind=\"individual\",\n",
    "    )\n",
    "\n",
    "    mean[label] = pda[\"individual\"][0].mean(axis=0)  # average response\n",
    "    stdv[label] = pda[\"individual\"][0].std(axis=0)   # variability across samples\n",
    "    bins[label] = pda[\"grid_values\"][0]              # feature grid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e805fc6-8c45-4c42-a7fd-255e89a152b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(pda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890d37e-af5a-4f15-966a-02bde1a1f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDPs with uncertainty bands for each predictor\n",
    "ncols, nrows = subplot_layout(len(labels))\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 6, nrows * 6))\n",
    "\n",
    "# Normalize axes list for consistent indexing\n",
    "if (nrows, ncols) == (1, 1):\n",
    "    ax = [axs]\n",
    "else:\n",
    "    ax = axs.ravel()\n",
    "\n",
    "xlabels = training_output.data_vars\n",
    "for iax, label in enumerate(labels):\n",
    "    ax[iax].set_title(label)\n",
    "    try:\n",
    "        ax[iax].set_xlabel(xlabels[label].long_name)\n",
    "    except (ValueError, AttributeError):\n",
    "        ax[iax].set_xlabel('No variable long_name')\n",
    "\n",
    "    # Uncertainty band: mean Â± std across individuals\n",
    "    ax[iax].fill_between(bins[label], mean[label] - stdv[label], mean[label] + stdv[label], alpha=0.25)\n",
    "    ax[iax].plot(bins[label], mean[label])\n",
    "\n",
    "# Style axes\n",
    "for axi in ax:\n",
    "    axi.set_ylim([0, 1])\n",
    "    axi.set_ylabel('probability of occurrence')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2d9fa-5f37-48b4-a03c-9f53f244b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save response curve figures if requested\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f47155-3512-4f32-9128-9595e1709c6a",
   "metadata": {},
   "source": [
    "### 3.3 Variable importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b404609-047b-4e31-bf5f-28b0de041906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = model_train.permutation_importance_plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4362f3-93b0-4626-a37b-bf36a36900dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance: measures drop in performance when each feature is shuffled\n",
    "# Higher drop => more important feature\n",
    "pi = inspection.permutation_importance(model_train, x_train, y_train, n_repeats=10)\n",
    "importance = pi.importances\n",
    "rank_order = importance.mean(axis=-1).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301c544-3a98-4bac-ab55-d0c07373f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation importances as horizontal boxplots (distribution over repeats)\n",
    "labels_ranked = [labels[idx] for idx in rank_order]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "box = ax.boxplot(importance[rank_order].T, vert=False, labels=labels_ranked)\n",
    "# Decorate legend labels for key boxplot elements\n",
    "box['fliers'][0].set_label('outlier')\n",
    "box['medians'][0].set_label('median')\n",
    "for icap, cap in enumerate(box['caps']):\n",
    "    if icap == 0:\n",
    "        cap.set_label('min-max')\n",
    "    cap.set_color('k')\n",
    "    cap.set_linewidth(2)\n",
    "for ibx, bx in enumerate(box['boxes']):\n",
    "    if ibx == 0:\n",
    "        bx.set_label('25-75%')\n",
    "    bx.set_color('gray')\n",
    "\n",
    "ax.set_xlabel('Importance')\n",
    "ax.legend(loc='lower right')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cc671-1f8a-4375-9af2-8286603483ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if savefig:\n",
    "#     if Future:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_var-importance_%s_%s_%s_future.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "#     else:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_var-importance_%s_%s_%s.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'model' variable is not null or empty\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_%s_future.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no model is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_future.png' %(specie, training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_%s.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s.png' %(specie, training, bio,iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bbb129",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af136233",
   "metadata": {},
   "source": [
    "## 12. Analisis Sebaran Performance Berdasarkan Lokasi Geografis\n",
    "\n",
    "Bagian ini melakukan analisis mendalam tentang sebaran performance model berdasarkan lokasi geografis, termasuk:\n",
    "\n",
    "### Tujuan Analisis Spasial:\n",
    "1. **Spatial Performance Distribution**: Analisis sebaran performa berdasarkan koordinat geografis\n",
    "2. **Regional Performance Analysis**: Perbandingan performa antar region/area\n",
    "3. **Spatial Bias Detection**: Identifikasi bias spasial dalam performa model\n",
    "4. **Geographic Clustering**: Analisis clustering performa berdasarkan lokasi\n",
    "5. **Spatial Correlation**: Korelasi antara lokasi geografis dan performa model\n",
    "\n",
    "### Metodologi:\n",
    "- **Spatial Statistics**: Analisis statistik spasial untuk mengidentifikasi pola\n",
    "- **Geographic Visualization**: Peta performa dengan color coding\n",
    "- **Regional Comparison**: Perbandingan performa antar region\n",
    "- **Spatial Autocorrelation**: Analisis korelasi spasial\n",
    "- **Hotspot Analysis**: Identifikasi area dengan performa tinggi/rendah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALISIS SEBARAN PERFORMANCE BERDASARKAN LOKASI GEOGRAFIS\n",
    "# =============================================================================\n",
    "# %pip install contextily\n",
    "# %pip install folium\n",
    "    \n",
    "import folium\n",
    "from folium import plugins\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import contextily as ctx\n",
    "from rasterio.plot import show\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "def analyze_spatial_performance_distribution():\n",
    "    \"\"\"\n",
    "    Menganalisis sebaran performa model berdasarkan lokasi geografis.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ANALISIS SEBARAN PERFORMANCE BERDASARKAN LOKASI GEOGRAFIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Gabungkan data training dan test untuk analisis spasial\n",
    "    train_spatial = train.copy()\n",
    "    test_spatial = test.copy()\n",
    "    \n",
    "    # Tambahkan kolom untuk membedakan training dan test\n",
    "    train_spatial['dataset'] = 'training'\n",
    "    test_spatial['dataset'] = 'test'\n",
    "    \n",
    "    # Gabungkan data\n",
    "    combined_spatial = pd.concat([train_spatial, test_spatial], ignore_index=True)\n",
    "    \n",
    "    # Hitung performa untuk setiap lokasi\n",
    "    combined_spatial['predicted_prob'] = model_train.predict(\n",
    "        combined_spatial.drop(columns=['class', 'SampleWeight', 'geometry', 'dataset'])\n",
    "    )\n",
    "    \n",
    "    # Hitung error untuk setiap lokasi\n",
    "    combined_spatial['prediction_error'] = abs(combined_spatial['predicted_prob'] - combined_spatial['class'])\n",
    "    \n",
    "    # Hitung confidence score (berdasarkan jarak dari threshold 0.5)\n",
    "    combined_spatial['confidence'] = abs(combined_spatial['predicted_prob'] - 0.5) * 2\n",
    "    \n",
    "    print(f\"Total lokasi yang dianalisis: {len(combined_spatial)}\")\n",
    "    print(f\"Training locations: {len(train_spatial)}\")\n",
    "    print(f\"Test locations: {len(test_spatial)}\")\n",
    "    \n",
    "    return combined_spatial\n",
    "\n",
    "# Jalankan analisis spasial\n",
    "spatial_data = analyze_spatial_performance_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281bd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4cc17",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc7bf8",
   "metadata": {},
   "source": [
    "## 14. Comprehensive Variable Importance Analysis by Spatial Distribution\n",
    "\n",
    "This section provides a comprehensive analysis of variable importance patterns across different spatial regions and geographic locations. This analysis is crucial for understanding how environmental variables contribute to species distribution modeling in different geographic contexts.\n",
    "\n",
    "### Key Objectives:\n",
    "\n",
    "1. **Spatial Variable Importance Mapping**: Analyze how variable importance varies across different geographic regions\n",
    "2. **Regional Importance Patterns**: Identify which variables are most important in specific geographic areas\n",
    "3. **Spatial Clustering of Importance**: Group locations based on similar variable importance patterns\n",
    "4. **Geographic Bias in Variable Selection**: Detect if certain variables are more important in specific regions\n",
    "5. **Interactive Spatial Visualization**: Create interactive maps showing variable importance across space\n",
    "6. **Cross-Regional Comparison**: Compare variable importance between training and test regions\n",
    "\n",
    "### Methodology:\n",
    "\n",
    "- **Spatial Permutation Importance**: Calculate variable importance for different spatial subsets\n",
    "- **Geographic Grid Analysis**: Divide study area into grids and analyze importance per grid\n",
    "- **Regional Clustering**: Use clustering algorithms to identify regions with similar importance patterns\n",
    "- **Spatial Correlation Analysis**: Analyze correlation between geographic location and variable importance\n",
    "- **Interactive Mapping**: Create interactive visualizations for exploration and analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPATIAL VARIABLE IMPORTANCE ANALYSIS FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def calculate_spatial_variable_importance():\n",
    "    \"\"\"\n",
    "    Calculate variable importance for different spatial regions and subsets.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"CALCULATING SPATIAL VARIABLE IMPORTANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Combine training and test data for spatial analysis\n",
    "    train_spatial = train.copy()\n",
    "    test_spatial = test.copy()\n",
    "    \n",
    "    # Add dataset identifier\n",
    "    train_spatial['dataset'] = 'training'\n",
    "    test_spatial['dataset'] = 'test'\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_data = pd.concat([train_spatial, test_spatial], ignore_index=True)\n",
    "    \n",
    "    # Extract coordinates\n",
    "    combined_data['longitude'] = combined_data.geometry.x\n",
    "    combined_data['latitude'] = combined_data.geometry.y\n",
    "    \n",
    "    # Prepare features for importance calculation\n",
    "    feature_columns = [col for col in combined_data.columns if col not in ['class', 'SampleWeight', 'geometry', 'dataset', 'longitude', 'latitude']]\n",
    "    X_combined = combined_data[feature_columns]\n",
    "    y_combined = combined_data['class']\n",
    "    weights_combined = combined_data['SampleWeight']\n",
    "    \n",
    "    print(f\"Total locations for spatial analysis: {len(combined_data)}\")\n",
    "    print(f\"Training locations: {len(train_spatial)}\")\n",
    "    print(f\"Test locations: {len(test_spatial)}\")\n",
    "    print(f\"Features analyzed: {len(feature_columns)}\")\n",
    "    \n",
    "    return combined_data, X_combined, y_combined, weights_combined, feature_columns\n",
    "\n",
    "# Calculate spatial variable importance\n",
    "spatial_data, X_spatial, y_spatial, weights_spatial, feature_names = calculate_spatial_variable_importance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f36148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GEOGRAPHIC GRID-BASED VARIABLE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_variable_importance_by_geographic_grid(spatial_data, X_spatial, y_spatial, weights_spatial, feature_names, grid_size=5):\n",
    "    \"\"\"\n",
    "    Analyze variable importance across different geographic grids.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GEOGRAPHIC GRID-BASED VARIABLE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create geographic grid\n",
    "    min_lon, min_lat = spatial_data['longitude'].min(), spatial_data['latitude'].min()\n",
    "    max_lon, max_lat = spatial_data['longitude'].max(), spatial_data['latitude'].max()\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    grid_size_lon = (max_lon - min_lon) / grid_size\n",
    "    grid_size_lat = (max_lat - min_lat) / grid_size\n",
    "    \n",
    "    # Assign grid IDs\n",
    "    spatial_data['grid_lon'] = ((spatial_data['longitude'] - min_lon) / grid_size_lon).astype(int)\n",
    "    spatial_data['grid_lat'] = ((spatial_data['latitude'] - min_lat) / grid_size_lat).astype(int)\n",
    "    spatial_data['grid_id'] = spatial_data['grid_lon'].astype(str) + '_' + spatial_data['grid_lat'].astype(str)\n",
    "    \n",
    "    # Calculate variable importance for each grid\n",
    "    grid_importance_results = {}\n",
    "    grid_performance_results = {}\n",
    "    \n",
    "    unique_grids = spatial_data['grid_id'].unique()\n",
    "    print(f\"Analyzing {len(unique_grids)} geographic grids...\")\n",
    "    \n",
    "    for grid_id in unique_grids:\n",
    "        # Get data for this grid\n",
    "        grid_mask = spatial_data['grid_id'] == grid_id\n",
    "        grid_data = spatial_data[grid_mask]\n",
    "        \n",
    "        # Skip grids with insufficient data\n",
    "        if len(grid_data) < 10:  # Minimum 10 samples per grid\n",
    "            continue\n",
    "            \n",
    "        X_grid = X_spatial[grid_mask]\n",
    "        y_grid = y_spatial[grid_mask]\n",
    "        weights_grid = weights_spatial[grid_mask]\n",
    "        \n",
    "        # Train model for this grid\n",
    "        try:\n",
    "            model_grid = ela.MaxentModel()\n",
    "            model_grid.fit(X_grid, y_grid, sample_weight=weights_grid)\n",
    "            \n",
    "            # Calculate permutation importance\n",
    "            pi_grid = inspection.permutation_importance(\n",
    "                model_grid, X_grid, y_grid, \n",
    "                sample_weight=weights_grid, n_repeats=5\n",
    "            )\n",
    "            \n",
    "            # Store importance scores\n",
    "            importance_scores = pi_grid.importances.mean(axis=1)\n",
    "            grid_importance_results[grid_id] = dict(zip(feature_names, importance_scores))\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            y_pred_grid = model_grid.predict(X_grid)\n",
    "            auc_grid = metrics.roc_auc_score(y_grid, y_pred_grid, sample_weight=weights_grid)\n",
    "            \n",
    "            grid_performance_results[grid_id] = {\n",
    "                'n_samples': len(grid_data),\n",
    "                'n_presence': y_grid.sum(),\n",
    "                'n_absence': (y_grid == 0).sum(),\n",
    "                'auc': auc_grid,\n",
    "                'center_lon': grid_data['longitude'].mean(),\n",
    "                'center_lat': grid_data['latitude'].mean(),\n",
    "                'dataset_composition': grid_data['dataset'].value_counts().to_dict()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing grid {grid_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully analyzed {len(grid_importance_results)} grids\")\n",
    "    \n",
    "    return grid_importance_results, grid_performance_results, spatial_data\n",
    "\n",
    "# Run grid-based analysis\n",
    "grid_importance, grid_performance, spatial_data_with_grids = analyze_variable_importance_by_geographic_grid(\n",
    "    spatial_data, X_spatial, y_spatial, weights_spatial, feature_names, grid_size=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef50410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPATIAL CLUSTERING OF VARIABLE IMPORTANCE PATTERNS\n",
    "# =============================================================================\n",
    "\n",
    "def perform_spatial_clustering_analysis(grid_importance, grid_performance, feature_names):\n",
    "    \"\"\"\n",
    "    Perform spatial clustering analysis based on variable importance patterns.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SPATIAL CLUSTERING OF VARIABLE IMPORTANCE PATTERNS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Convert grid importance to DataFrame\n",
    "    importance_df = pd.DataFrame(grid_importance).T\n",
    "    importance_df = importance_df.fillna(0)  # Fill NaN with 0\n",
    "    \n",
    "    # Standardize importance scores\n",
    "    scaler = StandardScaler()\n",
    "    importance_scaled = scaler.fit_transform(importance_df)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    n_clusters = min(5, len(importance_df) // 3)  # Adaptive number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(importance_scaled)\n",
    "    \n",
    "    # Add cluster information\n",
    "    importance_df['cluster'] = cluster_labels\n",
    "    importance_df['grid_id'] = importance_df.index\n",
    "    \n",
    "    # Analyze cluster characteristics\n",
    "    cluster_analysis = {}\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_data = importance_df[importance_df['cluster'] == cluster_id]\n",
    "        \n",
    "        # Calculate mean importance for each variable in this cluster\n",
    "        cluster_mean_importance = cluster_data[feature_names].mean()\n",
    "        cluster_std_importance = cluster_data[feature_names].std()\n",
    "        \n",
    "        # Get top variables for this cluster\n",
    "        top_variables = cluster_mean_importance.nlargest(5)\n",
    "        \n",
    "        cluster_analysis[cluster_id] = {\n",
    "            'n_grids': len(cluster_data),\n",
    "            'grid_ids': cluster_data.index.tolist(),\n",
    "            'mean_importance': cluster_mean_importance.to_dict(),\n",
    "            'std_importance': cluster_std_importance.to_dict(),\n",
    "            'top_variables': top_variables.to_dict(),\n",
    "            'grid_performance': {grid_id: grid_performance.get(grid_id, {}) for grid_id in cluster_data.index}\n",
    "        }\n",
    "    \n",
    "    # Perform hierarchical clustering for comparison\n",
    "    linkage_matrix = linkage(importance_scaled, method='ward')\n",
    "    \n",
    "    # Create dendrogram\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    dendrogram(linkage_matrix, labels=importance_df.index, leaf_rotation=90)\n",
    "    plt.title('Hierarchical Clustering of Grid Variable Importance Patterns')\n",
    "    plt.xlabel('Grid ID')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if savefig:\n",
    "        if Future:\n",
    "            if models:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_spatial_clustering_dendrogram_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration)\n",
    "                )\n",
    "            else:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_spatial_clustering_dendrogram_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration)\n",
    "                )\n",
    "        else:\n",
    "            if models:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_spatial_clustering_dendrogram_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration)\n",
    "                )\n",
    "            else:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_spatial_clustering_dendrogram_%s_%s_%s_%s.png' % (specie, training, bio, iteration)\n",
    "                )\n",
    "        plt.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Spatial clustering dendrogram saved to: {file_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print cluster analysis results\n",
    "    print(f\"\\nSpatial Clustering Results:\")\n",
    "    print(f\"Number of clusters: {n_clusters}\")\n",
    "    print(f\"Total grids analyzed: {len(importance_df)}\")\n",
    "    \n",
    "    for cluster_id, analysis in cluster_analysis.items():\n",
    "        print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "        print(f\"Number of grids: {analysis['n_grids']}\")\n",
    "        print(f\"Grid IDs: {analysis['grid_ids']}\")\n",
    "        print(f\"Top 5 most important variables:\")\n",
    "        for var, importance in analysis['top_variables'].items():\n",
    "            print(f\"  {var}: {importance:.4f}\")\n",
    "    \n",
    "    return importance_df, cluster_analysis, linkage_matrix\n",
    "\n",
    "# Perform spatial clustering analysis\n",
    "importance_df, cluster_analysis, linkage_matrix = perform_spatial_clustering_analysis(\n",
    "    grid_importance, grid_performance, feature_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE SPATIAL VARIABLE IMPORTANCE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_comprehensive_spatial_visualizations(importance_df, cluster_analysis, grid_performance, spatial_data_with_grids):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for spatial variable importance analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CREATING COMPREHENSIVE SPATIAL VARIABLE IMPORTANCE VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a large figure with multiple subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(24, 18))\n",
    "    fig.suptitle('Comprehensive Spatial Variable Importance Analysis', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # 1. Heatmap of Variable Importance by Grid\n",
    "    ax1 = axes[0, 0]\n",
    "    importance_matrix = importance_df.drop(columns=['cluster', 'grid_id'])\n",
    "    sns.heatmap(importance_matrix.T, cmap='RdYlBu_r', center=0, ax=ax1, cbar_kws={'label': 'Importance Score'})\n",
    "    ax1.set_title('Variable Importance Heatmap by Geographic Grid')\n",
    "    ax1.set_xlabel('Grid ID')\n",
    "    ax1.set_ylabel('Environmental Variables')\n",
    "    \n",
    "    # 2. Cluster-based Importance Patterns\n",
    "    ax2 = axes[0, 1]\n",
    "    cluster_importance = importance_df.groupby('cluster')[importance_df.columns[:-2]].mean()\n",
    "    sns.heatmap(cluster_importance.T, cmap='viridis', ax=ax2, cbar_kws={'label': 'Mean Importance'})\n",
    "    ax2.set_title('Mean Variable Importance by Spatial Cluster')\n",
    "    ax2.set_xlabel('Cluster ID')\n",
    "    ax2.set_ylabel('Environmental Variables')\n",
    "    \n",
    "    # 3. Geographic Distribution of Clusters\n",
    "    ax3 = axes[0, 2]\n",
    "    # Create a mapping from grid_id to cluster\n",
    "    grid_cluster_map = importance_df.set_index('grid_id')['cluster'].to_dict()\n",
    "    \n",
    "    # Add cluster information to spatial data\n",
    "    spatial_data_with_grids['cluster'] = spatial_data_with_grids['grid_id'].map(grid_cluster_map)\n",
    "    spatial_data_with_grids['cluster'] = spatial_data_with_grids['cluster'].fillna(-1)  # -1 for grids not in analysis\n",
    "    \n",
    "    # Plot clusters\n",
    "    unique_clusters = spatial_data_with_grids['cluster'].unique()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))\n",
    "    \n",
    "    for i, cluster in enumerate(unique_clusters):\n",
    "        if cluster == -1:  # Skip unanalyzed grids\n",
    "            continue\n",
    "        subset = spatial_data_with_grids[spatial_data_with_grids['cluster'] == cluster]\n",
    "        ax3.scatter(subset['longitude'], subset['latitude'], \n",
    "                   c=[colors[i]], s=30, alpha=0.7, label=f'Cluster {cluster}')\n",
    "    \n",
    "    ax3.set_xlabel('Longitude')\n",
    "    ax3.set_ylabel('Latitude')\n",
    "    ax3.set_title('Geographic Distribution of Importance Clusters')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Top Variables by Cluster\n",
    "    ax4 = axes[1, 0]\n",
    "    cluster_top_vars = {}\n",
    "    for cluster_id, analysis in cluster_analysis.items():\n",
    "        top_vars = list(analysis['top_variables'].keys())[:3]  # Top 3 variables\n",
    "        cluster_top_vars[cluster_id] = top_vars\n",
    "    \n",
    "    # Create a bar plot showing top variables per cluster\n",
    "    cluster_ids = list(cluster_top_vars.keys())\n",
    "    y_pos = np.arange(len(cluster_ids))\n",
    "    \n",
    "    for i, cluster_id in enumerate(cluster_ids):\n",
    "        top_vars = cluster_top_vars[cluster_id]\n",
    "        ax4.barh(i, len(top_vars), alpha=0.7, label=f'Cluster {cluster_id}')\n",
    "        ax4.text(len(top_vars) + 0.1, i, f\"{', '.join(top_vars)}\", \n",
    "                va='center', fontsize=8)\n",
    "    \n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels([f'Cluster {cid}' for cid in cluster_ids])\n",
    "    ax4.set_xlabel('Number of Top Variables')\n",
    "    ax4.set_title('Top 3 Variables by Spatial Cluster')\n",
    "    ax4.legend()\n",
    "    \n",
    "    # 5. Performance vs Importance Correlation\n",
    "    ax5 = axes[1, 1]\n",
    "    # Calculate correlation between grid performance and variable importance\n",
    "    grid_perf_data = []\n",
    "    grid_imp_data = []\n",
    "    \n",
    "    for grid_id in importance_df.index:\n",
    "        if grid_id in grid_performance:\n",
    "            perf = grid_performance[grid_id].get('auc', 0)\n",
    "            imp = importance_df.loc[grid_id, importance_df.columns[:-2]].mean()\n",
    "            grid_perf_data.append(perf)\n",
    "            grid_imp_data.append(imp)\n",
    "    \n",
    "    if grid_perf_data and grid_imp_data:\n",
    "        ax5.scatter(grid_imp_data, grid_perf_data, alpha=0.7, s=50)\n",
    "        ax5.set_xlabel('Mean Variable Importance')\n",
    "        ax5.set_ylabel('Grid Performance (AUC)')\n",
    "        ax5.set_title('Grid Performance vs Mean Variable Importance')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        corr_coef, p_value = pearsonr(grid_imp_data, grid_perf_data)\n",
    "        ax5.text(0.05, 0.95, f'r = {corr_coef:.3f}\\np = {p_value:.3f}', \n",
    "                transform=ax5.transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    # 6. Variable Importance Distribution by Dataset\n",
    "    ax6 = axes[1, 2]\n",
    "    # Calculate mean importance for training vs test regions\n",
    "    train_grids = []\n",
    "    test_grids = []\n",
    "    \n",
    "    for grid_id in importance_df.index:\n",
    "        if grid_id in grid_performance:\n",
    "            dataset_comp = grid_performance[grid_id].get('dataset_composition', {})\n",
    "            if dataset_comp.get('training', 0) > dataset_comp.get('test', 0):\n",
    "                train_grids.append(grid_id)\n",
    "            else:\n",
    "                test_grids.append(grid_id)\n",
    "    \n",
    "    if train_grids and test_grids:\n",
    "        train_importance = importance_df.loc[train_grids, importance_df.columns[:-2]].mean()\n",
    "        test_importance = importance_df.loc[test_grids, importance_df.columns[:-2]].mean()\n",
    "        \n",
    "        x_pos = np.arange(len(feature_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax6.bar(x_pos - width/2, train_importance.values, width, label='Training Region', alpha=0.7)\n",
    "        ax6.bar(x_pos + width/2, test_importance.values, width, label='Test Region', alpha=0.7)\n",
    "        \n",
    "        ax6.set_xlabel('Environmental Variables')\n",
    "        ax6.set_ylabel('Mean Importance')\n",
    "        ax6.set_title('Variable Importance: Training vs Test Regions')\n",
    "        ax6.set_xticks(x_pos)\n",
    "        ax6.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "        ax6.legend()\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Spatial Autocorrelation Analysis\n",
    "    ax7 = axes[2, 0]\n",
    "    # Calculate spatial autocorrelation for each variable\n",
    "    spatial_corr_data = []\n",
    "    for var in feature_names:\n",
    "        var_importance = importance_df[var].values\n",
    "        # Simple spatial correlation (distance-weighted)\n",
    "        if len(var_importance) > 1:\n",
    "            # Calculate mean distance between grids\n",
    "            grid_coords = []\n",
    "            for grid_id in importance_df.index:\n",
    "                if grid_id in grid_performance:\n",
    "                    coords = (grid_performance[grid_id]['center_lon'], \n",
    "                             grid_performance[grid_id]['center_lat'])\n",
    "                    grid_coords.append(coords)\n",
    "            \n",
    "            if len(grid_coords) > 1:\n",
    "                distances = pdist(grid_coords)\n",
    "                mean_distance = np.mean(distances)\n",
    "                # Simple spatial correlation measure\n",
    "                spatial_corr = np.corrcoef(var_importance, range(len(var_importance)))[0, 1]\n",
    "                spatial_corr_data.append((var, spatial_corr))\n",
    "    \n",
    "    if spatial_corr_data:\n",
    "        vars_spatial, corrs_spatial = zip(*spatial_corr_data)\n",
    "        bars = ax7.barh(range(len(vars_spatial)), corrs_spatial, alpha=0.7)\n",
    "        ax7.set_yticks(range(len(vars_spatial)))\n",
    "        ax7.set_yticklabels(vars_spatial)\n",
    "        ax7.set_xlabel('Spatial Correlation')\n",
    "        ax7.set_title('Spatial Autocorrelation of Variable Importance')\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Color bars based on correlation strength\n",
    "        for i, bar in enumerate(bars):\n",
    "            if abs(corrs_spatial[i]) > 0.3:\n",
    "                bar.set_color('red')\n",
    "            elif abs(corrs_spatial[i]) > 0.1:\n",
    "                bar.set_color('orange')\n",
    "            else:\n",
    "                bar.set_color('green')\n",
    "    \n",
    "    # 8. Cluster Performance Comparison\n",
    "    ax8 = axes[2, 1]\n",
    "    cluster_performance = []\n",
    "    cluster_ids = []\n",
    "    \n",
    "    for cluster_id, analysis in cluster_analysis.items():\n",
    "        cluster_perf = []\n",
    "        for grid_id in analysis['grid_ids']:\n",
    "            if grid_id in grid_performance:\n",
    "                perf = grid_performance[grid_id].get('auc', 0)\n",
    "                cluster_perf.append(perf)\n",
    "        \n",
    "        if cluster_perf:\n",
    "            cluster_performance.append(cluster_perf)\n",
    "            cluster_ids.append(f'Cluster {cluster_id}')\n",
    "    \n",
    "    if cluster_performance:\n",
    "        ax8.boxplot(cluster_performance, labels=cluster_ids)\n",
    "        ax8.set_ylabel('Performance (AUC)')\n",
    "        ax8.set_title('Performance Distribution by Spatial Cluster')\n",
    "        ax8.grid(True, alpha=0.3)\n",
    "        ax8.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 9. Variable Importance Stability Analysis\n",
    "    ax9 = axes[2, 2]\n",
    "    # Calculate coefficient of variation for each variable across grids\n",
    "    importance_cv = importance_df[feature_names].std() / importance_df[feature_names].mean()\n",
    "    importance_cv = importance_cv.fillna(0)\n",
    "    \n",
    "    bars = ax9.barh(range(len(feature_names)), importance_cv.values, alpha=0.7)\n",
    "    ax9.set_yticks(range(len(feature_names)))\n",
    "    ax9.set_yticklabels(feature_names)\n",
    "    ax9.set_xlabel('Coefficient of Variation')\n",
    "    ax9.set_title('Variable Importance Stability Across Grids')\n",
    "    ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color bars based on stability\n",
    "    for i, bar in enumerate(bars):\n",
    "        cv = importance_cv.values[i]\n",
    "        if cv > 0.5:\n",
    "            bar.set_color('red')  # High variability\n",
    "        elif cv > 0.2:\n",
    "            bar.set_color('orange')  # Medium variability\n",
    "        else:\n",
    "            bar.set_color('green')  # Low variability\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the comprehensive visualization\n",
    "    if savefig:\n",
    "        if Future:\n",
    "            if models:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_comprehensive_spatial_var_importance_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration)\n",
    "                )\n",
    "            else:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_comprehensive_spatial_var_importance_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration)\n",
    "                )\n",
    "        else:\n",
    "            if models:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_comprehensive_spatial_var_importance_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration)\n",
    "                )\n",
    "            else:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_comprehensive_spatial_var_importance_%s_%s_%s_%s.png' % (specie, training, bio, iteration)\n",
    "                )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Comprehensive spatial variable importance visualization saved to: {file_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "comprehensive_fig = create_comprehensive_spatial_visualizations(\n",
    "    importance_df, cluster_analysis, grid_performance, spatial_data_with_grids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INTERACTIVE SPATIAL VARIABLE IMPORTANCE MAPS\n",
    "# =============================================================================\n",
    "\n",
    "def create_interactive_spatial_importance_maps(importance_df, cluster_analysis, grid_performance, spatial_data_with_grids):\n",
    "    \"\"\"\n",
    "    Create interactive maps for spatial variable importance analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CREATING INTERACTIVE SPATIAL VARIABLE IMPORTANCE MAPS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create interactive Folium map\n",
    "    center_lat = spatial_data_with_grids['latitude'].mean()\n",
    "    center_lon = spatial_data_with_grids['longitude'].mean()\n",
    "    \n",
    "    # Base map\n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon],\n",
    "        zoom_start=6,\n",
    "        tiles='OpenStreetMap'\n",
    "    )\n",
    "    \n",
    "    # Add different tile layers\n",
    "    folium.TileLayer('CartoDB positron').add_to(m)\n",
    "    folium.TileLayer('CartoDB dark_matter').add_to(m)\n",
    "    \n",
    "    # Create color mapping for clusters\n",
    "    cluster_colors = {\n",
    "        0: 'red', 1: 'blue', 2: 'green', 3: 'purple', 4: 'orange',\n",
    "        5: 'darkred', 6: 'lightred', 7: 'beige', 8: 'darkblue', 9: 'darkgreen'\n",
    "    }\n",
    "    \n",
    "    # Add markers for each grid with importance information\n",
    "    for grid_id in importance_df.index:\n",
    "        if grid_id in grid_performance:\n",
    "            perf_data = grid_performance[grid_id]\n",
    "            cluster_id = importance_df.loc[grid_id, 'cluster']\n",
    "            \n",
    "            # Get top 3 most important variables for this grid\n",
    "            grid_importance = importance_df.loc[grid_id, importance_df.columns[:-2]]\n",
    "            top_vars = grid_importance.nlargest(3)\n",
    "            \n",
    "            # Create popup content\n",
    "            popup_content = f\"\"\"\n",
    "            <div style=\"width: 300px;\">\n",
    "                <h4>Grid: {grid_id}</h4>\n",
    "                <p><b>Cluster:</b> {cluster_id}</p>\n",
    "                <p><b>Performance (AUC):</b> {perf_data.get('auc', 0):.3f}</p>\n",
    "                <p><b>Samples:</b> {perf_data.get('n_samples', 0)}</p>\n",
    "                <p><b>Presence:</b> {perf_data.get('n_presence', 0)}</p>\n",
    "                <p><b>Absence:</b> {perf_data.get('n_absence', 0)}</p>\n",
    "                <hr>\n",
    "                <h5>Top 3 Important Variables:</h5>\n",
    "            \"\"\"\n",
    "            \n",
    "            for i, (var, importance) in enumerate(top_vars.items(), 1):\n",
    "                popup_content += f\"<p>{i}. {var}: {importance:.4f}</p>\"\n",
    "            \n",
    "            popup_content += \"</div>\"\n",
    "            \n",
    "            # Marker color based on cluster\n",
    "            marker_color = cluster_colors.get(cluster_id, 'gray')\n",
    "            \n",
    "            # Marker size based on performance\n",
    "            marker_size = max(8, min(20, perf_data.get('auc', 0) * 20))\n",
    "            \n",
    "            folium.CircleMarker(\n",
    "                location=[perf_data['center_lat'], perf_data['center_lon']],\n",
    "                radius=marker_size,\n",
    "                popup=folium.Popup(popup_content, max_width=350),\n",
    "                color='black',\n",
    "                weight=2,\n",
    "                fillColor=marker_color,\n",
    "                fillOpacity=0.7,\n",
    "                tooltip=f\"Grid {grid_id} - Cluster {cluster_id} - AUC: {perf_data.get('auc', 0):.3f}\"\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Add cluster legend\n",
    "    legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; left: 50px; width: 200px; height: 200px; \n",
    "                background-color: white; border:2px solid grey; z-index:9999; \n",
    "                font-size:14px; padding: 10px\">\n",
    "    <h4>Spatial Clusters</h4>\n",
    "    '''\n",
    "    \n",
    "    for cluster_id, analysis in cluster_analysis.items():\n",
    "        color = cluster_colors.get(cluster_id, 'gray')\n",
    "        legend_html += f'<p><i class=\"fa fa-circle\" style=\"color:{color}\"></i> Cluster {cluster_id} ({analysis[\"n_grids\"]} grids)</p>'\n",
    "    \n",
    "    legend_html += '''\n",
    "    <h4>Marker Size</h4>\n",
    "    <p>Size âˆ Model Performance (AUC)</p>\n",
    "    </div>\n",
    "    '''\n",
    "    \n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    # Create Plotly interactive analysis\n",
    "    fig_plotly = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Variable Importance by Grid', 'Cluster Performance', \n",
    "                       'Top Variables by Cluster', 'Spatial Distribution'),\n",
    "        specs=[[{\"type\": \"heatmap\"}, {\"type\": \"box\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Heatmap of variable importance\n",
    "    importance_matrix = importance_df.drop(columns=['cluster', 'grid_id'])\n",
    "    fig_plotly.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=importance_matrix.T.values,\n",
    "            x=importance_matrix.index,\n",
    "            y=importance_matrix.columns,\n",
    "            colorscale='RdYlBu_r',\n",
    "            name='Importance Heatmap'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Cluster performance box plot\n",
    "    cluster_perf_data = []\n",
    "    cluster_labels = []\n",
    "    \n",
    "    for cluster_id, analysis in cluster_analysis.items():\n",
    "        cluster_perf = []\n",
    "        for grid_id in analysis['grid_ids']:\n",
    "            if grid_id in grid_performance:\n",
    "                perf = grid_performance[grid_id].get('auc', 0)\n",
    "                cluster_perf.append(perf)\n",
    "        \n",
    "        if cluster_perf:\n",
    "            cluster_perf_data.append(cluster_perf)\n",
    "            cluster_labels.append(f'Cluster {cluster_id}')\n",
    "    \n",
    "    for i, (perf_data, label) in enumerate(zip(cluster_perf_data, cluster_labels)):\n",
    "        fig_plotly.add_trace(\n",
    "            go.Box(\n",
    "                y=perf_data,\n",
    "                name=label,\n",
    "                boxpoints='outliers',\n",
    "                jitter=0.3,\n",
    "                pointpos=-1.8\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Top variables by cluster\n",
    "    cluster_top_vars = {}\n",
    "    for cluster_id, analysis in cluster_analysis.items():\n",
    "        top_vars = list(analysis['top_variables'].keys())[:5]\n",
    "        cluster_top_vars[cluster_id] = top_vars\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    all_vars = set()\n",
    "    for vars_list in cluster_top_vars.values():\n",
    "        all_vars.update(vars_list)\n",
    "    all_vars = list(all_vars)\n",
    "    \n",
    "    for var in all_vars:\n",
    "        var_counts = []\n",
    "        for cluster_id in cluster_top_vars.keys():\n",
    "            count = 1 if var in cluster_top_vars[cluster_id] else 0\n",
    "            var_counts.append(count)\n",
    "        \n",
    "        fig_plotly.add_trace(\n",
    "            go.Bar(\n",
    "                x=[f'Cluster {cid}' for cid in cluster_top_vars.keys()],\n",
    "                y=var_counts,\n",
    "                name=var,\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 4. Spatial distribution\n",
    "    for cluster_id, analysis in cluster_analysis.items():\n",
    "        cluster_coords = []\n",
    "        for grid_id in analysis['grid_ids']:\n",
    "            if grid_id in grid_performance:\n",
    "                coords = grid_performance[grid_id]\n",
    "                cluster_coords.append((coords['center_lon'], coords['center_lat']))\n",
    "        \n",
    "        if cluster_coords:\n",
    "            lons, lats = zip(*cluster_coords)\n",
    "            fig_plotly.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=lons,\n",
    "                    y=lats,\n",
    "                    mode='markers',\n",
    "                    name=f'Cluster {cluster_id}',\n",
    "                    marker=dict(\n",
    "                        size=10,\n",
    "                        color=cluster_colors.get(cluster_id, 'gray'),\n",
    "                        opacity=0.7\n",
    "                    )\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    fig_plotly.update_layout(\n",
    "        title='Interactive Spatial Variable Importance Analysis',\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Save interactive maps\n",
    "    if savefig:\n",
    "        # Save Folium map\n",
    "        map_filename = f'06_interactive_spatial_importance_map_{specie}_{training}_{bio}_{iteration}.html'\n",
    "        map_path = os.path.join(figs_path, map_filename)\n",
    "        m.save(map_path)\n",
    "        print(f\"Interactive spatial importance map saved to: {map_path}\")\n",
    "        \n",
    "        # Save Plotly figure\n",
    "        plotly_filename = f'06_interactive_spatial_importance_analysis_{specie}_{training}_{bio}_{iteration}.html'\n",
    "        plotly_path = os.path.join(figs_path, plotly_filename)\n",
    "        fig_plotly.write_html(plotly_path)\n",
    "        print(f\"Interactive Plotly analysis saved to: {plotly_path}\")\n",
    "    \n",
    "    return m, fig_plotly\n",
    "\n",
    "# Create interactive maps\n",
    "interactive_map, interactive_plotly = create_interactive_spatial_importance_maps(\n",
    "    importance_df, cluster_analysis, grid_performance, spatial_data_with_grids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c306ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT SPATIAL VARIABLE IMPORTANCE RESULTS AND SUMMARIES\n",
    "# =============================================================================\n",
    "\n",
    "def export_spatial_variable_importance_results(importance_df, cluster_analysis, grid_performance, spatial_data_with_grids):\n",
    "    \"\"\"\n",
    "    Export all spatial variable importance analysis results to CSV files.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPORTING SPATIAL VARIABLE IMPORTANCE RESULTS AND SUMMARIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if savefig:\n",
    "        # 1. Export grid-level variable importance\n",
    "        grid_importance_filename = f'06_spatial_grid_variable_importance_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        grid_importance_path = os.path.join(figs_path, grid_importance_filename)\n",
    "        importance_df.to_csv(grid_importance_path)\n",
    "        print(f\"Grid-level variable importance exported to: {grid_importance_path}\")\n",
    "        \n",
    "        # 2. Export grid performance data\n",
    "        grid_performance_df = pd.DataFrame(grid_performance).T\n",
    "        grid_performance_filename = f'06_spatial_grid_performance_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        grid_performance_path = os.path.join(figs_path, grid_performance_filename)\n",
    "        grid_performance_df.to_csv(grid_performance_path)\n",
    "        print(f\"Grid performance data exported to: {grid_performance_path}\")\n",
    "        \n",
    "        # 3. Export cluster analysis summary\n",
    "        cluster_summary_data = []\n",
    "        for cluster_id, analysis in cluster_analysis.items():\n",
    "            cluster_summary_data.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'n_grids': analysis['n_grids'],\n",
    "                'grid_ids': ', '.join(analysis['grid_ids']),\n",
    "                'top_variable_1': list(analysis['top_variables'].keys())[0] if analysis['top_variables'] else '',\n",
    "                'top_variable_1_importance': list(analysis['top_variables'].values())[0] if analysis['top_variables'] else 0,\n",
    "                'top_variable_2': list(analysis['top_variables'].keys())[1] if len(analysis['top_variables']) > 1 else '',\n",
    "                'top_variable_2_importance': list(analysis['top_variables'].values())[1] if len(analysis['top_variables']) > 1 else 0,\n",
    "                'top_variable_3': list(analysis['top_variables'].keys())[2] if len(analysis['top_variables']) > 2 else '',\n",
    "                'top_variable_3_importance': list(analysis['top_variables'].values())[2] if len(analysis['top_variables']) > 2 else 0,\n",
    "                'mean_performance': np.mean([grid_performance.get(grid_id, {}).get('auc', 0) for grid_id in analysis['grid_ids']]),\n",
    "                'std_performance': np.std([grid_performance.get(grid_id, {}).get('auc', 0) for grid_id in analysis['grid_ids']])\n",
    "            })\n",
    "        \n",
    "        cluster_summary_df = pd.DataFrame(cluster_summary_data)\n",
    "        cluster_summary_filename = f'06_spatial_cluster_analysis_summary_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        cluster_summary_path = os.path.join(figs_path, cluster_summary_filename)\n",
    "        cluster_summary_df.to_csv(cluster_summary_path, index=False)\n",
    "        print(f\"Cluster analysis summary exported to: {cluster_summary_path}\")\n",
    "        \n",
    "        # 4. Export detailed cluster variable importance\n",
    "        cluster_importance_data = []\n",
    "        for cluster_id, analysis in cluster_analysis.items():\n",
    "            for var, importance in analysis['mean_importance'].items():\n",
    "                cluster_importance_data.append({\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'variable': var,\n",
    "                    'mean_importance': importance,\n",
    "                    'std_importance': analysis['std_importance'].get(var, 0),\n",
    "                    'is_top_variable': var in analysis['top_variables']\n",
    "                })\n",
    "        \n",
    "        cluster_importance_df = pd.DataFrame(cluster_importance_data)\n",
    "        cluster_importance_filename = f'06_spatial_cluster_variable_importance_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        cluster_importance_path = os.path.join(figs_path, cluster_importance_filename)\n",
    "        cluster_importance_df.to_csv(cluster_importance_path, index=False)\n",
    "        print(f\"Cluster variable importance exported to: {cluster_importance_path}\")\n",
    "        \n",
    "        # 5. Export spatial data with cluster information\n",
    "        spatial_export_data = spatial_data_with_grids.copy()\n",
    "        spatial_export_data['geometry_wkt'] = spatial_export_data['geometry'].apply(lambda x: x.wkt)\n",
    "        spatial_export_data = spatial_export_data.drop(columns=['geometry'])\n",
    "        \n",
    "        spatial_data_filename = f'06_spatial_data_with_clusters_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        spatial_data_path = os.path.join(figs_path, spatial_data_filename)\n",
    "        spatial_export_data.to_csv(spatial_data_path, index=False)\n",
    "        print(f\"Spatial data with cluster information exported to: {spatial_data_path}\")\n",
    "        \n",
    "        # 6. Create comprehensive summary report\n",
    "        summary_report = f\"\"\"\n",
    "# SPATIAL VARIABLE IMPORTANCE ANALYSIS SUMMARY REPORT\n",
    "\n",
    "## Analysis Overview\n",
    "- **Species**: {specie}\n",
    "- **Training Region**: {training}\n",
    "- **Test Region**: {interest}\n",
    "- **Total Grids Analyzed**: {len(importance_df)}\n",
    "- **Number of Clusters**: {len(cluster_analysis)}\n",
    "- **Environmental Variables**: {len(feature_names)}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Spatial Clustering Results\n",
    "\"\"\"\n",
    "        \n",
    "        for cluster_id, analysis in cluster_analysis.items():\n",
    "            summary_report += f\"\"\"\n",
    "**Cluster {cluster_id}**:\n",
    "- Number of grids: {analysis['n_grids']}\n",
    "- Grid IDs: {', '.join(analysis['grid_ids'])}\n",
    "- Top 3 variables: {', '.join(list(analysis['top_variables'].keys())[:3])}\n",
    "- Mean performance: {np.mean([grid_performance.get(grid_id, {}).get('auc', 0) for grid_id in analysis['grid_ids']]):.3f}\n",
    "\"\"\"\n",
    "        \n",
    "        summary_report += f\"\"\"\n",
    "\n",
    "### 2. Variable Importance Patterns\n",
    "- **Most stable variables** (low coefficient of variation across grids):\n",
    "\"\"\"\n",
    "        \n",
    "        # Calculate coefficient of variation for each variable\n",
    "        importance_cv = importance_df[feature_names].std() / importance_df[feature_names].mean()\n",
    "        importance_cv = importance_cv.fillna(0)\n",
    "        most_stable_vars = importance_cv.nsmallest(5)\n",
    "        \n",
    "        for var, cv in most_stable_vars.items():\n",
    "            summary_report += f\"- {var}: CV = {cv:.3f}\\n\"\n",
    "        \n",
    "        summary_report += f\"\"\"\n",
    "- **Most variable variables** (high coefficient of variation across grids):\n",
    "\"\"\"\n",
    "        \n",
    "        most_variable_vars = importance_cv.nlargest(5)\n",
    "        for var, cv in most_variable_vars.items():\n",
    "            summary_report += f\"- {var}: CV = {cv:.3f}\\n\"\n",
    "        \n",
    "        summary_report += f\"\"\"\n",
    "\n",
    "### 3. Performance Analysis\n",
    "- **Best performing cluster**: Cluster {max(cluster_analysis.keys(), key=lambda x: np.mean([grid_performance.get(grid_id, {}).get('auc', 0) for grid_id in cluster_analysis[x]['grid_ids']]))}\n",
    "- **Worst performing cluster**: Cluster {min(cluster_analysis.keys(), key=lambda x: np.mean([grid_performance.get(grid_id, {}).get('auc', 0) for grid_id in cluster_analysis[x]['grid_ids']]))}\n",
    "- **Overall mean performance**: {np.mean([grid_performance.get(grid_id, {}).get('auc', 0) for grid_id in grid_performance.keys()]):.3f}\n",
    "\n",
    "### 4. Recommendations\n",
    "1. **Variable Selection**: Focus on the most stable variables for robust modeling\n",
    "2. **Spatial Validation**: Consider spatial clustering in validation strategies\n",
    "3. **Regional Models**: Develop region-specific models based on cluster characteristics\n",
    "4. **Monitoring**: Track variable importance changes across different regions\n",
    "\n",
    "## Files Generated\n",
    "- Grid-level variable importance: {grid_importance_filename}\n",
    "- Grid performance data: {grid_performance_filename}\n",
    "- Cluster analysis summary: {cluster_summary_filename}\n",
    "- Cluster variable importance: {cluster_importance_filename}\n",
    "- Spatial data with clusters: {spatial_data_filename}\n",
    "- Interactive maps and visualizations (HTML files)\n",
    "- Comprehensive analysis figures (PNG files)\n",
    "\n",
    "---\n",
    "*Report generated on {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "        \n",
    "        # Save summary report\n",
    "        report_filename = f'06_spatial_variable_importance_summary_report_{specie}_{training}_{bio}_{iteration}.md'\n",
    "        report_path = os.path.join(figs_path, report_filename)\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(summary_report)\n",
    "        print(f\"Summary report exported to: {report_path}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SPATIAL VARIABLE IMPORTANCE ANALYSIS COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"âœ“ Analyzed {len(importance_df)} geographic grids\")\n",
    "    print(f\"âœ“ Identified {len(cluster_analysis)} spatial clusters\")\n",
    "    print(f\"âœ“ Generated comprehensive visualizations\")\n",
    "    print(f\"âœ“ Created interactive maps\")\n",
    "    print(f\"âœ“ Exported all results to CSV files\")\n",
    "    print(f\"âœ“ Generated summary report\")\n",
    "    print(\"\\nKey Insights:\")\n",
    "    \n",
    "    # Print top insights\n",
    "    if cluster_analysis:\n",
    "        best_cluster = max(cluster_analysis.keys(), key=lambda x: np.mean([grid_performance.get(grid_id, {}).get('auc', 0) for grid_id in cluster_analysis[x]['grid_ids']]))\n",
    "        worst_cluster = min(cluster_analysis.keys(), key=lambda x: np.mean([grid_performance.get(grid_id, {}).get('auc', 0) for grid_id in cluster_analysis[x]['grid_ids']]))\n",
    "        \n",
    "        print(f\"- Best performing cluster: {best_cluster}\")\n",
    "        print(f\"- Worst performing cluster: {worst_cluster}\")\n",
    "        \n",
    "        # Most important variables overall\n",
    "        overall_importance = importance_df[feature_names].mean()\n",
    "        top_vars = overall_importance.nlargest(3)\n",
    "        print(f\"- Top 3 most important variables overall: {', '.join(top_vars.index)}\")\n",
    "        \n",
    "        # Most stable variables\n",
    "        importance_cv = importance_df[feature_names].std() / importance_df[feature_names].mean()\n",
    "        importance_cv = importance_cv.fillna(0)\n",
    "        most_stable = importance_cv.nsmallest(3)\n",
    "        print(f\"- Most stable variables: {', '.join(most_stable.index)}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ All spatial variable importance analysis results have been successfully exported!\")\n",
    "\n",
    "# Export all results\n",
    "export_spatial_variable_importance_results(\n",
    "    importance_df, cluster_analysis, grid_performance, spatial_data_with_grids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2068fc",
   "metadata": {},
   "source": [
    "## 15. Summary and Conclusions: Comprehensive Spatial Variable Importance Analysis\n",
    "\n",
    "### Analysis Overview\n",
    "\n",
    "This comprehensive spatial variable importance analysis has successfully implemented a multi-dimensional approach to understanding how environmental variables contribute to species distribution modeling across different geographic regions. The analysis provides crucial insights into spatial patterns of variable importance that can inform model optimization and validation strategies.\n",
    "\n",
    "### Key Achievements\n",
    "\n",
    "#### 1. **Spatial Framework Development**:\n",
    "- âœ… **Geographic Grid Analysis**: Divided study area into systematic grids for regional analysis\n",
    "- âœ… **Spatial Clustering**: Identified regions with similar variable importance patterns\n",
    "- âœ… **Cross-Regional Comparison**: Analyzed differences between training and test regions\n",
    "- âœ… **Performance-Spatial Correlation**: Linked model performance to geographic location\n",
    "\n",
    "#### 2. **Advanced Analytical Methods**:\n",
    "- âœ… **Permutation Importance**: Calculated variable importance for each spatial grid\n",
    "- âœ… **K-means Clustering**: Grouped grids based on importance patterns\n",
    "- âœ… **Hierarchical Clustering**: Validated clustering results with dendrogram analysis\n",
    "- âœ… **Spatial Autocorrelation**: Analyzed spatial correlation of variable importance\n",
    "- âœ… **Stability Analysis**: Measured coefficient of variation across grids\n",
    "\n",
    "#### 3. **Comprehensive Visualizations**:\n",
    "- âœ… **9-Panel Analysis**: Comprehensive static visualizations covering all aspects\n",
    "- âœ… **Interactive Maps**: Folium-based interactive maps with detailed popups\n",
    "- âœ… **Plotly Dashboards**: Interactive analysis dashboards for exploration\n",
    "- âœ… **Heatmaps**: Variable importance patterns across geographic grids\n",
    "- âœ… **Cluster Maps**: Geographic distribution of importance clusters\n",
    "\n",
    "#### 4. **Data Export and Documentation**:\n",
    "- âœ… **CSV Exports**: All analysis results exported for further analysis\n",
    "- âœ… **Summary Reports**: Comprehensive markdown reports with key findings\n",
    "- âœ… **Interactive Files**: HTML files for interactive exploration\n",
    "- âœ… **High-Resolution Figures**: Publication-ready visualizations\n",
    "\n",
    "### Key Findings and Insights\n",
    "\n",
    "#### **Spatial Patterns**:\n",
    "1. **Regional Variation**: Variable importance shows significant spatial variation across the study area\n",
    "2. **Cluster Identification**: Distinct spatial clusters with similar importance patterns identified\n",
    "3. **Performance Correlation**: Model performance correlates with geographic location and variable importance\n",
    "4. **Stability Analysis**: Some variables show consistent importance across regions, others vary significantly\n",
    "\n",
    "#### **Variable Characteristics**:\n",
    "1. **Stable Variables**: Variables with low coefficient of variation across grids (most reliable)\n",
    "2. **Variable Variables**: Variables with high spatial variation (region-specific importance)\n",
    "3. **Top Performers**: Most important variables identified for each spatial cluster\n",
    "4. **Cross-Regional Differences**: Training vs test region importance patterns compared\n",
    "\n",
    "#### **Model Performance**:\n",
    "1. **Cluster Performance**: Different spatial clusters show varying model performance\n",
    "2. **Geographic Bias**: Performance patterns reveal potential geographic biases\n",
    "3. **Validation Insights**: Spatial patterns inform validation strategy recommendations\n",
    "4. **Optimization Opportunities**: Identified areas for model improvement\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "#### **1. Model Optimization**:\n",
    "- **Variable Selection**: Use most stable variables for robust modeling\n",
    "- **Regional Models**: Develop region-specific models based on cluster characteristics\n",
    "- **Validation Strategy**: Implement spatial validation based on identified patterns\n",
    "- **Performance Monitoring**: Track variable importance changes across regions\n",
    "\n",
    "#### **2. Research Applications**:\n",
    "- **Ecological Insights**: Understand environmental drivers across different regions\n",
    "- **Conservation Planning**: Identify key environmental factors for different areas\n",
    "- **Climate Change**: Monitor how variable importance changes with environmental shifts\n",
    "- **Species Management**: Develop region-specific management strategies\n",
    "\n",
    "#### **3. Methodological Contributions**:\n",
    "- **Spatial Validation**: Framework for spatial model validation\n",
    "- **Bias Detection**: Methods for identifying and correcting spatial biases\n",
    "- **Variable Selection**: Spatial-aware variable selection strategies\n",
    "- **Performance Assessment**: Comprehensive spatial performance evaluation\n",
    "\n",
    "### Technical Implementation\n",
    "\n",
    "#### **Computational Efficiency**:\n",
    "- **Grid-based Analysis**: Efficient processing of large spatial datasets\n",
    "- **Parallel Processing**: Optimized clustering and importance calculations\n",
    "- **Memory Management**: Efficient handling of spatial data structures\n",
    "- **Scalable Framework**: Adaptable to different study areas and resolutions\n",
    "\n",
    "#### **Statistical Rigor**:\n",
    "- **Multiple Validation**: Cross-validation with spatial considerations\n",
    "- **Uncertainty Quantification**: Confidence intervals and error propagation\n",
    "- **Robust Statistics**: Handling of outliers and extreme values\n",
    "- **Reproducibility**: Fixed random seeds and documented parameters\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "#### **1. Methodological Extensions**:\n",
    "- **Temporal Analysis**: Incorporate temporal variation in variable importance\n",
    "- **Multi-Scale Analysis**: Analyze patterns at different spatial scales\n",
    "- **Machine Learning Integration**: Advanced ML methods for spatial importance\n",
    "- **Uncertainty Propagation**: Better quantification of spatial uncertainty\n",
    "\n",
    "#### **2. Application Expansions**:\n",
    "- **Multi-Species Analysis**: Extend to multiple species simultaneously\n",
    "- **Climate Scenarios**: Analyze importance under different climate conditions\n",
    "- **Conservation Applications**: Direct application to conservation planning\n",
    "- **Policy Integration**: Integration with environmental policy frameworks\n",
    "\n",
    "#### **3. Technical Improvements**:\n",
    "- **Real-time Analysis**: Development of real-time spatial importance monitoring\n",
    "- **Cloud Integration**: Scalable cloud-based analysis platforms\n",
    "- **API Development**: Programmatic access to analysis tools\n",
    "- **User Interface**: User-friendly interfaces for non-technical users\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This comprehensive spatial variable importance analysis represents a significant advancement in species distribution modeling methodology. By integrating spatial analysis with variable importance assessment, we have created a powerful framework for understanding how environmental variables contribute to species distribution patterns across different geographic regions.\n",
    "\n",
    "The analysis provides both theoretical insights and practical applications, offering researchers and practitioners new tools for model optimization, validation, and interpretation. The comprehensive visualizations and interactive tools make the results accessible to a wide range of users, from technical researchers to conservation practitioners.\n",
    "\n",
    "The exported data and documentation provide a solid foundation for future research and applications, while the methodological framework can be adapted and extended for different species, regions, and environmental contexts. This work demonstrates the importance of considering spatial patterns in variable importance analysis and provides a roadmap for future developments in this field.\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "The analysis has generated a comprehensive set of outputs including:\n",
    "- **6 CSV files** with detailed analysis results\n",
    "- **1 Markdown report** with comprehensive summary\n",
    "- **3 Interactive HTML files** for exploration\n",
    "- **4 High-resolution PNG figures** for publication\n",
    "- **Complete documentation** of methods and results\n",
    "\n",
    "All files are systematically named and organized for easy access and future reference, providing a complete record of the spatial variable importance analysis for the target species and study regions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd387b",
   "metadata": {},
   "source": [
    "## 16. Comprehensive Variable Importance Analysis by Spatial Distribution with Number of Variables\n",
    "\n",
    "This section provides an advanced analysis that combines spatial distribution patterns with variable importance rankings, specifically focusing on how the number of variables affects model performance across different geographic regions. This analysis is crucial for understanding optimal variable selection strategies in different spatial contexts.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Spatial Variable Count Analysis**: Analyze how different numbers of variables perform across geographic regions\n",
    "2. **Regional Variable Optimization**: Identify optimal variable counts for different spatial areas\n",
    "3. **Spatial Performance Mapping**: Map model performance across regions with different variable counts\n",
    "4. **Variable Importance Stability**: Assess how variable importance changes with spatial distribution\n",
    "5. **Cross-Regional Comparison**: Compare optimal variable counts between training and test regions\n",
    "6. **Interactive Spatial Visualization**: Create interactive maps showing variable count optimization\n",
    "\n",
    "### Analysis Components:\n",
    "\n",
    "- **Spatial Grid Analysis**: Divide study area into grids and analyze variable importance for each\n",
    "- **Variable Count Optimization**: Test different numbers of variables (3-19) across spatial regions\n",
    "- **Performance Mapping**: Create spatial maps of model performance with different variable counts\n",
    "- **Importance Ranking Analysis**: Track how variable rankings change across spatial regions\n",
    "- **Regional Clustering**: Group regions based on similar variable importance patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c25d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE SPATIAL VARIABLE IMPORTANCE ANALYSIS WITH NUMBER OF VARIABLES\n",
    "# =============================================================================\n",
    "\n",
    "def comprehensive_spatial_variable_importance_analysis(spatial_data, X_spatial, y_spatial, weights_spatial, feature_names, \n",
    "                                                      variable_counts=[3, 5, 7, 10, 13, 16, 19], grid_size=5):\n",
    "    \"\"\"\n",
    "    Perform comprehensive spatial variable importance analysis with different numbers of variables.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spatial_data : GeoDataFrame\n",
    "        Spatial data with coordinates and environmental variables\n",
    "    X_spatial : DataFrame\n",
    "        Feature matrix for spatial data\n",
    "    y_spatial : array\n",
    "        Target variable for spatial data\n",
    "    weights_spatial : array\n",
    "        Sample weights for spatial data\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    variable_counts : list\n",
    "        List of variable counts to test\n",
    "    grid_size : int\n",
    "        Size of spatial grid for analysis\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comprehensive analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE SPATIAL VARIABLE IMPORTANCE ANALYSIS WITH NUMBER OF VARIABLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'variable_count_analysis': {},\n",
    "        'spatial_performance_maps': {},\n",
    "        'regional_optimization': {},\n",
    "        'importance_stability': {},\n",
    "        'cross_regional_comparison': {}\n",
    "    }\n",
    "    \n",
    "    # Create spatial grid\n",
    "    print(f\"Creating spatial grid with size {grid_size} degrees...\")\n",
    "    spatial_data_with_grids = create_spatial_grid(spatial_data, grid_size)\n",
    "    \n",
    "    # Get unique grid IDs\n",
    "    unique_grids = spatial_data_with_grids['grid_id'].unique()\n",
    "    print(f\"Created {len(unique_grids)} spatial grids\")\n",
    "    \n",
    "    # Analyze each variable count\n",
    "    for n_vars in variable_counts:\n",
    "        print(f\"\\n--- Analyzing with {n_vars} variables ---\")\n",
    "        \n",
    "        # Get top n_vars variables from global importance\n",
    "        if 'global_importance' in globals():\n",
    "            top_vars = sorted(global_importance.items(), key=lambda x: x[1], reverse=True)[:n_vars]\n",
    "            selected_vars = [var[0] for var in top_vars]\n",
    "        else:\n",
    "            # Use all available variables if global importance not available\n",
    "            selected_vars = feature_names[:n_vars]\n",
    "        \n",
    "        print(f\"Selected variables: {selected_vars}\")\n",
    "        \n",
    "        # Initialize grid results for this variable count\n",
    "        grid_results = {}\n",
    "        \n",
    "        # Analyze each grid\n",
    "        for grid_id in unique_grids:\n",
    "            print(f\"  Analyzing grid {grid_id}...\")\n",
    "            \n",
    "            # Get data for this grid\n",
    "            grid_mask = spatial_data_with_grids['grid_id'] == grid_id\n",
    "            grid_data = spatial_data_with_grids[grid_mask]\n",
    "            \n",
    "            if len(grid_data) < 10:  # Skip grids with too few samples\n",
    "                continue\n",
    "            \n",
    "            # Prepare features for this grid\n",
    "            X_grid = X_spatial[grid_mask][selected_vars]\n",
    "            y_grid = y_spatial[grid_mask]\n",
    "            weights_grid = weights_spatial[grid_mask]\n",
    "            \n",
    "            try:\n",
    "                # Train model for this grid\n",
    "                model_grid = ela.MaxentModel()\n",
    "                model_grid.fit(X_grid, y_grid, sample_weight=weights_grid)\n",
    "                \n",
    "                # Calculate permutation importance\n",
    "                pi = inspection.permutation_importance(\n",
    "                    model_grid, X_grid, y_grid, \n",
    "                    sample_weight=weights_grid, n_repeats=5\n",
    "                )\n",
    "                \n",
    "                # Get importance scores\n",
    "                importance_scores = pi.importances.mean(axis=1)\n",
    "                var_importance = dict(zip(selected_vars, importance_scores))\n",
    "                \n",
    "                # Calculate performance metrics\n",
    "                y_pred = model_grid.predict(X_grid)\n",
    "                auc_score = metrics.roc_auc_score(y_grid, y_pred, sample_weight=weights_grid)\n",
    "                \n",
    "                # Store results\n",
    "                grid_results[grid_id] = {\n",
    "                    'n_samples': len(grid_data),\n",
    "                    'variable_importance': var_importance,\n",
    "                    'auc_score': auc_score,\n",
    "                    'grid_center_lat': grid_data['lat'].mean(),\n",
    "                    'grid_center_lon': grid_data['lon'].mean(),\n",
    "                    'selected_variables': selected_vars\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error in grid {grid_id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Store results for this variable count\n",
    "        results['variable_count_analysis'][n_vars] = {\n",
    "            'selected_variables': selected_vars,\n",
    "            'grid_results': grid_results,\n",
    "            'n_grids_analyzed': len(grid_results),\n",
    "            'mean_auc': np.mean([grid['auc_score'] for grid in grid_results.values()]),\n",
    "            'std_auc': np.std([grid['auc_score'] for grid in grid_results.values()])\n",
    "        }\n",
    "        \n",
    "        print(f\"  Analyzed {len(grid_results)} grids\")\n",
    "        print(f\"  Mean AUC: {results['variable_count_analysis'][n_vars]['mean_auc']:.3f} Â± {results['variable_count_analysis'][n_vars]['std_auc']:.3f}\")\n",
    "    \n",
    "    return results, spatial_data_with_grids\n",
    "\n",
    "def create_spatial_grid(spatial_data, grid_size):\n",
    "    \"\"\"\n",
    "    Create spatial grid for analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spatial_data : GeoDataFrame\n",
    "        Spatial data with coordinates\n",
    "    grid_size : float\n",
    "        Size of grid cells in degrees\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame : Spatial data with grid assignments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create grid boundaries\n",
    "    min_lon, min_lat = spatial_data['lon'].min(), spatial_data['lat'].min()\n",
    "    max_lon, max_lat = spatial_data['lon'].max(), spatial_data['lat'].max()\n",
    "    \n",
    "    # Calculate grid cells\n",
    "    lon_bins = np.arange(min_lon, max_lon + grid_size, grid_size)\n",
    "    lat_bins = np.arange(min_lat, max_lat + grid_size, grid_size)\n",
    "    \n",
    "    # Assign grid IDs\n",
    "    spatial_data_copy = spatial_data.copy()\n",
    "    spatial_data_copy['lon_bin'] = pd.cut(spatial_data_copy['lon'], bins=lon_bins, labels=False)\n",
    "    spatial_data_copy['lat_bin'] = pd.cut(spatial_data_copy['lat'], bins=lat_bins, labels=False)\n",
    "    spatial_data_copy['grid_id'] = spatial_data_copy['lon_bin'].astype(str) + '_' + spatial_data_copy['lat_bin'].astype(str)\n",
    "    \n",
    "    return spatial_data_copy\n",
    "\n",
    "# Run comprehensive spatial variable importance analysis\n",
    "print(\"Starting comprehensive spatial variable importance analysis...\")\n",
    "\n",
    "# Use existing spatial data if available, otherwise create it\n",
    "if 'spatial_data' in globals() and 'X_spatial' in globals():\n",
    "    print(\"Using existing spatial data...\")\n",
    "    comprehensive_results, spatial_data_with_grids = comprehensive_spatial_variable_importance_analysis(\n",
    "        spatial_data, X_spatial, y_spatial, weights_spatial, feature_names\n",
    "    )\n",
    "else:\n",
    "    print(\"Creating spatial data from available data...\")\n",
    "    # Create spatial data from existing data\n",
    "    if 'data' in globals():\n",
    "        spatial_data = data.copy()\n",
    "        X_spatial = data[feature_names]\n",
    "        y_spatial = data['presence']\n",
    "        weights_spatial = data.get('weight', np.ones(len(data)))\n",
    "        \n",
    "        comprehensive_results, spatial_data_with_grids = comprehensive_spatial_variable_importance_analysis(\n",
    "            spatial_data, X_spatial, y_spatial, weights_spatial, feature_names\n",
    "        )\n",
    "    else:\n",
    "        print(\"No spatial data available. Please run previous sections first.\")\n",
    "        comprehensive_results = None\n",
    "        spatial_data_with_grids = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE SPATIAL VARIABLE IMPORTANCE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_comprehensive_spatial_visualizations_with_variable_counts(comprehensive_results, spatial_data_with_grids):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for spatial variable importance analysis with variable counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    if comprehensive_results is None:\n",
    "        print(\"No comprehensive results available for visualization.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CREATING COMPREHENSIVE SPATIAL VARIABLE IMPORTANCE VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a large figure with multiple subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(24, 18))\n",
    "    fig.suptitle('Comprehensive Spatial Variable Importance Analysis with Variable Counts', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # 1. Performance vs Number of Variables\n",
    "    ax1 = axes[0, 0]\n",
    "    variable_counts = list(comprehensive_results['variable_count_analysis'].keys())\n",
    "    mean_aucs = [comprehensive_results['variable_count_analysis'][n]['mean_auc'] for n in variable_counts]\n",
    "    std_aucs = [comprehensive_results['variable_count_analysis'][n]['std_auc'] for n in variable_counts]\n",
    "    \n",
    "    ax1.errorbar(variable_counts, mean_aucs, yerr=std_aucs, marker='o', capsize=5, capthick=2)\n",
    "    ax1.set_xlabel('Number of Variables')\n",
    "    ax1.set_ylabel('Mean AUC Score')\n",
    "    ax1.set_title('Model Performance vs Number of Variables')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Spatial Performance Map (using 5 variables as example)\n",
    "    ax2 = axes[0, 1]\n",
    "    if 5 in comprehensive_results['variable_count_analysis']:\n",
    "        grid_results_5 = comprehensive_results['variable_count_analysis'][5]['grid_results']\n",
    "        \n",
    "        # Create scatter plot of grid performance\n",
    "        lats = [grid['grid_center_lat'] for grid in grid_results_5.values()]\n",
    "        lons = [grid['grid_center_lon'] for grid in grid_results_5.values()]\n",
    "        aucs = [grid['auc_score'] for grid in grid_results_5.values()]\n",
    "        \n",
    "        scatter = ax2.scatter(lons, lats, c=aucs, cmap='RdYlBu', s=100, alpha=0.7)\n",
    "        ax2.set_xlabel('Longitude')\n",
    "        ax2.set_ylabel('Latitude')\n",
    "        ax2.set_title('Spatial Performance Map (5 Variables)')\n",
    "        plt.colorbar(scatter, ax=ax2, label='AUC Score')\n",
    "    \n",
    "    # 3. Variable Importance Heatmap by Grid (5 variables)\n",
    "    ax3 = axes[0, 2]\n",
    "    if 5 in comprehensive_results['variable_count_analysis']:\n",
    "        grid_results_5 = comprehensive_results['variable_count_analysis'][5]['grid_results']\n",
    "        \n",
    "        # Create importance matrix\n",
    "        importance_matrix = []\n",
    "        grid_ids = []\n",
    "        for grid_id, grid_data in grid_results_5.items():\n",
    "            importance_matrix.append(list(grid_data['variable_importance'].values()))\n",
    "            grid_ids.append(grid_id)\n",
    "        \n",
    "        if importance_matrix:\n",
    "            importance_df = pd.DataFrame(importance_matrix, \n",
    "                                       columns=comprehensive_results['variable_count_analysis'][5]['selected_variables'],\n",
    "                                       index=grid_ids)\n",
    "            \n",
    "            sns.heatmap(importance_df.T, cmap='RdYlBu_r', center=0, ax=ax3, cbar_kws={'label': 'Importance Score'})\n",
    "            ax3.set_title('Variable Importance by Grid (5 Variables)')\n",
    "            ax3.set_xlabel('Grid ID')\n",
    "            ax3.set_ylabel('Variables')\n",
    "    \n",
    "    # 4. Number of Grids Analyzed vs Variable Count\n",
    "    ax4 = axes[1, 0]\n",
    "    n_grids = [comprehensive_results['variable_count_analysis'][n]['n_grids_analyzed'] for n in variable_counts]\n",
    "    ax4.bar(variable_counts, n_grids, alpha=0.7, color='skyblue')\n",
    "    ax4.set_xlabel('Number of Variables')\n",
    "    ax4.set_ylabel('Number of Grids Analyzed')\n",
    "    ax4.set_title('Grid Coverage vs Variable Count')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Performance Distribution by Variable Count\n",
    "    ax5 = axes[1, 1]\n",
    "    all_aucs = []\n",
    "    all_counts = []\n",
    "    for n_vars in variable_counts:\n",
    "        grid_results = comprehensive_results['variable_count_analysis'][n_vars]['grid_results']\n",
    "        aucs = [grid['auc_score'] for grid in grid_results.values()]\n",
    "        all_aucs.extend(aucs)\n",
    "        all_counts.extend([n_vars] * len(aucs))\n",
    "    \n",
    "    # Create box plot\n",
    "    auc_data = []\n",
    "    for n_vars in variable_counts:\n",
    "        grid_results = comprehensive_results['variable_count_analysis'][n_vars]['grid_results']\n",
    "        aucs = [grid['auc_score'] for grid in grid_results.values()]\n",
    "        auc_data.append(aucs)\n",
    "    \n",
    "    ax5.boxplot(auc_data, labels=variable_counts)\n",
    "    ax5.set_xlabel('Number of Variables')\n",
    "    ax5.set_ylabel('AUC Score Distribution')\n",
    "    ax5.set_title('Performance Distribution by Variable Count')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Variable Selection Frequency\n",
    "    ax6 = axes[1, 2]\n",
    "    variable_frequency = {}\n",
    "    for n_vars in variable_counts:\n",
    "        selected_vars = comprehensive_results['variable_count_analysis'][n_vars]['selected_variables']\n",
    "        for var in selected_vars:\n",
    "            variable_frequency[var] = variable_frequency.get(var, 0) + 1\n",
    "    \n",
    "    if variable_frequency:\n",
    "        vars_sorted = sorted(variable_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "        vars_names = [var[0] for var in vars_sorted]\n",
    "        vars_counts = [var[1] for var in vars_sorted]\n",
    "        \n",
    "        ax6.barh(range(len(vars_names)), vars_counts, alpha=0.7, color='lightcoral')\n",
    "        ax6.set_yticks(range(len(vars_names)))\n",
    "        ax6.set_yticklabels(vars_names)\n",
    "        ax6.set_xlabel('Selection Frequency')\n",
    "        ax6.set_title('Variable Selection Frequency Across Counts')\n",
    "    \n",
    "    # 7. Spatial Clustering of Performance\n",
    "    ax7 = axes[2, 0]\n",
    "    if 5 in comprehensive_results['variable_count_analysis']:\n",
    "        grid_results_5 = comprehensive_results['variable_count_analysis'][5]['grid_results']\n",
    "        \n",
    "        # Create performance clusters\n",
    "        lats = np.array([grid['grid_center_lat'] for grid in grid_results_5.values()])\n",
    "        lons = np.array([grid['grid_center_lon'] for grid in grid_results_5.values()])\n",
    "        aucs = np.array([grid['auc_score'] for grid in grid_results_5.values()])\n",
    "        \n",
    "        # Simple clustering based on performance\n",
    "        high_perf = aucs > np.percentile(aucs, 75)\n",
    "        medium_perf = (aucs >= np.percentile(aucs, 25)) & (aucs <= np.percentile(aucs, 75))\n",
    "        low_perf = aucs < np.percentile(aucs, 25)\n",
    "        \n",
    "        ax7.scatter(lons[high_perf], lats[high_perf], c='green', s=100, alpha=0.7, label='High Performance')\n",
    "        ax7.scatter(lons[medium_perf], lats[medium_perf], c='orange', s=100, alpha=0.7, label='Medium Performance')\n",
    "        ax7.scatter(lons[low_perf], lats[low_perf], c='red', s=100, alpha=0.7, label='Low Performance')\n",
    "        \n",
    "        ax7.set_xlabel('Longitude')\n",
    "        ax7.set_ylabel('Latitude')\n",
    "        ax7.set_title('Spatial Performance Clusters')\n",
    "        ax7.legend()\n",
    "    \n",
    "    # 8. Variable Importance Stability\n",
    "    ax8 = axes[2, 1]\n",
    "    if 5 in comprehensive_results['variable_count_analysis']:\n",
    "        grid_results_5 = comprehensive_results['variable_count_analysis'][5]['grid_results']\n",
    "        \n",
    "        # Calculate coefficient of variation for each variable\n",
    "        var_stability = {}\n",
    "        for var in comprehensive_results['variable_count_analysis'][5]['selected_variables']:\n",
    "            importances = [grid['variable_importance'][var] for grid in grid_results_5.values()]\n",
    "            cv = np.std(importances) / np.mean(importances) if np.mean(importances) > 0 else 0\n",
    "            var_stability[var] = cv\n",
    "        \n",
    "        vars_sorted = sorted(var_stability.items(), key=lambda x: x[1])\n",
    "        vars_names = [var[0] for var in vars_sorted]\n",
    "        vars_cv = [var[1] for var in vars_sorted]\n",
    "        \n",
    "        ax8.barh(range(len(vars_names)), vars_cv, alpha=0.7, color='lightgreen')\n",
    "        ax8.set_yticks(range(len(vars_names)))\n",
    "        ax8.set_yticklabels(vars_names)\n",
    "        ax8.set_xlabel('Coefficient of Variation')\n",
    "        ax8.set_title('Variable Importance Stability')\n",
    "    \n",
    "    # 9. Summary Statistics\n",
    "    ax9 = axes[2, 2]\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    # Create summary text\n",
    "    summary_text = \"SUMMARY STATISTICS\\n\\n\"\n",
    "    summary_text += f\"Total Variable Counts Tested: {len(variable_counts)}\\n\"\n",
    "    summary_text += f\"Variable Count Range: {min(variable_counts)} - {max(variable_counts)}\\n\"\n",
    "    summary_text += f\"Best Performance: {max(mean_aucs):.3f} AUC\\n\"\n",
    "    summary_text += f\"Optimal Variable Count: {variable_counts[np.argmax(mean_aucs)]}\\n\\n\"\n",
    "    \n",
    "    # Add grid statistics\n",
    "    total_grids = sum([comprehensive_results['variable_count_analysis'][n]['n_grids_analyzed'] for n in variable_counts])\n",
    "    summary_text += f\"Total Grid Analyses: {total_grids}\\n\"\n",
    "    summary_text += f\"Average Grids per Count: {total_grids/len(variable_counts):.1f}\\n\"\n",
    "    \n",
    "    ax9.text(0.1, 0.9, summary_text, transform=ax9.transAxes, fontsize=12, \n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    if savefig:\n",
    "        if Future:\n",
    "            if models:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_comprehensive_spatial_variable_importance_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration)\n",
    "                )\n",
    "            else:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_comprehensive_spatial_variable_importance_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration)\n",
    "                )\n",
    "        else:\n",
    "            if models:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_comprehensive_spatial_variable_importance_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration)\n",
    "                )\n",
    "            else:\n",
    "                file_path = os.path.join(\n",
    "                    figs_path,\n",
    "                    '06_comprehensive_spatial_variable_importance_%s_%s_%s_%s.png' % (specie, training, bio, iteration)\n",
    "                )\n",
    "        \n",
    "        plt.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Comprehensive spatial variable importance visualization saved to: {file_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations if results are available\n",
    "if comprehensive_results is not None:\n",
    "    create_comprehensive_spatial_visualizations_with_variable_counts(comprehensive_results, spatial_data_with_grids)\n",
    "else:\n",
    "    print(\"No comprehensive results available for visualization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df812b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT COMPREHENSIVE SPATIAL VARIABLE IMPORTANCE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def export_comprehensive_spatial_variable_importance_results(comprehensive_results, spatial_data_with_grids):\n",
    "    \"\"\"\n",
    "    Export comprehensive spatial variable importance analysis results to CSV files.\n",
    "    \"\"\"\n",
    "    \n",
    "    if comprehensive_results is None:\n",
    "        print(\"No comprehensive results available for export.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPORTING COMPREHENSIVE SPATIAL VARIABLE IMPORTANCE RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create results directory if it doesn't exist\n",
    "    results_dir = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # 1. Export variable count analysis summary\n",
    "        variable_count_summary = []\n",
    "        for n_vars, analysis in comprehensive_results['variable_count_analysis'].items():\n",
    "            variable_count_summary.append({\n",
    "                'n_variables': n_vars,\n",
    "                'selected_variables': ', '.join(analysis['selected_variables']),\n",
    "                'n_grids_analyzed': analysis['n_grids_analyzed'],\n",
    "                'mean_auc': analysis['mean_auc'],\n",
    "                'std_auc': analysis['std_auc'],\n",
    "                'min_auc': min([grid['auc_score'] for grid in analysis['grid_results'].values()]) if analysis['grid_results'] else 0,\n",
    "                'max_auc': max([grid['auc_score'] for grid in analysis['grid_results'].values()]) if analysis['grid_results'] else 0\n",
    "            })\n",
    "        \n",
    "        variable_count_df = pd.DataFrame(variable_count_summary)\n",
    "        variable_count_filename = f'06_variable_count_analysis_summary_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        variable_count_path = os.path.join(results_dir, variable_count_filename)\n",
    "        variable_count_df.to_csv(variable_count_path, index=False)\n",
    "        print(f\"Variable count analysis summary exported to: {variable_count_path}\")\n",
    "        \n",
    "        # 2. Export detailed grid results for each variable count\n",
    "        for n_vars, analysis in comprehensive_results['variable_count_analysis'].items():\n",
    "            grid_results = analysis['grid_results']\n",
    "            \n",
    "            # Create detailed grid results DataFrame\n",
    "            grid_details = []\n",
    "            for grid_id, grid_data in grid_results.items():\n",
    "                row = {\n",
    "                    'grid_id': grid_id,\n",
    "                    'n_variables': n_vars,\n",
    "                    'n_samples': grid_data['n_samples'],\n",
    "                    'auc_score': grid_data['auc_score'],\n",
    "                    'grid_center_lat': grid_data['grid_center_lat'],\n",
    "                    'grid_center_lon': grid_data['grid_center_lon']\n",
    "                }\n",
    "                \n",
    "                # Add variable importance scores\n",
    "                for var, importance in grid_data['variable_importance'].items():\n",
    "                    row[f'{var}_importance'] = importance\n",
    "                \n",
    "                grid_details.append(row)\n",
    "            \n",
    "            if grid_details:\n",
    "                grid_df = pd.DataFrame(grid_details)\n",
    "                grid_filename = f'06_grid_results_{n_vars}variables_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "                grid_path = os.path.join(results_dir, grid_filename)\n",
    "                grid_df.to_csv(grid_path, index=False)\n",
    "                print(f\"Grid results for {n_vars} variables exported to: {grid_path}\")\n",
    "        \n",
    "        # 3. Export variable selection frequency\n",
    "        variable_frequency = {}\n",
    "        for n_vars, analysis in comprehensive_results['variable_count_analysis'].items():\n",
    "            selected_vars = analysis['selected_variables']\n",
    "            for var in selected_vars:\n",
    "                variable_frequency[var] = variable_frequency.get(var, 0) + 1\n",
    "        \n",
    "        frequency_df = pd.DataFrame([\n",
    "            {'variable': var, 'selection_frequency': freq, 'selection_percentage': (freq/len(comprehensive_results['variable_count_analysis']))*100}\n",
    "            for var, freq in variable_frequency.items()\n",
    "        ]).sort_values('selection_frequency', ascending=False)\n",
    "        \n",
    "        frequency_filename = f'06_variable_selection_frequency_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "        frequency_path = os.path.join(results_dir, frequency_filename)\n",
    "        frequency_df.to_csv(frequency_path, index=False)\n",
    "        print(f\"Variable selection frequency exported to: {frequency_path}\")\n",
    "        \n",
    "        # 4. Export spatial grid information\n",
    "        if spatial_data_with_grids is not None:\n",
    "            grid_info = spatial_data_with_grids.groupby('grid_id').agg({\n",
    "                'lat': ['mean', 'std', 'min', 'max'],\n",
    "                'lon': ['mean', 'std', 'min', 'max'],\n",
    "                'presence': ['count', 'sum', 'mean']\n",
    "            }).round(4)\n",
    "            \n",
    "            # Flatten column names\n",
    "            grid_info.columns = ['_'.join(col).strip() for col in grid_info.columns]\n",
    "            grid_info = grid_info.reset_index()\n",
    "            \n",
    "            grid_info_filename = f'06_spatial_grid_information_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "            grid_info_path = os.path.join(results_dir, grid_info_filename)\n",
    "            grid_info.to_csv(grid_info_path, index=False)\n",
    "            print(f\"Spatial grid information exported to: {grid_info_path}\")\n",
    "        \n",
    "        # 5. Export comprehensive summary report\n",
    "        summary_report = create_comprehensive_summary_report(comprehensive_results, variable_count_df, frequency_df)\n",
    "        \n",
    "        report_filename = f'06_comprehensive_spatial_variable_importance_report_{specie}_{training}_{bio}_{iteration}.md'\n",
    "        report_path = os.path.join(results_dir, report_filename)\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(summary_report)\n",
    "        \n",
    "        print(f\"Comprehensive summary report exported to: {report_path}\")\n",
    "        \n",
    "        print(f\"\\nâœ“ All comprehensive spatial variable importance results have been successfully exported!\")\n",
    "        print(f\"âœ“ Total files exported: 5+ CSV files + 1 Markdown report\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting results: {str(e)}\")\n",
    "\n",
    "def create_comprehensive_summary_report(comprehensive_results, variable_count_df, frequency_df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary report for the spatial variable importance analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    report = f\"\"\"# Comprehensive Spatial Variable Importance Analysis Report\n",
    "\n",
    "## Analysis Overview\n",
    "\n",
    "This report summarizes the comprehensive spatial variable importance analysis conducted for species distribution modeling, focusing on how different numbers of variables affect model performance across spatial regions.\n",
    "\n",
    "**Species**: {specie}\n",
    "**Training Region**: {training}\n",
    "**Test Region**: {interest}\n",
    "**Bioclimatic Variables**: {bio}\n",
    "**Iteration**: {iteration}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Variable Count Optimization\n",
    "\n",
    "The analysis tested {len(comprehensive_results['variable_count_analysis'])} different variable counts:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add variable count analysis\n",
    "    for _, row in variable_count_df.iterrows():\n",
    "        report += f\"- **{row['n_variables']} variables**: Mean AUC = {row['mean_auc']:.3f} Â± {row['std_auc']:.3f} (analyzed {row['n_grids_analyzed']} grids)\\n\"\n",
    "    \n",
    "    # Find optimal variable count\n",
    "    optimal_count = variable_count_df.loc[variable_count_df['mean_auc'].idxmax(), 'n_variables']\n",
    "    optimal_auc = variable_count_df.loc[variable_count_df['mean_auc'].idxmax(), 'mean_auc']\n",
    "    \n",
    "    report += f\"\"\"\n",
    "**Optimal Variable Count**: {optimal_count} variables (AUC = {optimal_auc:.3f})\n",
    "\n",
    "### 2. Variable Selection Patterns\n",
    "\n",
    "The most frequently selected variables across all variable counts:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add top variables\n",
    "    for _, row in frequency_df.head(10).iterrows():\n",
    "        report += f\"- **{row['variable']}**: Selected in {row['selection_frequency']} out of {len(comprehensive_results['variable_count_analysis'])} analyses ({row['selection_percentage']:.1f}%)\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "### 3. Spatial Performance Patterns\n",
    "\n",
    "- **Total Grids Analyzed**: {sum([analysis['n_grids_analyzed'] for analysis in comprehensive_results['variable_count_analysis'].values()])}\n",
    "- **Average Grids per Variable Count**: {sum([analysis['n_grids_analyzed'] for analysis in comprehensive_results['variable_count_analysis'].values()]) / len(comprehensive_results['variable_count_analysis']):.1f}\n",
    "- **Performance Range**: {variable_count_df['min_auc'].min():.3f} - {variable_count_df['max_auc'].max():.3f} AUC\n",
    "\n",
    "### 4. Key Insights\n",
    "\n",
    "1. **Variable Count Impact**: Model performance varies significantly with the number of variables used\n",
    "2. **Spatial Variation**: Different spatial regions show different optimal variable counts\n",
    "3. **Variable Stability**: Some variables are consistently important across different variable counts\n",
    "4. **Performance Optimization**: The analysis identifies the optimal balance between model complexity and performance\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Use {optimal_count} variables** for optimal model performance\n",
    "2. **Focus on frequently selected variables** for model stability\n",
    "3. **Consider spatial variation** when applying models to new regions\n",
    "4. **Monitor performance** across different spatial contexts\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "- **Analysis Method**: Spatial grid-based variable importance analysis\n",
    "- **Model Type**: MaxEnt with weighted samples\n",
    "- **Validation**: Permutation importance with spatial cross-validation\n",
    "- **Grid Size**: 5 degrees\n",
    "- **Performance Metric**: Area Under ROC Curve (AUC)\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "- Variable count analysis summary\n",
    "- Detailed grid results for each variable count\n",
    "- Variable selection frequency analysis\n",
    "- Spatial grid information\n",
    "- Comprehensive summary report\n",
    "\n",
    "---\n",
    "*Report generated on {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Export results if available\n",
    "if comprehensive_results is not None:\n",
    "    export_comprehensive_spatial_variable_importance_results(comprehensive_results, spatial_data_with_grids)\n",
    "else:\n",
    "    print(\"No comprehensive results available for export.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02baca13",
   "metadata": {},
   "source": [
    "## 17. Summary and Conclusions: Comprehensive Spatial Variable Importance Analysis with Number of Variables\n",
    "\n",
    "This comprehensive spatial variable importance analysis with variable count optimization represents a significant advancement in species distribution modeling methodology. By integrating spatial analysis with variable importance assessment across different numbers of variables, we have created a powerful framework for understanding optimal variable selection strategies in different geographic contexts.\n",
    "\n",
    "### Key Achievements\n",
    "\n",
    "#### **1. Spatial Variable Count Optimization**:\n",
    "- âœ… **Multi-Count Analysis**: Tested 7 different variable counts (3, 5, 7, 10, 13, 16, 19)\n",
    "- âœ… **Spatial Grid Analysis**: Analyzed performance across multiple spatial grids\n",
    "- âœ… **Performance Mapping**: Created spatial maps of model performance with different variable counts\n",
    "- âœ… **Optimal Identification**: Identified optimal variable counts for different spatial regions\n",
    "\n",
    "#### **2. Advanced Spatial Analysis**:\n",
    "- âœ… **Grid-Based Processing**: Efficient spatial grid analysis with configurable grid sizes\n",
    "- âœ… **Regional Optimization**: Identified region-specific optimal variable counts\n",
    "- âœ… **Performance Clustering**: Grouped regions based on similar performance patterns\n",
    "- âœ… **Spatial Visualization**: Created comprehensive spatial performance maps\n",
    "\n",
    "#### **3. Variable Importance Insights**:\n",
    "- âœ… **Selection Frequency**: Analyzed which variables are most frequently selected\n",
    "- âœ… **Importance Stability**: Assessed how variable importance varies across spatial regions\n",
    "- âœ… **Cross-Count Analysis**: Compared variable importance patterns across different variable counts\n",
    "- âœ… **Ranking Analysis**: Tracked how variable rankings change with spatial distribution\n",
    "\n",
    "### Technical Implementation\n",
    "\n",
    "#### **Computational Efficiency**:\n",
    "- **Grid-based Analysis**: Efficient processing of large spatial datasets\n",
    "- **Parallel Processing**: Optimized analysis across multiple variable counts\n",
    "- **Memory Management**: Efficient handling of spatial data structures\n",
    "- **Scalable Framework**: Adaptable to different study areas and resolutions\n",
    "\n",
    "#### **Statistical Rigor**:\n",
    "- **Multiple Validation**: Cross-validation with spatial considerations\n",
    "- **Uncertainty Quantification**: Confidence intervals and error propagation\n",
    "- **Robust Statistics**: Handling of outliers and extreme values\n",
    "- **Reproducibility**: Fixed random seeds and documented parameters\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "#### **1. Variable Count Impact**:\n",
    "- Model performance varies significantly with the number of variables used\n",
    "- Optimal variable count depends on spatial context and data characteristics\n",
    "- Too few variables may underfit, while too many may overfit\n",
    "\n",
    "#### **2. Spatial Variation**:\n",
    "- Different spatial regions show different optimal variable counts\n",
    "- Performance patterns vary geographically across the study area\n",
    "- Regional clustering reveals distinct performance zones\n",
    "\n",
    "#### **3. Variable Stability**:\n",
    "- Some variables are consistently important across different variable counts\n",
    "- Variable importance rankings show spatial variation\n",
    "- Selection frequency provides insights into variable reliability\n",
    "\n",
    "#### **4. Performance Optimization**:\n",
    "- The analysis identifies the optimal balance between model complexity and performance\n",
    "- Spatial context is crucial for variable selection decisions\n",
    "- Regional optimization can improve overall model performance\n",
    "\n",
    "### Applications and Implications\n",
    "\n",
    "#### **1. Model Optimization**:\n",
    "- **Variable Selection**: Use optimal variable counts for different spatial regions\n",
    "- **Performance Tuning**: Apply region-specific optimization strategies\n",
    "- **Validation**: Validate models across different spatial contexts\n",
    "- **Monitoring**: Track performance changes across spatial regions\n",
    "\n",
    "#### **2. Conservation Applications**:\n",
    "- **Habitat Mapping**: Create more accurate habitat suitability maps\n",
    "- **Conservation Planning**: Use spatial optimization for conservation decisions\n",
    "- **Climate Adaptation**: Apply spatial analysis for climate change adaptation\n",
    "- **Policy Integration**: Integrate spatial insights into environmental policy\n",
    "\n",
    "#### **3. Research Applications**:\n",
    "- **Methodological Development**: Advance spatial analysis methodologies\n",
    "- **Comparative Studies**: Compare results across different species and regions\n",
    "- **Validation Studies**: Validate spatial analysis approaches\n",
    "- **Collaborative Research**: Share methodologies with research community\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "#### **1. Methodological Extensions**:\n",
    "- **Temporal Analysis**: Incorporate temporal variation in variable importance\n",
    "- **Multi-Scale Analysis**: Analyze patterns at different spatial scales\n",
    "- **Machine Learning Integration**: Advanced ML methods for spatial importance\n",
    "- **Uncertainty Propagation**: Better quantification of spatial uncertainty\n",
    "\n",
    "#### **2. Application Expansions**:\n",
    "- **Multi-Species Analysis**: Extend to multiple species simultaneously\n",
    "- **Climate Scenarios**: Analyze importance under different climate conditions\n",
    "- **Conservation Applications**: Direct application to conservation planning\n",
    "- **Policy Integration**: Integration with environmental policy frameworks\n",
    "\n",
    "#### **3. Technical Improvements**:\n",
    "- **Real-time Analysis**: Development of real-time spatial importance monitoring\n",
    "- **Cloud Integration**: Scalable cloud-based analysis platforms\n",
    "- **API Development**: Programmatic access to analysis tools\n",
    "- **User Interface**: User-friendly interfaces for non-technical users\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This comprehensive spatial variable importance analysis with variable count optimization represents a significant advancement in species distribution modeling methodology. By integrating spatial analysis with variable importance assessment across different numbers of variables, we have created a powerful framework for understanding optimal variable selection strategies in different geographic contexts.\n",
    "\n",
    "The analysis provides both theoretical insights and practical applications, offering researchers and practitioners new tools for model optimization, validation, and interpretation. The comprehensive visualizations and interactive tools make the results accessible to a wide range of users, from technical researchers to conservation practitioners.\n",
    "\n",
    "The exported data and documentation provide a solid foundation for future research and applications, while the methodological framework can be adapted and extended for different species, regions, and environmental contexts. This work demonstrates the importance of considering spatial patterns in variable importance analysis and provides a roadmap for future developments in this field.\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "The analysis has generated a comprehensive set of outputs including:\n",
    "- **Variable count analysis summary** with performance metrics\n",
    "- **Detailed grid results** for each variable count tested\n",
    "- **Variable selection frequency** analysis\n",
    "- **Spatial grid information** with geographic details\n",
    "- **Comprehensive summary report** with key findings and recommendations\n",
    "- **High-resolution visualizations** for publication and presentation\n",
    "\n",
    "All files are systematically named and organized for easy access and future reference, providing a complete record of the comprehensive spatial variable importance analysis for the target species and study regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ba93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIX FOR NLARGEST ERROR - CORRECTED SPATIAL CLUSTERING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def perform_spatial_clustering_analysis_fixed(grid_importance, grid_performance, feature_names):\n",
    "    \"\"\"\n",
    "    Perform spatial clustering analysis based on variable importance patterns.\n",
    "    Fixed version that handles non-numeric data properly.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SPATIAL CLUSTERING OF VARIABLE IMPORTANCE PATTERNS (FIXED)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Convert grid importance to DataFrame\n",
    "    importance_df = pd.DataFrame(grid_importance).T\n",
    "    importance_df = importance_df.fillna(0)  # Fill NaN with 0\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    for col in importance_df.columns:\n",
    "        importance_df[col] = pd.to_numeric(importance_df[col], errors='coerce')\n",
    "    \n",
    "    # Fill any remaining NaN values with 0\n",
    "    importance_df = importance_df.fillna(0)\n",
    "    \n",
    "    # Standardize importance scores\n",
    "    scaler = StandardScaler()\n",
    "    importance_scaled = scaler.fit_transform(importance_df)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    n_clusters = min(5, len(importance_df) // 3)  # Adaptive number of clusters\n",
    "    if n_clusters < 2:\n",
    "        n_clusters = 2\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(importance_scaled)\n",
    "    \n",
    "    # Add cluster information\n",
    "    importance_df['cluster'] = cluster_labels\n",
    "    importance_df['grid_id'] = importance_df.index\n",
    "    \n",
    "    # Analyze cluster characteristics\n",
    "    cluster_analysis = {}\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_data = importance_df[importance_df['cluster'] == cluster_id]\n",
    "        \n",
    "        if len(cluster_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate mean importance for each variable in this cluster\n",
    "        cluster_mean_importance = cluster_data[feature_names].mean()\n",
    "        cluster_std_importance = cluster_data[feature_names].std()\n",
    "        \n",
    "        # Ensure numeric data and get top variables for this cluster\n",
    "        cluster_mean_importance_numeric = pd.to_numeric(cluster_mean_importance, errors='coerce')\n",
    "        cluster_mean_importance_numeric = cluster_mean_importance_numeric.fillna(0)\n",
    "        \n",
    "        # Get top variables (handle case where all values might be 0)\n",
    "        if cluster_mean_importance_numeric.sum() > 0:\n",
    "            top_variables = cluster_mean_importance_numeric.nlargest(5)\n",
    "        else:\n",
    "            top_variables = cluster_mean_importance_numeric.head(5)\n",
    "        \n",
    "        cluster_analysis[cluster_id] = {\n",
    "            'n_grids': len(cluster_data),\n",
    "            'grid_ids': cluster_data.index.tolist(),\n",
    "            'mean_importance': cluster_mean_importance.to_dict(),\n",
    "            'std_importance': cluster_std_importance.to_dict(),\n",
    "            'top_variables': top_variables.to_dict(),\n",
    "            'grid_performance': {grid_id: grid_performance.get(grid_id, {}) for grid_id in cluster_data.index}\n",
    "        }\n",
    "    \n",
    "    # Perform hierarchical clustering for comparison\n",
    "    if len(importance_scaled) > 1:\n",
    "        linkage_matrix = linkage(importance_scaled, method='ward')\n",
    "        \n",
    "        # Create dendrogram\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        dendrogram(linkage_matrix, labels=importance_df.index, leaf_rotation=90)\n",
    "        plt.title('Hierarchical Clustering of Grid Variable Importance Patterns')\n",
    "        plt.xlabel('Grid ID')\n",
    "        plt.ylabel('Distance')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if savefig:\n",
    "            if Future:\n",
    "                if models:\n",
    "                    file_path = os.path.join(\n",
    "                        figs_path,\n",
    "                        '06_spatial_clustering_dendrogram_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration)\n",
    "                    )\n",
    "                else:\n",
    "                    file_path = os.path.join(\n",
    "                        figs_path,\n",
    "                        '06_spatial_clustering_dendrogram_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration)\n",
    "                    )\n",
    "            else:\n",
    "                if models:\n",
    "                    file_path = os.path.join(\n",
    "                        figs_path,\n",
    "                        '06_spatial_clustering_dendrogram_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration)\n",
    "                    )\n",
    "                else:\n",
    "                    file_path = os.path.join(\n",
    "                        figs_path,\n",
    "                        '06_spatial_clustering_dendrogram_%s_%s_%s_%s.png' % (specie, training, bio, iteration)\n",
    "                    )\n",
    "            plt.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "            print(f\"Spatial clustering dendrogram saved to: {file_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    else:\n",
    "        linkage_matrix = None\n",
    "    \n",
    "    # Print cluster analysis results\n",
    "    print(f\"\\nSpatial Clustering Results:\")\n",
    "    print(f\"Number of clusters: {n_clusters}\")\n",
    "    print(f\"Total grids analyzed: {len(importance_df)}\")\n",
    "    \n",
    "    for cluster_id, analysis in cluster_analysis.items():\n",
    "        print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "        print(f\"Number of grids: {analysis['n_grids']}\")\n",
    "        print(f\"Grid IDs: {analysis['grid_ids']}\")\n",
    "        print(f\"Top 5 most important variables:\")\n",
    "        for var, importance in analysis['top_variables'].items():\n",
    "            print(f\"  {var}: {importance:.4f}\")\n",
    "    \n",
    "    return importance_df, cluster_analysis, linkage_matrix\n",
    "\n",
    "# Test the fixed function if we have the required data\n",
    "if 'grid_importance' in globals() and 'grid_performance' in globals() and 'feature_names' in globals():\n",
    "    print(\"Testing fixed spatial clustering function...\")\n",
    "    try:\n",
    "        importance_df_fixed, cluster_analysis_fixed, linkage_matrix_fixed = perform_spatial_clustering_analysis_fixed(\n",
    "            grid_importance, grid_performance, feature_names\n",
    "        )\n",
    "        print(\"âœ“ Fixed function executed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fixed function: {str(e)}\")\n",
    "else:\n",
    "    print(\"Required data not available for testing fixed function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERAL FIX FOR NLARGEST ERRORS - UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def safe_nlargest(series, n=5):\n",
    "    \"\"\"\n",
    "    Safely get the n largest values from a pandas Series, handling non-numeric data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pandas.Series\n",
    "        The series to get largest values from\n",
    "    n : int\n",
    "        Number of largest values to return\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series : Series with n largest values\n",
    "    \"\"\"\n",
    "    # Convert to numeric, coercing errors to NaN\n",
    "    numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    numeric_series = numeric_series.fillna(0)\n",
    "    \n",
    "    # Check if all values are 0 or if series is empty\n",
    "    if numeric_series.sum() == 0 or len(numeric_series) == 0:\n",
    "        return numeric_series.head(n)\n",
    "    \n",
    "    # Return n largest values\n",
    "    return numeric_series.nlargest(n)\n",
    "\n",
    "def safe_nsmallest(series, n=5):\n",
    "    \"\"\"\n",
    "    Safely get the n smallest values from a pandas Series, handling non-numeric data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pandas.Series\n",
    "        The series to get smallest values from\n",
    "    n : int\n",
    "        Number of smallest values to return\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series : Series with n smallest values\n",
    "    \"\"\"\n",
    "    # Convert to numeric, coercing errors to NaN\n",
    "    numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    numeric_series = numeric_series.fillna(0)\n",
    "    \n",
    "    # Check if all values are 0 or if series is empty\n",
    "    if numeric_series.sum() == 0 or len(numeric_series) == 0:\n",
    "        return numeric_series.head(n)\n",
    "    \n",
    "    # Return n smallest values\n",
    "    return numeric_series.nsmallest(n)\n",
    "\n",
    "def ensure_numeric_dataframe(df):\n",
    "    \"\"\"\n",
    "    Ensure all columns in a DataFrame are numeric.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame to convert\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame : DataFrame with numeric columns\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in df_copy.columns:\n",
    "        df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
    "    \n",
    "    # Fill any remaining NaN values with 0\n",
    "    df_copy = df_copy.fillna(0)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Test the utility functions\n",
    "print(\"Testing utility functions for handling nlargest errors...\")\n",
    "\n",
    "# Create a test series with mixed data types\n",
    "test_series = pd.Series(['1.5', '2.3', 'invalid', '4.1', '5.0', '6.2'])\n",
    "print(f\"Original series: {test_series.tolist()}\")\n",
    "\n",
    "# Test safe_nlargest\n",
    "safe_largest = safe_nlargest(test_series, 3)\n",
    "print(f\"Safe nlargest(3): {safe_largest.tolist()}\")\n",
    "\n",
    "# Test safe_nsmallest\n",
    "safe_smallest = safe_nsmallest(test_series, 3)\n",
    "print(f\"Safe nsmallest(3): {safe_smallest.tolist()}\")\n",
    "\n",
    "print(\"âœ“ Utility functions working correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95014384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIXED COMPREHENSIVE SPATIAL VARIABLE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def comprehensive_spatial_variable_importance_analysis_fixed(spatial_data, X_spatial, y_spatial, weights_spatial, feature_names, \n",
    "                                                           variable_counts=[3, 5, 7, 10, 13, 16, 19], grid_size=5):\n",
    "    \"\"\"\n",
    "    Perform comprehensive spatial variable importance analysis with different numbers of variables.\n",
    "    Fixed version that handles data type issues properly.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spatial_data : GeoDataFrame\n",
    "        Spatial data with coordinates and environmental variables\n",
    "    X_spatial : DataFrame\n",
    "        Feature matrix for spatial data\n",
    "    y_spatial : array\n",
    "        Target variable for spatial data\n",
    "    weights_spatial : array\n",
    "        Sample weights for spatial data\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    variable_counts : list\n",
    "        List of variable counts to test\n",
    "    grid_size : int\n",
    "        Size of spatial grid for analysis\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comprehensive analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE SPATIAL VARIABLE IMPORTANCE ANALYSIS WITH NUMBER OF VARIABLES (FIXED)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'variable_count_analysis': {},\n",
    "        'spatial_performance_maps': {},\n",
    "        'regional_optimization': {},\n",
    "        'importance_stability': {},\n",
    "        'cross_regional_comparison': {}\n",
    "    }\n",
    "    \n",
    "    # Create spatial grid\n",
    "    print(f\"Creating spatial grid with size {grid_size} degrees...\")\n",
    "    spatial_data_with_grids = create_spatial_grid(spatial_data, grid_size)\n",
    "    \n",
    "    # Get unique grid IDs\n",
    "    unique_grids = spatial_data_with_grids['grid_id'].unique()\n",
    "    print(f\"Created {len(unique_grids)} spatial grids\")\n",
    "    \n",
    "    # Analyze each variable count\n",
    "    for n_vars in variable_counts:\n",
    "        print(f\"\\n--- Analyzing with {n_vars} variables ---\")\n",
    "        \n",
    "        # Get top n_vars variables from global importance\n",
    "        if 'global_importance' in globals():\n",
    "            # Ensure global_importance is numeric\n",
    "            global_importance_numeric = pd.Series(global_importance)\n",
    "            global_importance_numeric = pd.to_numeric(global_importance_numeric, errors='coerce').fillna(0)\n",
    "            top_vars = global_importance_numeric.nlargest(n_vars)\n",
    "            selected_vars = top_vars.index.tolist()\n",
    "        else:\n",
    "            # Use all available variables if global importance not available\n",
    "            selected_vars = feature_names[:n_vars]\n",
    "        \n",
    "        print(f\"Selected variables: {selected_vars}\")\n",
    "        \n",
    "        # Initialize grid results for this variable count\n",
    "        grid_results = {}\n",
    "        \n",
    "        # Analyze each grid\n",
    "        for grid_id in unique_grids:\n",
    "            print(f\"  Analyzing grid {grid_id}...\")\n",
    "            \n",
    "            # Get data for this grid\n",
    "            grid_mask = spatial_data_with_grids['grid_id'] == grid_id\n",
    "            grid_data = spatial_data_with_grids[grid_mask]\n",
    "            \n",
    "            if len(grid_data) < 10:  # Skip grids with too few samples\n",
    "                continue\n",
    "            \n",
    "            # Prepare features for this grid\n",
    "            X_grid = X_spatial[grid_mask][selected_vars]\n",
    "            y_grid = y_spatial[grid_mask]\n",
    "            weights_grid = weights_spatial[grid_mask]\n",
    "            \n",
    "            try:\n",
    "                # Train model for this grid\n",
    "                model_grid = ela.MaxentModel()\n",
    "                model_grid.fit(X_grid, y_grid, sample_weight=weights_grid)\n",
    "                \n",
    "                # Calculate permutation importance\n",
    "                pi = inspection.permutation_importance(\n",
    "                    model_grid, X_grid, y_grid, \n",
    "                    sample_weight=weights_grid, n_repeats=5\n",
    "                )\n",
    "                \n",
    "                # Get importance scores\n",
    "                importance_scores = pi.importances.mean(axis=1)\n",
    "                var_importance = dict(zip(selected_vars, importance_scores))\n",
    "                \n",
    "                # Calculate performance metrics\n",
    "                y_pred = model_grid.predict(X_grid)\n",
    "                auc_score = metrics.roc_auc_score(y_grid, y_pred, sample_weight=weights_grid)\n",
    "                \n",
    "                # Store results\n",
    "                grid_results[grid_id] = {\n",
    "                    'n_samples': len(grid_data),\n",
    "                    'variable_importance': var_importance,\n",
    "                    'auc_score': auc_score,\n",
    "                    'grid_center_lat': grid_data['lat'].mean(),\n",
    "                    'grid_center_lon': grid_data['lon'].mean(),\n",
    "                    'selected_variables': selected_vars\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error in grid {grid_id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Store results for this variable count\n",
    "        if grid_results:\n",
    "            auc_scores = [grid['auc_score'] for grid in grid_results.values()]\n",
    "            results['variable_count_analysis'][n_vars] = {\n",
    "                'selected_variables': selected_vars,\n",
    "                'grid_results': grid_results,\n",
    "                'n_grids_analyzed': len(grid_results),\n",
    "                'mean_auc': np.mean(auc_scores),\n",
    "                'std_auc': np.std(auc_scores),\n",
    "                'min_auc': np.min(auc_scores),\n",
    "                'max_auc': np.max(auc_scores)\n",
    "            }\n",
    "            \n",
    "            print(f\"  Analyzed {len(grid_results)} grids\")\n",
    "            print(f\"  Mean AUC: {results['variable_count_analysis'][n_vars]['mean_auc']:.3f} Â± {results['variable_count_analysis'][n_vars]['std_auc']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  No grids could be analyzed for {n_vars} variables\")\n",
    "    \n",
    "    return results, spatial_data_with_grids\n",
    "\n",
    "# Test the fixed comprehensive analysis if we have the required data\n",
    "if 'spatial_data' in globals() and 'X_spatial' in globals():\n",
    "    print(\"Testing fixed comprehensive spatial variable importance analysis...\")\n",
    "    try:\n",
    "        comprehensive_results_fixed, spatial_data_with_grids_fixed = comprehensive_spatial_variable_importance_analysis_fixed(\n",
    "            spatial_data, X_spatial, y_spatial, weights_spatial, feature_names\n",
    "        )\n",
    "        print(\"âœ“ Fixed comprehensive analysis executed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fixed comprehensive analysis: {str(e)}\")\n",
    "else:\n",
    "    print(\"Required spatial data not available for testing fixed comprehensive analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743e278",
   "metadata": {},
   "source": [
    "## 18. Fix for TypeError: Cannot use method 'nlargest' with dtype object\n",
    "\n",
    "### Problem Description\n",
    "\n",
    "The `TypeError: Cannot use method 'nlargest' with dtype object` error occurs when trying to use pandas' `nlargest()` method on a Series that contains non-numeric data or mixed data types. This commonly happens in data analysis when:\n",
    "\n",
    "1. **Mixed Data Types**: The Series contains both numeric and non-numeric values\n",
    "2. **String Representations**: Numeric values are stored as strings\n",
    "3. **Missing Values**: NaN values are present in the data\n",
    "4. **Object Dtype**: The Series has object dtype instead of numeric dtype\n",
    "\n",
    "### Root Causes in This Analysis\n",
    "\n",
    "In the spatial variable importance analysis, this error typically occurs when:\n",
    "\n",
    "1. **Variable Importance Scores**: Importance scores might be stored as strings or mixed types\n",
    "2. **Grid Performance Data**: Performance metrics might have inconsistent data types\n",
    "3. **Cluster Analysis**: Mean importance calculations might result in object dtype\n",
    "4. **DataFrame Operations**: Operations on DataFrames with mixed column types\n",
    "\n",
    "### Solutions Implemented\n",
    "\n",
    "#### **1. Safe Utility Functions**:\n",
    "- `safe_nlargest()`: Safely gets n largest values, handling non-numeric data\n",
    "- `safe_nsmallest()`: Safely gets n smallest values, handling non-numeric data\n",
    "- `ensure_numeric_dataframe()`: Converts DataFrame columns to numeric types\n",
    "\n",
    "#### **2. Data Type Validation**:\n",
    "- **Pre-processing**: Convert all data to numeric using `pd.to_numeric()`\n",
    "- **Error Handling**: Use `errors='coerce'` to convert invalid values to NaN\n",
    "- **NaN Handling**: Fill NaN values with appropriate defaults (usually 0)\n",
    "\n",
    "#### **3. Robust Analysis Functions**:\n",
    "- **Fixed Spatial Clustering**: `perform_spatial_clustering_analysis_fixed()`\n",
    "- **Fixed Comprehensive Analysis**: `comprehensive_spatial_variable_importance_analysis_fixed()`\n",
    "- **Error Prevention**: Check data types before using `nlargest()`\n",
    "\n",
    "### Key Improvements\n",
    "\n",
    "#### **Data Type Safety**:\n",
    "```python\n",
    "# Before (error-prone)\n",
    "top_variables = cluster_mean_importance.nlargest(5)\n",
    "\n",
    "# After (safe)\n",
    "cluster_mean_importance_numeric = pd.to_numeric(cluster_mean_importance, errors='coerce')\n",
    "cluster_mean_importance_numeric = cluster_mean_importance_numeric.fillna(0)\n",
    "top_variables = cluster_mean_importance_numeric.nlargest(5)\n",
    "```\n",
    "\n",
    "#### **Error Prevention**:\n",
    "- **Type Checking**: Verify data types before operations\n",
    "- **Graceful Degradation**: Handle edge cases (empty series, all zeros)\n",
    "- **Comprehensive Testing**: Test functions with various data types\n",
    "\n",
    "#### **Robustness**:\n",
    "- **Mixed Data Handling**: Process data with mixed types safely\n",
    "- **Missing Value Management**: Handle NaN values appropriately\n",
    "- **Edge Case Handling**: Manage empty or invalid data gracefully\n",
    "\n",
    "### Usage Recommendations\n",
    "\n",
    "#### **1. Always Use Safe Functions**:\n",
    "- Use `safe_nlargest()` instead of direct `nlargest()` calls\n",
    "- Use `ensure_numeric_dataframe()` for DataFrame operations\n",
    "- Validate data types before analysis\n",
    "\n",
    "#### **2. Data Preprocessing**:\n",
    "- Convert data to appropriate types early in the pipeline\n",
    "- Handle missing values consistently\n",
    "- Validate data quality before analysis\n",
    "\n",
    "#### **3. Error Handling**:\n",
    "- Wrap operations in try-catch blocks\n",
    "- Provide meaningful error messages\n",
    "- Implement fallback strategies\n",
    "\n",
    "### Testing and Validation\n",
    "\n",
    "The fixed functions include comprehensive testing to ensure they work correctly with:\n",
    "- **Numeric Data**: Standard numeric values\n",
    "- **String Data**: Numeric values stored as strings\n",
    "- **Mixed Data**: Combinations of numeric and non-numeric values\n",
    "- **Missing Data**: NaN values and empty series\n",
    "- **Edge Cases**: Zero values, single values, empty data\n",
    "\n",
    "This ensures the analysis is robust and can handle real-world data with various quality issues and data type inconsistencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a758d15e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aciar",
   "language": "python",
   "name": "aciar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
