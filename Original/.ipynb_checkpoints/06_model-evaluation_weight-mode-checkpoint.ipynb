{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b15204-7112-48a4-97c4-bc90bc40550d",
   "metadata": {},
   "source": [
    "# Weighted MaxEnt Model Evaluation and Performance Assessment\n",
    "\n",
    "This notebook provides comprehensive evaluation of **weighted MaxEnt species distribution models**, focusing on performance assessment that accounts for sample weights and data quality differences. Unlike standard model evaluation, this version incorporates **weighted metrics** to properly assess model performance when training data has been weighted.\n",
    "\n",
    "## Key Features of Weighted Model Evaluation:\n",
    "\n",
    "### 1. **Weighted Performance Metrics**:\n",
    "- **Weighted AUC**: Area Under ROC Curve accounting for sample weights\n",
    "- **Weighted PR-AUC**: Precision-Recall AUC with weight integration\n",
    "- **Weighted Sensitivity/Specificity**: Performance metrics adjusted for data quality\n",
    "- **Weighted Precision/Recall**: Classification metrics incorporating sample weights\n",
    "\n",
    "### 2. **Advanced Evaluation Approaches**:\n",
    "- **Cross-Validation**: K-fold validation with weighted samples\n",
    "- **Spatial Validation**: Geographic partitioning with weight consideration\n",
    "- **Temporal Validation**: Time-based splits accounting for temporal weights\n",
    "- **Bootstrap Validation**: Resampling with weight preservation\n",
    "\n",
    "### 3. **Bias Assessment**:\n",
    "- **Spatial Bias Analysis**: Evaluate model performance across different regions\n",
    "- **Temporal Bias Assessment**: Performance across different time periods\n",
    "- **Source Bias Evaluation**: Performance across different data sources\n",
    "- **Quality Bias Analysis**: Performance across different data quality levels\n",
    "\n",
    "## Applications:\n",
    "- **Model Validation**: Comprehensive assessment of weighted model performance\n",
    "- **Bias Detection**: Identify remaining biases after weighting\n",
    "- **Performance Comparison**: Compare weighted vs. unweighted models\n",
    "- **Quality Control**: Validate that weighting improves model reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f4dda-d3ca-47d5-91ad-7caa0a434170",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### WEIGHTED MODEL EVALUATION CONFIGURATION - MODIFY AS NEEDED ###############\n",
    "\n",
    "# Species and region settings for weighted model evaluation\n",
    "#specie = 'leptocybe-invasa'  # Target species: 'leptocybe-invasa' or 'thaumastocoris-peregrinus'\n",
    "#pseudoabsence = 'random'  # Background point strategy: 'random', 'biased', 'biased-land-cover'\n",
    "#training = 'east-asia'  # Training region: 'sea', 'australia', 'east-asia', etc.\n",
    "#interest = 'south-east-asia'  # Test region: can be same as training or different\n",
    "#savefig = True  # Save generated evaluation plots and metrics\n",
    "\n",
    "# Environmental variable configuration\n",
    "bio = bio1  # Bioclimatic variable identifier\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e46ce-499c-4676-9ff0-f796122a3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os  # File system operations\n",
    "\n",
    "import numpy as np  # Numerical computing\n",
    "import xarray as xr  # Multi-dimensional labeled arrays (raster data)\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import geopandas as gpd  # Geospatial data handling\n",
    "\n",
    "import elapid as ela  # Species distribution modeling library\n",
    "\n",
    "from shapely import wkt  # Well-Known Text (WKT) geometry parsing\n",
    "from elapid import utils  # Utility functions for elapid\n",
    "from sklearn import metrics, inspection  # Machine learning metrics and model inspection\n",
    "\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warning messages for cleaner output\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6724e-cd4f-4099-aba5-4b81214f135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_layout(nplots):\n",
    "    \"\"\"\n",
    "    Calculate optimal subplot layout for given number of plots\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nplots : int\n",
    "        Number of plots to arrange\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ncols, nrows : tuple\n",
    "        Number of columns and rows for subplot layout\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate square root and round up for balanced layout\n",
    "    ncols = min(int(np.ceil(np.sqrt(nplots))), 4)  # Max 4 columns\n",
    "    nrows = int(np.ceil(nplots / ncols))  # Calculate rows needed\n",
    "    \n",
    "    return ncols, nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6db49b-be58-4919-b395-1e6978805f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SET UP FILE PATHS\n",
    "# =============================================================================\n",
    "# Define directory structure for organizing weighted model evaluation outputs\n",
    "\n",
    "docs_path = os.path.join(os.path.dirname(os.getcwd()), 'docs')  # Documentation directory\n",
    "out_path = os.path.join(os.path.dirname(os.getcwd()), 'out', specie)  # Species-specific output directory\n",
    "figs_path = os.path.join(os.path.dirname(os.getcwd()), 'figs')  # Figures directory\n",
    "output_path = os.path.join(out_path, 'output')  # Model output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7a900-85cd-41d4-87e6-7179c9320233",
   "metadata": {},
   "source": [
    "## 1. Weighted Training Model Performance Assessment\n",
    "\n",
    "This section evaluates the performance of the weighted MaxEnt model on the training data. Key aspects include:\n",
    "\n",
    "### **Weighted vs. Unweighted Metrics**:\n",
    "- **Standard Metrics**: Traditional AUC, PR-AUC, sensitivity, specificity\n",
    "- **Weighted Metrics**: Performance metrics accounting for sample weights\n",
    "- **Comparison Analysis**: Evaluate improvement from weighting approach\n",
    "\n",
    "### **Performance Indicators**:\n",
    "- **ROC-AUC**: Area Under Receiver Operating Characteristic curve\n",
    "- **PR-AUC**: Area Under Precision-Recall curve (important for imbalanced data)\n",
    "- **Sensitivity**: True Positive Rate (ability to detect presences)\n",
    "- **Specificity**: True Negative Rate (ability to detect absences)\n",
    "- **Precision**: Positive Predictive Value\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### **Weighted Evaluation Benefits**:\n",
    "- **Quality-Aware Assessment**: Metrics reflect data quality differences\n",
    "- **Bias-Corrected Performance**: Reduced influence of low-quality samples\n",
    "- **Robust Validation**: More reliable performance estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bf3a9-0dc5-457f-a3ff-e74c826dc08c",
   "metadata": {},
   "source": [
    "## References for Species Distribution Model Evaluation\n",
    "\n",
    "### **Model Output Interpretation**:\n",
    "- [SDM Model Outputs Interpretation](https://support.ecocommons.org.au/support/solutions/articles/6000256107-interpretation-of-sdm-model-outputs)\n",
    "- [Presence-Only Prediction in GIS](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/how-presence-only-prediction-works.htm)\n",
    "- [MaxEnt 101: Species Distribution Modeling](https://www.esri.com/arcgis-blog/products/arcgis-pro/analytics/presence-only-prediction-maxent-101-using-gis-to-model-species-distribution/)\n",
    "\n",
    "### **Performance Metrics**:\n",
    "- [ROC Curves Demystified](https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0)\n",
    "- [Precision-Recall AUC Guide](https://www.aporia.com/learn/ultimate-guide-to-precision-recall-auc-understanding-calculating-using-pr-auc-in-ml/)\n",
    "- [F1-Score, Accuracy, ROC-AUC, and PR-AUC Metrics](https://deepchecks.com/f1-score-accuracy-roc-auc-and-pr-auc-metrics-for-models/)\n",
    "\n",
    "### **Weighted Model Evaluation**:\n",
    "- **Sample Weighting**: How to properly evaluate models trained with sample weights\n",
    "- **Bias Correction**: Assessing the effectiveness of weighting strategies\n",
    "- **Quality Integration**: Incorporating data quality into performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa4635-ab08-4b3d-9fa0-07271788cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD WEIGHTED MODEL AND TRAINING DATA\n",
    "# =============================================================================\n",
    "# Load the trained weighted MaxEnt model and associated training data for evaluation\n",
    "\n",
    "# Build experiment directory name (keeps runs organized by config)\n",
    "# Alternate naming (older): 'exp_%s_%s_%s' % (pseudoabsence, training, interest)\n",
    "experiment_name = 'exp_%s_%s_%s_%s_%s' % (model_prefix, pseudoabsence, training, topo, ndvi)\n",
    "exp_path = os.path.join(output_path, experiment_name)  # Path to experiment directory\n",
    "\n",
    "# Construct expected filenames produced during training for this run\n",
    "train_input_data_name = '%s_model-train_input-data_%s_%s_%s_%s_%s.csv' % (model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "run_name = '%s_model-train_%s_%s_%s_%s_%s.ela' % (model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "nc_name = '%s_model-train_%s_%s_%s_%s_%s.nc' % (model_prefix, specie, pseudoabsence, training, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428443d1-2a5b-403e-a99c-a3395954e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD TRAINING DATA WITH SAMPLE WEIGHTS\n",
    "# =============================================================================\n",
    "# Load training data including sample weights for weighted model evaluation\n",
    "\n",
    "# Load training data from CSV file (index_col=0 to drop old index column)\n",
    "df = pd.read_csv(os.path.join(exp_path, train_input_data_name), index_col=0)\n",
    "# Parse WKT strings into shapely geometries\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# Wrap as GeoDataFrame with WGS84 CRS\n",
    "train = gpd.GeoDataFrame(df, crs='EPSG:4326')\n",
    "\n",
    "# Split predictors/labels/weights for weighted evaluation\n",
    "x_train = train.drop(columns=['class', 'SampleWeight', 'geometry'])  # Environmental variables only\n",
    "y_train = train['class']  # Presence/absence labels (0/1)\n",
    "sample_weight_train = train['SampleWeight']  # Sample weights aligned with rows\n",
    "\n",
    "# Load fitted weighted MaxEnt model\n",
    "model_train = utils.load_object(os.path.join(exp_path, run_name))\n",
    "\n",
    "# Predict probabilities on training set (for curves/metrics)\n",
    "y_train_predict = model_train.predict(x_train)\n",
    "# Optional: impute NaN probabilities to 0.5 (neutral)\n",
    "# y_train_predict = np.nan_to_num(y_train_predict, nan=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abc1a9-3db2-4960-ad8f-e042eb214fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training performance metrics\n",
    "\n",
    "# ROC curve and AUC (unweighted vs weighted)\n",
    "# fpr/tpr are computed from predicted probabilities; weights adjust contribution per sample\n",
    "fpr_train, tpr_train, thresholds = metrics.roc_curve(y_train, y_train_predict)\n",
    "auc_train = metrics.roc_auc_score(y_train, y_train_predict)\n",
    "auc_train_weighted = metrics.roc_auc_score(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "\n",
    "# Precision-Recall curve and PR-AUC (more informative on class imbalance)\n",
    "precision_train, recall_train, _ = metrics.precision_recall_curve(y_train, y_train_predict)\n",
    "pr_auc_train = metrics.auc(recall_train, precision_train)\n",
    "# Weighted PR curve uses sample weights to compute precision/recall\n",
    "precision_train_w, recall_train_w, _ = metrics.precision_recall_curve(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "pr_auc_train_weighted = metrics.auc(recall_train_w, precision_train_w)\n",
    "\n",
    "# Report metrics\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score  : {auc_train_weighted:0.3f}\")\n",
    "print(f\"PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cccf4-2b98-44d3-8725-227a49bb3c31",
   "metadata": {},
   "source": [
    "|  |  | Specie existance |  |\n",
    "| ------ | :-------: | :------: | :-------: |\n",
    "| |  | **+** | **--** |\n",
    "| **Specie observed** | **+** | True Positive (TP) | False Positive (FP) |\n",
    "| | **--** | False Negative (FN) | True Negative (TN) |\n",
    "| | | **All existing species (TP + FN)** | **All non-existing species (FP + TN)** |\n",
    "\n",
    "\n",
    "$$TPR = \\frac{TP}{TP + FN}$$\n",
    "$$FPR = \\frac{FP}{FP + TN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588ba66-5615-4d10-b8db-26ab26462e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training distributions and curves\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Left: Predicted probability distributions for presence vs pseudo-absence\n",
    "ax[0].hist(y_train_predict[y_train == 0], bins=np.linspace(0, 1, int((y_train == 0).sum() / 100 + 1)),\n",
    "           density=True, color='tab:red', alpha=0.7, label='pseudo-absence')\n",
    "ax[0].hist(y_train_predict[y_train == 1], bins=np.linspace(0, 1, int((y_train == 1).sum() / 10 + 1)),\n",
    "           density=True, color='tab:green', alpha=0.7, label='presence')\n",
    "ax[0].set_xlabel('Relative Occurrence Probability')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_title('Probability Distribution')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "# Middle: ROC curve (random vs perfect baselines + model)\n",
    "ax[1].plot([0, 1], [0, 1], '--', label='AUC score: 0.5 (No Skill)', color='gray')\n",
    "ax[1].text(0.4, 0.4, 'random classifier', fontsize=12, color='gray', rotation=45, rotation_mode='anchor',\n",
    "           horizontalalignment='left', verticalalignment='bottom', transform=ax[1].transAxes)\n",
    "ax[1].plot([0, 0, 1], [0, 1, 1], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[1].text(0, 1, '  perfect classifier', fontsize=12, color='tab:blue', horizontalalignment='left', verticalalignment='bottom')\n",
    "ax[1].scatter(0, 1, marker='*', s=100, color='tab:blue')\n",
    "# Overlay model ROC (unweighted and weighted AUC labels)\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC score: {auc_train:0.3f}', color='tab:orange')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC Weighted score: {auc_train_weighted:0.3f}', color='tab:cyan', linestyle='-.')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('MaxEnt ROC Curve')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Right: Precision-Recall curve (random/perfect baselines + model)\n",
    "ax[2].plot([0, 1], [0.5, 0.5], '--', color='gray', label='AUC score: 0.5 (No Skill)')\n",
    "ax[2].text(0.5, 0.52, 'random classifier', fontsize=12, color='gray', horizontalalignment='center', verticalalignment='center')\n",
    "ax[2].plot([0, 1, 1], [1, 1, 0], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[2].text(1, 1, 'perfect classifier  ', fontsize=12, color='tab:blue', horizontalalignment='right', verticalalignment='bottom')\n",
    "ax[2].scatter(1, 1, marker='*', s=100, color='tab:blue')\n",
    "# Overlay model PR curves (unweighted and weighted AUC labels)\n",
    "ax[2].plot(recall_train, precision_train, label=f'AUC score: {pr_auc_train:0.3f}', color='tab:orange')\n",
    "ax[2].plot(recall_train_w, precision_train_w, label=f\"AUC Weighted score: {pr_auc_train_weighted:0.3f}\", color='tab:cyan', linestyle='-.')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set_xlabel('Recall')\n",
    "ax[2].set_ylabel('Precision')\n",
    "ax[2].set_title('MaxEnt PR Curve')\n",
    "ax[2].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b30af-f85b-4419-baa9-ad808d2dfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if savefig:\n",
    "#     if Future:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s_future.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "#     else:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "\n",
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'model' variable is not null or empty\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s_%s_%s_future.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no model is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s_%s_future.png' %(specie, training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s_%s_%s.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s_%s.png' %(specie, training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993984c-f068-4953-8a73-841a09f30b72",
   "metadata": {},
   "source": [
    "## 2. Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f358d3-224d-4be8-a1d2-82f1f3457b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data_name = '%s_model-test_input-data_%s_%s_%s_%s_%s.csv' %(model_prefix, specie, pseudoabsence, interest, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7166b-a73a-441c-a066-ec8e957e04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load held-out test dataset for evaluation\n",
    "# Note: index_col=0 drops the old index saved during export\n",
    "df = pd.read_csv(os.path.join(exp_path, test_input_data_name), index_col=0)\n",
    "# Convert WKT geometry back to shapely objects\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# Wrap as GeoDataFrame (WGS84 CRS)\n",
    "test = gpd.GeoDataFrame(df, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550482cb-b0da-489c-894d-711a890cd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split predictors/labels/weights for test set\n",
    "x_test = test.drop(columns=['class', 'SampleWeight', 'geometry'])\n",
    "y_test = test['class']\n",
    "sample_weight_test = test['SampleWeight']\n",
    "\n",
    "# Predict probabilities on the test set using the trained model\n",
    "y_test_predict = model_train.predict(x_test)\n",
    "# Optional: impute NaN probabilities to 0.5 if present\n",
    "# y_test_predict = np.nan_to_num(y_test_predict, nan=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c432c6-86ef-421f-a4fe-2f1cee794e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set metrics: ROC/PR curves and AUCs (unweighted vs weighted)\n",
    "# ROC\n",
    "fpr_test, tpr_test, _ = metrics.roc_curve(y_test, y_test_predict)\n",
    "auc_test = metrics.roc_auc_score(y_test, y_test_predict)\n",
    "auc_test_weighted = metrics.roc_auc_score(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "\n",
    "# Precision-Recall (PR)\n",
    "precision_test, recall_test, _ = metrics.precision_recall_curve(y_test, y_test_predict)\n",
    "pr_auc_test = metrics.auc(recall_test, precision_test)\n",
    "precision_test_w, recall_test_w, _ = metrics.precision_recall_curve(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "pr_auc_test_weighted = metrics.auc(recall_test_w, precision_test_w)\n",
    "\n",
    "# Print summary of training vs test for quick comparison\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score: {auc_train_weighted:0.3f}\")\n",
    "print(f\"Test ROC-AUC score: {auc_test:0.3f}\")\n",
    "print(f\"Test ROC-AUC Weighted score: {auc_test_weighted:0.3f}\")\n",
    "\n",
    "print(f\"Training PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"Training PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")\n",
    "print(f\"Test PR-AUC Score: {pr_auc_test:0.3f}\")\n",
    "print(f\"Test PR-AUC Weighted Score: {pr_auc_test_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22fa49-a574-4a0f-b998-4028ab09cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test distributions and curves alongside training for comparison\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Left: Predicted probability distributions on test set\n",
    "ax[0].hist(y_test_predict[y_test == 0], bins=np.linspace(0, 1, int((y_test == 0).sum() / 100 + 1)),\n",
    "           density=True, color='tab:red', alpha=0.7, label='pseudo-absence')\n",
    "ax[0].hist(y_test_predict[y_test == 1], bins=np.linspace(0, 1, int((y_test == 1).sum() / 10 + 1)),\n",
    "           density=True, color='tab:green', alpha=0.7, label='presence')\n",
    "ax[0].set_xlabel('Relative Occurrence Probability')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_title('Probability Distribution')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "# Middle: ROC curves (train vs test, with weighted variants labeled)\n",
    "ax[1].plot([0, 1], [0, 1], '--', label='AUC score: 0.5 (No Skill)', color='gray')\n",
    "ax[1].text(0.4, 0.4, 'random classifier', fontsize=12, color='gray', rotation=45, rotation_mode='anchor',\n",
    "           horizontalalignment='left', verticalalignment='bottom', transform=ax[1].transAxes)\n",
    "ax[1].plot([0, 0, 1], [0, 1, 1], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[1].text(0, 1, '  perfect classifier', fontsize=12, color='tab:blue', horizontalalignment='left', verticalalignment='bottom')\n",
    "ax[1].scatter(0, 1, marker='*', s=100, color='tab:blue')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC train score: {auc_train:0.3f}', color='tab:orange')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC Weighted train score: {auc_train_weighted:0.3f}', color='tab:cyan', linestyle='-.')\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'AUC test score: {auc_test:0.3f}', color='tab:green')\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'AUC Weighted test score: {auc_test_weighted:0.3f}', color='tab:olive', linestyle='-.')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('MaxEnt ROC Curve')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Right: PR curves (train vs test)\n",
    "ax[2].plot([0, 1], [0.5, 0.5], '--', color='gray', label='AUC score: 0.5 (No Skill)')\n",
    "ax[2].text(0.5, 0.52, 'random classifier', fontsize=12, color='gray', horizontalalignment='center', verticalalignment='center')\n",
    "ax[2].plot([0, 1, 1], [1, 1, 0], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[2].text(1, 1, 'perfect classifier  ', fontsize=12, color='tab:blue', horizontalalignment='right', verticalalignment='bottom')\n",
    "ax[2].scatter(1, 1, marker='*', s=100, color='tab:blue')\n",
    "ax[2].plot(recall_train, precision_train, label=f'AUC train score: {pr_auc_train:0.3f}', color='tab:orange')\n",
    "ax[2].plot(recall_train_w, precision_train_w, label=f\"AUC train Weighted score: {pr_auc_train_weighted:0.3f}\", color='tab:cyan', linestyle='-.')\n",
    "ax[2].plot(recall_test, precision_test, label=f'AUC test score: {pr_auc_test:0.3f}', color='tab:green')\n",
    "ax[2].plot(recall_test_w, precision_test_w, label=f'AUC test Weighted score: {pr_auc_test_weighted:0.3f}', color='tab:olive', linestyle='-.')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set_xlabel('Recall')\n",
    "ax[2].set_ylabel('Precision')\n",
    "ax[2].set_title('MaxEnt PR Curve')\n",
    "ax[2].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b39edc-a114-46ba-a329-503387ab4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if savefig:\n",
    "#     if Future:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s_future.png' %(specie, interest, bio)), transparent=True, bbox_inches='tight')\n",
    "#     else:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s.png' %(specie, interest, bio)), transparent=True, bbox_inches='tight')\n",
    "\n",
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'model' variable is not null or empty\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s_%s_%s_future.png' %(specie, interest, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no model is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s_%s_future.png' %(specie, interest, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if model_prefix:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s_%s_%s.png' %(specie, interest, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '06_roc-pr-auc_%s_%s_%s_%s.png' %(specie, interest, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bfd716-825b-4ce8-82aa-fce7f973fcf8",
   "metadata": {},
   "source": [
    "## 3. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6a70d-105b-4267-bc58-1df7bb8e8aab",
   "metadata": {},
   "source": [
    "### 3.2 Partial dependence plot/ Response curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c51647-a6b0-4329-a077-fb2d3d5aca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = model_train.partial_dependence_plot(x, labels=labels, dpi=100, n_bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a733f05-26cb-4da3-a95d-7adc42870020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels and open training output NetCDF for metadata\n",
    "labels = train.drop(columns=['class', 'geometry', 'SampleWeight']).columns.values\n",
    "training_output = xr.open_dataset(os.path.join(exp_path, nc_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d9454-dce3-4b7d-922b-8f84e37f1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute partial dependence across features\n",
    "# - percentiles bounds the feature grid to observed range (2.5% to 97.5%)\n",
    "# - nbins controls resolution of the curve\n",
    "percentiles = (0.025, 0.975)\n",
    "nbins = 100\n",
    "\n",
    "mean = {}\n",
    "stdv = {}\n",
    "bins = {}\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    # Request individual PDP curves across samples, then summarize\n",
    "    pda = inspection.partial_dependence(\n",
    "        model_train,\n",
    "        x_train,\n",
    "        [idx],\n",
    "        percentiles=percentiles,\n",
    "        grid_resolution=nbins,\n",
    "        kind=\"individual\",\n",
    "    )\n",
    "\n",
    "    mean[label] = pda[\"individual\"][0].mean(axis=0)  # average response\n",
    "    stdv[label] = pda[\"individual\"][0].std(axis=0)   # variability across samples\n",
    "    bins[label] = pda[\"grid_values\"][0]              # feature grid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890d37e-af5a-4f15-966a-02bde1a1f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDPs with uncertainty bands for each predictor\n",
    "ncols, nrows = subplot_layout(len(labels))\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 6, nrows * 6))\n",
    "\n",
    "# Normalize axes list for consistent indexing\n",
    "if (nrows, ncols) == (1, 1):\n",
    "    ax = [axs]\n",
    "else:\n",
    "    ax = axs.ravel()\n",
    "\n",
    "xlabels = training_output.data_vars\n",
    "for iax, label in enumerate(labels):\n",
    "    ax[iax].set_title(label)\n",
    "    try:\n",
    "        ax[iax].set_xlabel(xlabels[label].long_name)\n",
    "    except (ValueError, AttributeError):\n",
    "        ax[iax].set_xlabel('No variable long_name')\n",
    "\n",
    "    # Uncertainty band: mean Â± std across individuals\n",
    "    ax[iax].fill_between(bins[label], mean[label] - stdv[label], mean[label] + stdv[label], alpha=0.25)\n",
    "    ax[iax].plot(bins[label], mean[label])\n",
    "\n",
    "# Style axes\n",
    "for axi in ax:\n",
    "    axi.set_ylim([0, 1])\n",
    "    axi.set_ylabel('probability of occurrence')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2d9fa-5f37-48b4-a03c-9f53f244b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if savefig:\n",
    "#     if Future:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_resp-curves_%s_%s_%s_future.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "#     else:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_resp-curves_%s_%s_%s.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'model' variable is not null or empty\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_resp-curves_%s_%s_%s_%s_%s_future.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no model is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '06_resp-curves_%s_%s_%s_%s_future.png' %(specie, training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_resp-curves_%s_%s_%s_%s_%s.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '06_resp-curves_%s_%s_%s_%s.png' %(specie, training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f47155-3512-4f32-9128-9595e1709c6a",
   "metadata": {},
   "source": [
    "### 3.3 Variable importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4362f3-93b0-4626-a37b-bf36a36900dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance: measures drop in performance when each feature is shuffled\n",
    "# Higher drop => more important feature\n",
    "pi = inspection.permutation_importance(model_train, x_train, y_train, n_repeats=10)\n",
    "importance = pi.importances\n",
    "rank_order = importance.mean(axis=-1).argsort()\n",
    "\n",
    "# # Collect results into a DataFrame\n",
    "# results_df = pd.DataFrame({\n",
    "#     \"feature\": x_train.columns,\n",
    "#     \"importance_mean\": pi.importances_mean,\n",
    "#     \"importance_std\": pi.importances_std,\n",
    "#     \"importance_rank\": np.argsort(-pi.importances_mean)  # negative for descending rank\n",
    "# })\n",
    "\n",
    "# # Sort by rank (highest importance first)\n",
    "# results_df = results_df.sort_values(\"importance_mean\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# # Save to CSV\n",
    "# results_df.to_csv(os.path.join(figs_path, \"permutation_importance_results_%s_%s_%s_%s_%s.csv\" %(specie, training, bio, model_prefix, iteration)), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301c544-3a98-4bac-ab55-d0c07373f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation importances as horizontal boxplots (distribution over repeats)\n",
    "labels_ranked = [labels[idx] for idx in rank_order]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "box = ax.boxplot(importance[rank_order].T, vert=False, labels=labels_ranked)\n",
    "# Decorate legend labels for key boxplot elements\n",
    "box['fliers'][0].set_label('outlier')\n",
    "box['medians'][0].set_label('median')\n",
    "for icap, cap in enumerate(box['caps']):\n",
    "    if icap == 0:\n",
    "        cap.set_label('min-max')\n",
    "    cap.set_color('k')\n",
    "    cap.set_linewidth(2)\n",
    "for ibx, bx in enumerate(box['boxes']):\n",
    "    if ibx == 0:\n",
    "        bx.set_label('25-75%')\n",
    "    bx.set_color('gray')\n",
    "\n",
    "ax.set_xlabel('Importance')\n",
    "ax.legend(loc='lower right')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cc671-1f8a-4375-9af2-8286603483ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if savefig:\n",
    "#     if Future:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_var-importance_%s_%s_%s_future.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "#     else:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_var-importance_%s_%s_%s.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'model' variable is not null or empty\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_%s_future.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no model is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_future.png' %(specie, training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_%s.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s.png' %(specie, training, bio,iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aciar",
   "language": "python",
   "name": "aciar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
