{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a39a75e",
   "metadata": {},
   "source": [
    "# Model Evaluation (Weighted) â€“ Notebook Guide\n",
    "\n",
    "This notebook evaluates models with class/observation weights applied.\n",
    "\n",
    "## What this notebook does\n",
    "- Compute weighted metrics (e.g., weighted AUC, threshold metrics)\n",
    "- Plot diagnostic figures considering weights\n",
    "- Summarize results per model/run and export\n",
    "\n",
    "## Inputs\n",
    "- Predictions/scores, ground-truth labels, and weights per observation\n",
    "- Optional: CV fold info or test set indicators\n",
    "\n",
    "## Workflow\n",
    "1. Load predictions, labels, and weights\n",
    "2. Validate alignment and handle missing values\n",
    "3. Compute weighted metrics across thresholds/folds\n",
    "4. Plot weighted ROC/curves and summaries\n",
    "5. Save metrics tables and figures\n",
    "\n",
    "## Outputs\n",
    "- Weighted per-model/per-fold metrics tables\n",
    "- Plots reflecting weights\n",
    "- CSV/JSON exports for downstream use\n",
    "\n",
    "## Notes\n",
    "- Ensure weights are normalized or in intended scale\n",
    "- Use consistent preprocessing as training\n",
    "- Fix random seeds for reproducibility where applicable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce62eb",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook evaluates weighted SDMs with metrics and plots, mirroring standard evaluation but accounting for weights in analysis where relevant.\n",
    "\n",
    "- Key steps: load weighted predictions, compute metrics, plot curves, thresholds, reporting\n",
    "- Inputs: weighted model predictions and labels\n",
    "- Outputs: evaluation tables and plots\n",
    "- Run order: After weighted model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b15204-7112-48a4-97c4-bc90bc40550d",
   "metadata": {},
   "source": [
    "# Weighted MaxEnt Model Evaluation and Performance Assessment\n",
    "\n",
    "This notebook provides comprehensive evaluation of **weighted MaxEnt species distribution models**, focusing on performance assessment that accounts for sample weights and data quality differences. Unlike standard model evaluation, this version incorporates **weighted metrics** to properly assess model performance when training data has been weighted.\n",
    "\n",
    "## Key Features of Weighted Model Evaluation:\n",
    "\n",
    "### 1. **Weighted Performance Metrics**:\n",
    "- **Weighted AUC**: Area Under ROC Curve accounting for sample weights\n",
    "- **Weighted PR-AUC**: Precision-Recall AUC with weight integration\n",
    "- **Weighted Sensitivity/Specificity**: Performance metrics adjusted for data quality\n",
    "- **Weighted Precision/Recall**: Classification metrics incorporating sample weights\n",
    "\n",
    "### 2. **Advanced Evaluation Approaches**:\n",
    "- **Cross-Validation**: K-fold validation with weighted samples\n",
    "- **Spatial Validation**: Geographic partitioning with weight consideration\n",
    "- **Temporal Validation**: Time-based splits accounting for temporal weights\n",
    "- **Bootstrap Validation**: Resampling with weight preservation\n",
    "\n",
    "### 3. **Bias Assessment**:\n",
    "- **Spatial Bias Analysis**: Evaluate model performance across different regions\n",
    "- **Temporal Bias Assessment**: Performance across different time periods\n",
    "- **Source Bias Evaluation**: Performance across different data sources\n",
    "- **Quality Bias Analysis**: Performance across different data quality levels\n",
    "\n",
    "## Applications:\n",
    "- **Model Validation**: Comprehensive assessment of weighted model performance\n",
    "- **Bias Detection**: Identify remaining biases after weighting\n",
    "- **Performance Comparison**: Compare weighted vs. unweighted models\n",
    "- **Quality Control**: Validate that weighting improves model reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087f4dda-d3ca-47d5-91ad-7caa0a434170",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bio1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m############### WEIGHTED MODEL EVALUATION CONFIGURATION - MODIFY AS NEEDED ###############\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Species and region settings for weighted model evaluation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Environmental variable configuration\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m bio \u001b[38;5;241m=\u001b[39m bio1\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bio1' is not defined"
     ]
    }
   ],
   "source": [
    "############### WEIGHTED MODEL EVALUATION CONFIGURATION - MODIFY AS NEEDED ###############\n",
    "\n",
    "# Species and region settings for weighted model evaluation\n",
    "#specie = 'leptocybe-invasa'  # Target species: 'leptocybe-invasa' or 'thaumastocoris-peregrinus'\n",
    "#pseudoabsence = 'random'  # Background point strategy: 'random', 'biased', 'biased-land-cover'\n",
    "#training = 'east-asia'  # Training region: 'sea', 'australia', 'east-asia', etc.\n",
    "#interest = 'south-east-asia'  # Test region: can be same as training or different\n",
    "#savefig = True  # Save generated evaluation plots and metrics\n",
    "\n",
    "# Environmental variable configuration\n",
    "bio = bio1  # Bioclimatic variable identifier\n",
    "\n",
    "# Evaluation settings (specific to weighted model evaluation)\n",
    "# evaluation_method = 'cross_validation'  # 'cross_validation', 'spatial_validation', 'temporal_validation'\n",
    "# n_folds = 5  # Number of folds for cross-validation\n",
    "# spatial_buffer = 100  # Buffer distance (km) for spatial validation\n",
    "# temporal_split = 0.7  # Proportion of data for training in temporal validation\n",
    "\n",
    "# Weighted metrics configuration\n",
    "# include_weighted_metrics = True  # Calculate weighted performance metrics\n",
    "# include_unweighted_metrics = True  # Calculate standard metrics for comparison\n",
    "# weight_threshold = 0.1  # Minimum weight threshold for sample inclusion\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e46ce-499c-4676-9ff0-f796122a3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os  # File system operations\n",
    "\n",
    "import numpy as np  # Numerical computing\n",
    "import xarray as xr  # Multi-dimensional labeled arrays (raster data)\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import geopandas as gpd  # Geospatial data handling\n",
    "\n",
    "import elapid as ela  # Species distribution modeling library\n",
    "\n",
    "from shapely import wkt  # Well-Known Text (WKT) geometry parsing\n",
    "from elapid import utils  # Utility functions for elapid\n",
    "from sklearn import metrics, inspection  # Machine learning metrics and model inspection\n",
    "\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warning messages for cleaner output\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6724e-cd4f-4099-aba5-4b81214f135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_layout(nplots):\n",
    "    \"\"\"\n",
    "    Calculate optimal subplot layout for given number of plots\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nplots : int\n",
    "        Number of plots to arrange\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ncols, nrows : tuple\n",
    "        Number of columns and rows for subplot layout\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate square root and round up for balanced layout\n",
    "    ncols = min(int(np.ceil(np.sqrt(nplots))), 4)  # Max 4 columns\n",
    "    nrows = int(np.ceil(nplots / ncols))  # Calculate rows needed\n",
    "    \n",
    "    return ncols, nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6db49b-be58-4919-b395-1e6978805f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SET UP FILE PATHS\n",
    "# =============================================================================\n",
    "# Define directory structure for organizing weighted model evaluation outputs\n",
    "\n",
    "docs_path = os.path.join(os.path.dirname(os.getcwd()), 'docs')  # Documentation directory\n",
    "out_path = os.path.join(os.path.dirname(os.getcwd()), 'out', specie)  # Species-specific output directory\n",
    "figs_path = os.path.join(os.path.dirname(os.getcwd()), 'figs')  # Figures directory\n",
    "output_path = os.path.join(out_path, 'output')  # Model output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7a900-85cd-41d4-87e6-7179c9320233",
   "metadata": {},
   "source": [
    "## 1. Weighted Training Model Performance Assessment\n",
    "\n",
    "This section evaluates the performance of the weighted MaxEnt model on the training data. Key aspects include:\n",
    "\n",
    "### **Weighted vs. Unweighted Metrics**:\n",
    "- **Standard Metrics**: Traditional AUC, PR-AUC, sensitivity, specificity\n",
    "- **Weighted Metrics**: Performance metrics accounting for sample weights\n",
    "- **Comparison Analysis**: Evaluate improvement from weighting approach\n",
    "\n",
    "### **Performance Indicators**:\n",
    "- **ROC-AUC**: Area Under Receiver Operating Characteristic curve\n",
    "- **PR-AUC**: Area Under Precision-Recall curve (important for imbalanced data)\n",
    "- **Sensitivity**: True Positive Rate (ability to detect presences)\n",
    "- **Specificity**: True Negative Rate (ability to detect absences)\n",
    "- **Precision**: Positive Predictive Value\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### **Weighted Evaluation Benefits**:\n",
    "- **Quality-Aware Assessment**: Metrics reflect data quality differences\n",
    "- **Bias-Corrected Performance**: Reduced influence of low-quality samples\n",
    "- **Robust Validation**: More reliable performance estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945bf2a-bbae-4122-8195-001b259c1d1d",
   "metadata": {},
   "source": [
    "## References for Species Distribution Model Evaluation\n",
    "\n",
    "### **Model Output Interpretation**:\n",
    "- [SDM Model Outputs Interpretation](https://support.ecocommons.org.au/support/solutions/articles/6000256107-interpretation-of-sdm-model-outputs)\n",
    "- [Presence-Only Prediction in GIS](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/how-presence-only-prediction-works.htm)\n",
    "- [MaxEnt 101: Species Distribution Modeling](https://www.esri.com/arcgis-blog/products/arcgis-pro/analytics/presence-only-prediction-maxent-101-using-gis-to-model-species-distribution/)\n",
    "\n",
    "### **Performance Metrics**:\n",
    "- [ROC Curves Demystified](https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0)\n",
    "- [Precision-Recall AUC Guide](https://www.aporia.com/learn/ultimate-guide-to-precision-recall-auc-understanding-calculating-using-pr-auc-in-ml/)\n",
    "- [F1-Score, Accuracy, ROC-AUC, and PR-AUC Metrics](https://deepchecks.com/f1-score-accuracy-roc-auc-and-pr-auc-metrics-for-models/)\n",
    "\n",
    "### **Weighted Model Evaluation**:\n",
    "- **Sample Weighting**: How to properly evaluate models trained with sample weights\n",
    "- **Bias Correction**: Assessing the effectiveness of weighting strategies\n",
    "- **Quality Integration**: Incorporating data quality into performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa4635-ab08-4b3d-9fa0-07271788cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD WEIGHTED MODEL AND TRAINING DATA\n",
    "# =============================================================================\n",
    "# Load the trained weighted MaxEnt model and associated training data for evaluation\n",
    "\n",
    "# Build experiment directory name (keeps runs organized by config)\n",
    "# Alternate naming (older): 'exp_%s_%s_%s' % (pseudoabsence, training, interest)\n",
    "experiment_name = 'exp_%s_%s_%s_%s_%s' % (model_prefix, pseudoabsence, training, topo, ndvi)\n",
    "exp_path = os.path.join(output_path, experiment_name)  # Path to experiment directory\n",
    "\n",
    "# Construct expected filenames produced during training for this run\n",
    "train_input_data_name = '%s_model-train_input-data_%s_%s_%s_%s_%s.csv' % (model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "run_name = '%s_model-train_%s_%s_%s_%s_%s.ela' % (model_prefix, specie, pseudoabsence, training, bio, iteration)\n",
    "nc_name = '%s_model-train_%s_%s_%s_%s_%s.nc' % (model_prefix, specie, pseudoabsence, training, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428443d1-2a5b-403e-a99c-a3395954e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD TRAINING DATA WITH SAMPLE WEIGHTS\n",
    "# =============================================================================\n",
    "# Load training data including sample weights for weighted model evaluation\n",
    "\n",
    "# Load training data from CSV file (index_col=0 to drop old index column)\n",
    "df = pd.read_csv(os.path.join(exp_path, train_input_data_name), index_col=0)\n",
    "# Parse WKT strings into shapely geometries\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# Wrap as GeoDataFrame with WGS84 CRS\n",
    "train = gpd.GeoDataFrame(df, crs='EPSG:4326')\n",
    "\n",
    "# Split predictors/labels/weights for weighted evaluation\n",
    "x_train = train.drop(columns=['class', 'SampleWeight', 'geometry'])  # Environmental variables only\n",
    "y_train = train['class']  # Presence/absence labels (0/1)\n",
    "sample_weight_train = train['SampleWeight']  # Sample weights aligned with rows\n",
    "\n",
    "# Load fitted weighted MaxEnt model\n",
    "model_train = utils.load_object(os.path.join(exp_path, run_name))\n",
    "\n",
    "# Predict probabilities on training set (for curves/metrics)\n",
    "y_train_predict = model_train.predict(x_train)\n",
    "# Optional: impute NaN probabilities to 0.5 (neutral)\n",
    "# y_train_predict = np.nan_to_num(y_train_predict, nan=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abc1a9-3db2-4960-ad8f-e042eb214fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training performance metrics\n",
    "\n",
    "# ROC curve and AUC (unweighted vs weighted)\n",
    "# fpr/tpr are computed from predicted probabilities; weights adjust contribution per sample\n",
    "fpr_train, tpr_train, thresholds = metrics.roc_curve(y_train, y_train_predict)\n",
    "auc_train = metrics.roc_auc_score(y_train, y_train_predict)\n",
    "auc_train_weighted = metrics.roc_auc_score(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "\n",
    "# Precision-Recall curve and PR-AUC (more informative on class imbalance)\n",
    "precision_train, recall_train, _ = metrics.precision_recall_curve(y_train, y_train_predict)\n",
    "pr_auc_train = metrics.auc(recall_train, precision_train)\n",
    "# Weighted PR curve uses sample weights to compute precision/recall\n",
    "precision_train_w, recall_train_w, _ = metrics.precision_recall_curve(y_train, y_train_predict, sample_weight=sample_weight_train)\n",
    "pr_auc_train_weighted = metrics.auc(recall_train_w, precision_train_w)\n",
    "\n",
    "# Report metrics\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score  : {auc_train_weighted:0.3f}\")\n",
    "print(f\"PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cccf4-2b98-44d3-8725-227a49bb3c31",
   "metadata": {},
   "source": [
    "|  |  | Specie existance |  |\n",
    "| ------ | :-------: | :------: | :-------: |\n",
    "| |  | **+** | **--** |\n",
    "| **Specie observed** | **+** | True Positive (TP) | False Positive (FP) |\n",
    "| | **--** | False Negative (FN) | True Negative (TN) |\n",
    "| | | **All existing species (TP + FN)** | **All non-existing species (FP + TN)** |\n",
    "\n",
    "\n",
    "$$TPR = \\frac{TP}{TP + FN}$$\n",
    "$$FPR = \\frac{FP}{FP + TN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588ba66-5615-4d10-b8db-26ab26462e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training distributions and curves\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Left: Predicted probability distributions for presence vs pseudo-absence\n",
    "ax[0].hist(y_train_predict[y_train == 0], bins=np.linspace(0, 1, int((y_train == 0).sum() / 100 + 1)),\n",
    "           density=True, color='tab:red', alpha=0.7, label='pseudo-absence')\n",
    "ax[0].hist(y_train_predict[y_train == 1], bins=np.linspace(0, 1, int((y_train == 1).sum() / 10 + 1)),\n",
    "           density=True, color='tab:green', alpha=0.7, label='presence')\n",
    "ax[0].set_xlabel('Relative Occurrence Probability')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_title('Probability Distribution')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "# Middle: ROC curve (random vs perfect baselines + model)\n",
    "ax[1].plot([0, 1], [0, 1], '--', label='AUC score: 0.5 (No Skill)', color='gray')\n",
    "ax[1].text(0.4, 0.4, 'random classifier', fontsize=12, color='gray', rotation=45, rotation_mode='anchor',\n",
    "           horizontalalignment='left', verticalalignment='bottom', transform=ax[1].transAxes)\n",
    "ax[1].plot([0, 0, 1], [0, 1, 1], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[1].text(0, 1, '  perfect classifier', fontsize=12, color='tab:blue', horizontalalignment='left', verticalalignment='bottom')\n",
    "ax[1].scatter(0, 1, marker='*', s=100, color='tab:blue')\n",
    "# Overlay model ROC (unweighted and weighted AUC labels)\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC score: {auc_train:0.3f}', color='tab:orange')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC Weighted score: {auc_train_weighted:0.3f}', color='tab:cyan', linestyle='-.')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('MaxEnt ROC Curve')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Right: Precision-Recall curve (random/perfect baselines + model)\n",
    "ax[2].plot([0, 1], [0.5, 0.5], '--', color='gray', label='AUC score: 0.5 (No Skill)')\n",
    "ax[2].text(0.5, 0.52, 'random classifier', fontsize=12, color='gray', horizontalalignment='center', verticalalignment='center')\n",
    "ax[2].plot([0, 1, 1], [1, 1, 0], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[2].text(1, 1, 'perfect classifier  ', fontsize=12, color='tab:blue', horizontalalignment='right', verticalalignment='bottom')\n",
    "ax[2].scatter(1, 1, marker='*', s=100, color='tab:blue')\n",
    "# Overlay model PR curves (unweighted and weighted AUC labels)\n",
    "ax[2].plot(recall_train, precision_train, label=f'AUC score: {pr_auc_train:0.3f}', color='tab:orange')\n",
    "ax[2].plot(recall_train_w, precision_train_w, label=f\"AUC Weighted score: {pr_auc_train_weighted:0.3f}\", color='tab:cyan', linestyle='-.')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set_xlabel('Recall')\n",
    "ax[2].set_ylabel('Precision')\n",
    "ax[2].set_title('MaxEnt PR Curve')\n",
    "ax[2].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b30af-f85b-4419-baa9-ad808d2dfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figures if requested. Uses different filename patterns for current vs future scenarios.\n",
    "# Note: 'models' is used to gate inclusion of model prefix; ensure it exists in your session.\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:  # include model identifier when available\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: omit model prefix when not specified\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993984c-f068-4953-8a73-841a09f30b72",
   "metadata": {},
   "source": [
    "## 2. Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f358d3-224d-4be8-a1d2-82f1f3457b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data_name = '%s_model-test_input-data_%s_%s_%s_%s_%s.csv' %(model_prefix, specie, pseudoabsence, interest, bio, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7166b-a73a-441c-a066-ec8e957e04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load held-out test dataset for evaluation\n",
    "# Note: index_col=0 drops the old index saved during export\n",
    "df = pd.read_csv(os.path.join(exp_path, test_input_data_name), index_col=0)\n",
    "# Convert WKT geometry back to shapely objects\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "# Wrap as GeoDataFrame (WGS84 CRS)\n",
    "test = gpd.GeoDataFrame(df, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550482cb-b0da-489c-894d-711a890cd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split predictors/labels/weights for test set\n",
    "x_test = test.drop(columns=['class', 'SampleWeight', 'geometry'])\n",
    "y_test = test['class']\n",
    "sample_weight_test = test['SampleWeight']\n",
    "\n",
    "# Predict probabilities on the test set using the trained model\n",
    "y_test_predict = model_train.predict(x_test)\n",
    "# Optional: impute NaN probabilities to 0.5 if present\n",
    "# y_test_predict = np.nan_to_num(y_test_predict, nan=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c432c6-86ef-421f-a4fe-2f1cee794e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set metrics: ROC/PR curves and AUCs (unweighted vs weighted)\n",
    "# ROC\n",
    "fpr_test, tpr_test, _ = metrics.roc_curve(y_test, y_test_predict)\n",
    "auc_test = metrics.roc_auc_score(y_test, y_test_predict)\n",
    "auc_test_weighted = metrics.roc_auc_score(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "\n",
    "# Precision-Recall (PR)\n",
    "precision_test, recall_test, _ = metrics.precision_recall_curve(y_test, y_test_predict)\n",
    "pr_auc_test = metrics.auc(recall_test, precision_test)\n",
    "precision_test_w, recall_test_w, _ = metrics.precision_recall_curve(y_test, y_test_predict, sample_weight=sample_weight_test)\n",
    "pr_auc_test_weighted = metrics.auc(recall_test_w, precision_test_w)\n",
    "\n",
    "# Print summary of training vs test for quick comparison\n",
    "print(f\"Training ROC-AUC score: {auc_train:0.3f}\")\n",
    "print(f\"Training ROC-AUC Weighted score: {auc_train_weighted:0.3f}\")\n",
    "print(f\"Test ROC-AUC score: {auc_test:0.3f}\")\n",
    "print(f\"Test ROC-AUC Weighted score: {auc_test_weighted:0.3f}\")\n",
    "\n",
    "print(f\"Training PR-AUC Score: {pr_auc_train:0.3f}\")\n",
    "print(f\"Training PR-AUC Weighted Score: {pr_auc_train_weighted:0.3f}\")\n",
    "print(f\"Test PR-AUC Score: {pr_auc_test:0.3f}\")\n",
    "print(f\"Test PR-AUC Weighted Score: {pr_auc_test_weighted:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22fa49-a574-4a0f-b998-4028ab09cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test distributions and curves alongside training for comparison\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# Left: Predicted probability distributions on test set\n",
    "ax[0].hist(y_test_predict[y_test == 0], bins=np.linspace(0, 1, int((y_test == 0).sum() / 100 + 1)),\n",
    "           density=True, color='tab:red', alpha=0.7, label='pseudo-absence')\n",
    "ax[0].hist(y_test_predict[y_test == 1], bins=np.linspace(0, 1, int((y_test == 1).sum() / 10 + 1)),\n",
    "           density=True, color='tab:green', alpha=0.7, label='presence')\n",
    "ax[0].set_xlabel('Relative Occurrence Probability')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_title('Probability Distribution')\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "# Middle: ROC curves (train vs test, with weighted variants labeled)\n",
    "ax[1].plot([0, 1], [0, 1], '--', label='AUC score: 0.5 (No Skill)', color='gray')\n",
    "ax[1].text(0.4, 0.4, 'random classifier', fontsize=12, color='gray', rotation=45, rotation_mode='anchor',\n",
    "           horizontalalignment='left', verticalalignment='bottom', transform=ax[1].transAxes)\n",
    "ax[1].plot([0, 0, 1], [0, 1, 1], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[1].text(0, 1, '  perfect classifier', fontsize=12, color='tab:blue', horizontalalignment='left', verticalalignment='bottom')\n",
    "ax[1].scatter(0, 1, marker='*', s=100, color='tab:blue')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC train score: {auc_train:0.3f}', color='tab:orange')\n",
    "ax[1].plot(fpr_train, tpr_train, label=f'AUC Weighted train score: {auc_train_weighted:0.3f}', color='tab:cyan', linestyle='-.')\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'AUC test score: {auc_test:0.3f}', color='tab:green')\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'AUC Weighted test score: {auc_test_weighted:0.3f}', color='tab:olive', linestyle='-.')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('MaxEnt ROC Curve')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Right: PR curves (train vs test)\n",
    "ax[2].plot([0, 1], [0.5, 0.5], '--', color='gray', label='AUC score: 0.5 (No Skill)')\n",
    "ax[2].text(0.5, 0.52, 'random classifier', fontsize=12, color='gray', horizontalalignment='center', verticalalignment='center')\n",
    "ax[2].plot([0, 1, 1], [1, 1, 0], '--', label='AUC score: 1 (Ideal Model)', color='tab:blue', zorder=-1)\n",
    "ax[2].text(1, 1, 'perfect classifier  ', fontsize=12, color='tab:blue', horizontalalignment='right', verticalalignment='bottom')\n",
    "ax[2].scatter(1, 1, marker='*', s=100, color='tab:blue')\n",
    "ax[2].plot(recall_train, precision_train, label=f'AUC train score: {pr_auc_train:0.3f}', color='tab:orange')\n",
    "ax[2].plot(recall_train_w, precision_train_w, label=f\"AUC train Weighted score: {pr_auc_train_weighted:0.3f}\", color='tab:cyan', linestyle='-.')\n",
    "ax[2].plot(recall_test, precision_test, label=f'AUC test score: {pr_auc_test:0.3f}', color='tab:green')\n",
    "ax[2].plot(recall_test_w, precision_test_w, label=f'AUC test Weighted score: {pr_auc_test_weighted:0.3f}', color='tab:olive', linestyle='-.')\n",
    "ax[2].axis('equal')\n",
    "ax[2].set_xlabel('Recall')\n",
    "ax[2].set_ylabel('Precision')\n",
    "ax[2].set_title('MaxEnt PR Curve')\n",
    "ax[2].legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b39edc-a114-46ba-a329-503387ab4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test figures if requested (future vs current naming handled similarly to training)\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s_future.png' % (specie, interest, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_future.png' % (specie, interest, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if model_prefix:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s_%s.png' % (specie, interest, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_roc-pr-auc_%s_%s_%s_%s.png' % (specie, interest, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bfd716-825b-4ce8-82aa-fce7f973fcf8",
   "metadata": {},
   "source": [
    "## 3. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6a70d-105b-4267-bc58-1df7bb8e8aab",
   "metadata": {},
   "source": [
    "### 3.2 Partial dependence plot/ Response curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c51647-a6b0-4329-a077-fb2d3d5aca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = model_train.partial_dependence_plot(x, labels=labels, dpi=100, n_bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeacf3a",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Variable Importance Analysis\n",
    "\n",
    "This section performs a thorough analysis of variable importance by:\n",
    "\n",
    "1. **Initial Analysis**: Running the model with all 19 bioclimatic variables to establish baseline importance\n",
    "2. **Iterative Removal**: Systematically removing the least important variables until we reach ~5 most important variables\n",
    "3. **Performance Tracking**: Monitoring model performance as variables are removed\n",
    "4. **Final Recommendations**: Identifying the optimal subset of variables for the species distribution model\n",
    "\n",
    "### Methodology:\n",
    "- **Permutation Importance**: Measures the drop in model performance when each variable is randomly shuffled\n",
    "- **Iterative Backward Elimination**: Removes least important variables one at a time\n",
    "- **Performance Monitoring**: Tracks AUC, PR-AUC, and other metrics throughout the process\n",
    "- **Cross-Validation**: Ensures robust importance estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b286503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE VARIABLE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Initialize storage for results\n",
    "importance_results = {}\n",
    "performance_history = {}\n",
    "variable_subsets = {}\n",
    "\n",
    "# Get current variable names from training data\n",
    "current_variables = list(x_train.columns)\n",
    "print(f\"Starting with {len(current_variables)} variables:\")\n",
    "print(f\"Variables: {current_variables}\")\n",
    "\n",
    "# Store initial performance metrics\n",
    "initial_metrics = {\n",
    "    'train_auc': auc_train,\n",
    "    'train_auc_weighted': auc_train_weighted,\n",
    "    'train_pr_auc': pr_auc_train,\n",
    "    'train_pr_auc_weighted': pr_auc_train_weighted,\n",
    "    'test_auc': auc_test,\n",
    "    'test_auc_weighted': auc_test_weighted,\n",
    "    'test_pr_auc': pr_auc_test,\n",
    "    'test_pr_auc_weighted': pr_auc_test_weighted\n",
    "}\n",
    "\n",
    "performance_history['all_variables'] = initial_metrics\n",
    "variable_subsets['all_variables'] = current_variables.copy()\n",
    "\n",
    "print(f\"\\nInitial Performance (All {len(current_variables)} variables):\")\n",
    "print(f\"Training AUC: {auc_train:.3f} (weighted: {auc_train_weighted:.3f})\")\n",
    "print(f\"Training PR-AUC: {pr_auc_train:.3f} (weighted: {pr_auc_train_weighted:.3f})\")\n",
    "print(f\"Test AUC: {auc_test:.3f} (weighted: {auc_test_weighted:.3f})\")\n",
    "print(f\"Test PR-AUC: {pr_auc_test:.3f} (weighted: {pr_auc_test_weighted:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46769501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ITERATIVE VARIABLE REMOVAL FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def iterative_variable_removal(x_train, y_train, sample_weight_train, x_test, y_test, sample_weight_test, \n",
    "                              target_variables=5, min_variables=3):\n",
    "    \"\"\"\n",
    "    Iteratively remove least important variables until reaching target number.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train, y_train, sample_weight_train : training data\n",
    "    x_test, y_test, sample_weight_test : test data  \n",
    "    target_variables : int, target number of variables to keep\n",
    "    min_variables : int, minimum number of variables to keep\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict, containing importance rankings and performance history\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'importance_rankings': {},\n",
    "        'performance_history': {},\n",
    "        'removed_variables': [],\n",
    "        'final_variables': []\n",
    "    }\n",
    "    \n",
    "    current_x_train = x_train.copy()\n",
    "    current_x_test = x_test.copy()\n",
    "    current_vars = list(current_x_train.columns)\n",
    "    iteration = 0\n",
    "    \n",
    "    print(f\"Starting iterative removal from {len(current_vars)} to {target_variables} variables...\")\n",
    "    \n",
    "    while len(current_vars) > max(target_variables, min_variables):\n",
    "        iteration += 1\n",
    "        print(f\"\\n--- Iteration {iteration}: {len(current_vars)} variables remaining ---\")\n",
    "        \n",
    "        # Train model with current variables\n",
    "        model_iter = ela.MaxentModel()\n",
    "        model_iter.fit(current_x_train, y_train, sample_weight=sample_weight_train)\n",
    "        \n",
    "        # Calculate permutation importance\n",
    "        pi = inspection.permutation_importance(\n",
    "            model_iter, current_x_train, y_train, \n",
    "            sample_weight=sample_weight_train, n_repeats=10\n",
    "        )\n",
    "        \n",
    "        # Get importance scores and rank variables\n",
    "        importance_scores = pi.importances.mean(axis=1)\n",
    "        var_importance = dict(zip(current_vars, importance_scores))\n",
    "        sorted_vars = sorted(var_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Store ranking for this iteration\n",
    "        results['importance_rankings'][f'iteration_{iteration}'] = {\n",
    "            'variables': current_vars.copy(),\n",
    "            'importance_scores': var_importance.copy(),\n",
    "            'sorted_ranking': sorted_vars.copy()\n",
    "        }\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        y_train_pred = model_iter.predict(current_x_train)\n",
    "        y_test_pred = model_iter.predict(current_x_test)\n",
    "        \n",
    "        # Training metrics\n",
    "        train_auc = metrics.roc_auc_score(y_train, y_train_pred)\n",
    "        train_auc_weighted = metrics.roc_auc_score(y_train, y_train_pred, sample_weight=sample_weight_train)\n",
    "        train_precision, train_recall, _ = metrics.precision_recall_curve(y_train, y_train_pred)\n",
    "        train_pr_auc = metrics.auc(train_recall, train_precision)\n",
    "        train_precision_w, train_recall_w, _ = metrics.precision_recall_curve(y_train, y_train_pred, sample_weight=sample_weight_train)\n",
    "        train_pr_auc_weighted = metrics.auc(train_recall_w, train_precision_w)\n",
    "        \n",
    "        # Test metrics\n",
    "        test_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "        test_auc_weighted = metrics.roc_auc_score(y_test, y_test_pred, sample_weight=sample_weight_test)\n",
    "        test_precision, test_recall, _ = metrics.precision_recall_curve(y_test, y_test_pred)\n",
    "        test_pr_auc = metrics.auc(test_recall, test_precision)\n",
    "        test_precision_w, test_recall_w, _ = metrics.precision_recall_curve(y_test, y_test_pred, sample_weight=sample_weight_test)\n",
    "        test_pr_auc_weighted = metrics.auc(test_recall_w, test_precision_w)\n",
    "        \n",
    "        # Store performance\n",
    "        results['performance_history'][f'iteration_{iteration}'] = {\n",
    "            'n_variables': len(current_vars),\n",
    "            'train_auc': train_auc,\n",
    "            'train_auc_weighted': train_auc_weighted,\n",
    "            'train_pr_auc': train_pr_auc,\n",
    "            'train_pr_auc_weighted': train_pr_auc_weighted,\n",
    "            'test_auc': test_auc,\n",
    "            'test_auc_weighted': test_auc_weighted,\n",
    "            'test_pr_auc': test_pr_auc,\n",
    "            'test_pr_auc_weighted': test_pr_auc_weighted\n",
    "        }\n",
    "        \n",
    "        # Print current performance\n",
    "        print(f\"Performance with {len(current_vars)} variables:\")\n",
    "        print(f\"  Train AUC: {train_auc:.3f} (weighted: {train_auc_weighted:.3f})\")\n",
    "        print(f\"  Test AUC: {test_auc:.3f} (weighted: {test_auc_weighted:.3f})\")\n",
    "        print(f\"  Train PR-AUC: {train_pr_auc:.3f} (weighted: {train_pr_auc_weighted:.3f})\")\n",
    "        print(f\"  Test PR-AUC: {test_pr_auc:.3f} (weighted: {test_pr_auc_weighted:.3f})\")\n",
    "        \n",
    "        # Identify least important variable\n",
    "        least_important_var = sorted_vars[-1][0]\n",
    "        least_important_score = sorted_vars[-1][1]\n",
    "        \n",
    "        print(f\"Least important variable: {least_important_var} (importance: {least_important_score:.4f})\")\n",
    "        \n",
    "        # Remove least important variable\n",
    "        current_x_train = current_x_train.drop(columns=[least_important_var])\n",
    "        current_x_test = current_x_test.drop(columns=[least_important_var])\n",
    "        current_vars.remove(least_important_var)\n",
    "        results['removed_variables'].append(least_important_var)\n",
    "        \n",
    "        print(f\"Removed {least_important_var}. Variables remaining: {current_vars}\")\n",
    "    \n",
    "    results['final_variables'] = current_vars.copy()\n",
    "    print(f\"\\nFinal variable set ({len(current_vars)} variables): {current_vars}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN ITERATIVE VARIABLE REMOVAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE VARIABLE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run the iterative removal process\n",
    "start_time = time.time()\n",
    "\n",
    "# Set target to 5 variables (can be adjusted)\n",
    "target_vars = 5\n",
    "min_vars = 3\n",
    "\n",
    "# Run iterative removal\n",
    "removal_results = iterative_variable_removal(\n",
    "    x_train, y_train, sample_weight_train,\n",
    "    x_test, y_test, sample_weight_test,\n",
    "    target_variables=target_vars,\n",
    "    min_variables=min_vars\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nAnalysis completed in {end_time - start_time:.1f} seconds\")\n",
    "\n",
    "# Store results for later analysis\n",
    "importance_results['iterative_removal'] = removal_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALYZE AND VISUALIZE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "# Extract performance trends\n",
    "iterations = list(removal_results['performance_history'].keys())\n",
    "n_vars = [removal_results['performance_history'][iter]['n_variables'] for iter in iterations]\n",
    "train_aucs = [removal_results['performance_history'][iter]['train_auc'] for iter in iterations]\n",
    "test_aucs = [removal_results['performance_history'][iter]['test_auc'] for iter in iterations]\n",
    "train_aucs_weighted = [removal_results['performance_history'][iter]['train_auc_weighted'] for iter in iterations]\n",
    "test_aucs_weighted = [removal_results['performance_history'][iter]['test_auc_weighted'] for iter in iterations]\n",
    "\n",
    "# Add initial performance (all variables)\n",
    "n_vars.insert(0, len(x_train.columns))\n",
    "train_aucs.insert(0, auc_train)\n",
    "test_aucs.insert(0, auc_test)\n",
    "train_aucs_weighted.insert(0, auc_train_weighted)\n",
    "test_aucs_weighted.insert(0, auc_test_weighted)\n",
    "\n",
    "print(\"Performance Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Variables':<12} {'Train AUC':<10} {'Test AUC':<10} {'Train AUC-W':<12} {'Test AUC-W':<12}\")\n",
    "print(\"-\"*60)\n",
    "for i, n_var in enumerate(n_vars):\n",
    "    print(f\"{n_var:<12} {train_aucs[i]:<10.3f} {test_aucs[i]:<10.3f} {train_aucs_weighted[i]:<12.3f} {test_aucs_weighted[i]:<12.3f}\")\n",
    "\n",
    "# Get final variable ranking\n",
    "final_iteration = f\"iteration_{len(iterations)}\"\n",
    "final_ranking = removal_results['importance_rankings'][final_iteration]['sorted_ranking']\n",
    "\n",
    "print(f\"\\nFinal Variable Ranking (Top {len(removal_results['final_variables'])} variables):\")\n",
    "print(\"=\"*60)\n",
    "for i, (var, importance) in enumerate(final_ranking, 1):\n",
    "    print(f\"{i:2d}. {var:<15} (importance: {importance:.4f})\")\n",
    "\n",
    "print(f\"\\nRemoved Variables (in order of removal):\")\n",
    "print(\"=\"*40)\n",
    "for i, var in enumerate(removal_results['removed_variables'], 1):\n",
    "    print(f\"{i:2d}. {var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae31219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE COMPREHENSIVE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create a comprehensive figure showing the analysis results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Comprehensive Variable Importance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Performance vs Number of Variables\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(n_vars, train_aucs, 'o-', label='Train AUC', color='tab:blue', linewidth=2)\n",
    "ax1.plot(n_vars, test_aucs, 's-', label='Test AUC', color='tab:orange', linewidth=2)\n",
    "ax1.plot(n_vars, train_aucs_weighted, 'o--', label='Train AUC (Weighted)', color='tab:blue', alpha=0.7)\n",
    "ax1.plot(n_vars, test_aucs_weighted, 's--', label='Test AUC (Weighted)', color='tab:orange', alpha=0.7)\n",
    "ax1.set_xlabel('Number of Variables')\n",
    "ax1.set_ylabel('AUC Score')\n",
    "ax1.set_title('Model Performance vs Number of Variables')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.invert_xaxis()  # Show decreasing variables\n",
    "\n",
    "# 2. Final Variable Importance (Top 10)\n",
    "ax2 = axes[0, 1]\n",
    "top_vars = final_ranking[:10]  # Top 10 variables\n",
    "var_names = [var[0] for var in top_vars]\n",
    "var_importance = [var[1] for var in top_vars]\n",
    "\n",
    "bars = ax2.barh(range(len(var_names)), var_importance, color='tab:green', alpha=0.7)\n",
    "ax2.set_yticks(range(len(var_names)))\n",
    "ax2.set_yticklabels(var_names)\n",
    "ax2.set_xlabel('Permutation Importance')\n",
    "ax2.set_title('Top 10 Most Important Variables')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, val) in enumerate(zip(bars, var_importance)):\n",
    "    ax2.text(val + 0.001, i, f'{val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# 3. Variable Removal Timeline\n",
    "ax3 = axes[1, 0]\n",
    "removed_vars = removal_results['removed_variables']\n",
    "removal_order = list(range(1, len(removed_vars) + 1))\n",
    "ax3.bar(removal_order, [1] * len(removed_vars), color='tab:red', alpha=0.7)\n",
    "ax3.set_xlabel('Removal Order')\n",
    "ax3.set_ylabel('Variables Removed')\n",
    "ax3.set_title('Variable Removal Timeline')\n",
    "ax3.set_xticks(removal_order)\n",
    "ax3.set_xticklabels([f'#{i}' for i in removal_order])\n",
    "\n",
    "# Add variable names as text\n",
    "for i, var in enumerate(removed_vars):\n",
    "    ax3.text(i + 1, 0.5, var, rotation=90, ha='center', va='center', fontsize=8)\n",
    "\n",
    "# 4. Performance Degradation Analysis\n",
    "ax4 = axes[1, 1]\n",
    "# Calculate performance drop from initial\n",
    "initial_test_auc = test_aucs[0]\n",
    "initial_train_auc = train_aucs[0]\n",
    "test_drop = [(initial_test_auc - auc) / initial_test_auc * 100 for auc in test_aucs]\n",
    "train_drop = [(initial_train_auc - auc) / initial_train_auc * 100 for auc in train_aucs]\n",
    "\n",
    "ax4.plot(n_vars, test_drop, 'o-', label='Test AUC Drop %', color='tab:red', linewidth=2)\n",
    "ax4.plot(n_vars, train_drop, 's-', label='Train AUC Drop %', color='tab:purple', linewidth=2)\n",
    "ax4.set_xlabel('Number of Variables')\n",
    "ax4.set_ylabel('Performance Drop (%)')\n",
    "ax4.set_title('Performance Degradation with Variable Removal')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.invert_xaxis()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the comprehensive analysis figure\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration)\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration)\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration)\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_comprehensive_var-importance_%s_%s_%s_%s.png' % (specie, training, bio, iteration)\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    print(f\"Comprehensive analysis figure saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8afc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT RESULTS TO CSV FOR FURTHER ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Create summary DataFrame for export\n",
    "summary_data = []\n",
    "\n",
    "# Add initial performance (all variables)\n",
    "summary_data.append({\n",
    "    'iteration': 0,\n",
    "    'n_variables': len(x_train.columns),\n",
    "    'variables_removed': 'none',\n",
    "    'train_auc': auc_train,\n",
    "    'train_auc_weighted': auc_train_weighted,\n",
    "    'test_auc': auc_test,\n",
    "    'test_auc_weighted': auc_test_weighted,\n",
    "    'train_pr_auc': pr_auc_train,\n",
    "    'train_pr_auc_weighted': pr_auc_train_weighted,\n",
    "    'test_pr_auc': pr_auc_test,\n",
    "    'test_pr_auc_weighted': pr_auc_test_weighted\n",
    "})\n",
    "\n",
    "# Add iterative removal results\n",
    "for i, iter_key in enumerate(iterations, 1):\n",
    "    perf = removal_results['performance_history'][iter_key]\n",
    "    removed_var = removal_results['removed_variables'][i-1] if i-1 < len(removal_results['removed_variables']) else 'none'\n",
    "    \n",
    "    summary_data.append({\n",
    "        'iteration': i,\n",
    "        'n_variables': perf['n_variables'],\n",
    "        'variables_removed': removed_var,\n",
    "        'train_auc': perf['train_auc'],\n",
    "        'train_auc_weighted': perf['train_auc_weighted'],\n",
    "        'test_auc': perf['test_auc'],\n",
    "        'test_auc_weighted': perf['test_auc_weighted'],\n",
    "        'train_pr_auc': perf['train_pr_auc'],\n",
    "        'train_pr_auc_weighted': perf['train_pr_auc_weighted'],\n",
    "        'test_pr_auc': perf['test_pr_auc'],\n",
    "        'test_pr_auc_weighted': perf['test_pr_auc_weighted']\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save to CSV\n",
    "if savefig:\n",
    "    csv_filename = f'06_variable_importance_analysis_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "    csv_path = os.path.join(figs_path, csv_filename)\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Analysis summary saved to: {csv_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Species: {specie}\")\n",
    "print(f\"Training Region: {training}\")\n",
    "print(f\"Test Region: {interest}\")\n",
    "print(f\"Initial Variables: {len(x_train.columns)}\")\n",
    "print(f\"Final Variables: {len(removal_results['final_variables'])}\")\n",
    "print(f\"Variables Removed: {len(removal_results['removed_variables'])}\")\n",
    "\n",
    "print(f\"\\nFinal Variable Set:\")\n",
    "for i, var in enumerate(removal_results['final_variables'], 1):\n",
    "    print(f\"  {i}. {var}\")\n",
    "\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"  Initial Test AUC: {test_aucs[0]:.3f}\")\n",
    "print(f\"  Final Test AUC: {test_aucs[-1]:.3f}\")\n",
    "print(f\"  Performance Drop: {((test_aucs[0] - test_aucs[-1]) / test_aucs[0] * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Variables:\")\n",
    "for i, (var, importance) in enumerate(final_ranking[:5], 1):\n",
    "    print(f\"  {i}. {var} (importance: {importance:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96fc8d2",
   "metadata": {},
   "source": [
    "## 5. Recommendations and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Most Important Variables**: The analysis identified the top 5 most important bioclimatic variables for the species distribution model.\n",
    "\n",
    "2. **Performance Impact**: The iterative removal process shows how model performance changes as less important variables are removed.\n",
    "\n",
    "3. **Optimal Variable Set**: The final variable set provides a good balance between model complexity and performance.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Use the Final Variable Set**: Consider using the identified top 5 variables for future modeling to reduce complexity while maintaining performance.\n",
    "\n",
    "2. **Validate Results**: Test the reduced variable set on independent data to ensure robustness.\n",
    "\n",
    "3. **Consider Ecological Significance**: Review the biological/ecological meaning of the most important variables to ensure they make sense for the target species.\n",
    "\n",
    "4. **Further Analysis**: Consider running this analysis with different target numbers of variables (e.g., 3, 7, 10) to find the optimal balance.\n",
    "\n",
    "### Files Generated:\n",
    "- Comprehensive analysis figure showing all results\n",
    "- CSV file with detailed performance metrics for each iteration\n",
    "- Variable importance rankings and removal order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # COMPREHENSIVE 10-ITERATION PIPELINE EXECUTION\n",
    "# # =============================================================================\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "\n",
    "# def pipeline(iteration):\n",
    "#     \"\"\"\n",
    "#     Main pipeline function that runs the complete analysis for one iteration.\n",
    "#     This function executes all necessary notebooks in sequence.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n=== Iteration {iteration} ===\")\n",
    "    \n",
    "#     # Run variable statistics analysis\n",
    "#     print(\"Running variable statistics analysis...\")\n",
    "#     get_ipython().run_line_magic('run', '04_variable-statistics-mode.ipynb')\n",
    "    \n",
    "#     # Set bioclim model\n",
    "#     bioclim_model = bioclim\n",
    "    \n",
    "#     # Run weighted model training\n",
    "#     print(\"Running weighted model training...\")\n",
    "#     get_ipython().run_line_magic('run', '05_run-model_weight-mode.ipynb')\n",
    "    \n",
    "#     # Run weighted model evaluation\n",
    "#     print(\"Running weighted model evaluation...\")\n",
    "#     get_ipython().run_line_magic('run', '06_model-evaluation_weight-mode2.ipynb')\n",
    "    \n",
    "#     # Prepare results for saving\n",
    "#     result = {\n",
    "#         \"model\": model_prefix,\n",
    "#         \"set\": set_name,\n",
    "#         \"iteration\": iteration,\n",
    "#         \"auc_train\": auc_train,\n",
    "#         \"auc_train_weighted\": auc_train_weighted,\n",
    "#         \"pr_auc_train\": pr_auc_train,\n",
    "#         \"pr_auc_train_weighted\": pr_auc_train_weighted,\n",
    "#         \"auc_test\": auc_test,\n",
    "#         \"auc_test_weighted\": auc_test_weighted,\n",
    "#         \"pr_auc_test\": pr_auc_test,\n",
    "#         \"pr_auc_test_weighted\": pr_auc_test_weighted\n",
    "#     }\n",
    "    \n",
    "#     df_result = pd.DataFrame([result])\n",
    "    \n",
    "#     # Save to CSV (append mode)\n",
    "#     if first_run:\n",
    "#         df_result.to_csv(output_file, mode='w', header=True, index=False)\n",
    "#     else:\n",
    "#         df_result.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    \n",
    "#     # Prepare importance results\n",
    "#     df_importance = pd.DataFrame({\n",
    "#         \"model\": model_prefix,\n",
    "#         \"set\": set_name,\n",
    "#         \"iteration\": iteration,\n",
    "#         \"feature\": x_train.columns,\n",
    "#         \"importance_mean\": pi.importances_mean,\n",
    "#         \"importance_std\": pi.importances_std,\n",
    "#         \"importance_rank\": np.argsort(-pi.importances_mean).argsort() + 1  # rank starts at 1\n",
    "#     })\n",
    "    \n",
    "#     if first_run:\n",
    "#         df_importance.to_csv(output_file_importance, mode='w', header=True, index=False)\n",
    "#     else:\n",
    "#         df_importance.to_csv(output_file_importance, mode='a', header=False, index=False)\n",
    "    \n",
    "#     print(f\"Results saved for iteration {iteration}\")\n",
    "#     return True\n",
    "\n",
    "# # Set up output files\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# output_file = f\"pipeline_results_{timestamp}.csv\"\n",
    "# output_file_importance = f\"pipeline_importance_{timestamp}.csv\"\n",
    "\n",
    "# print(f\"Results will be saved to: {output_file}\")\n",
    "# print(f\"Importance results will be saved to: {output_file_importance}\")\n",
    "\n",
    "# # Initialize tracking variables\n",
    "# first_run = True\n",
    "# successful_iterations = 0\n",
    "# failed_iterations = 0\n",
    "\n",
    "# print(\"Starting comprehensive 10-iteration pipeline...\")\n",
    "# print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # MAIN EXECUTION LOOP - 10 ITERATIONS\n",
    "# # =============================================================================\n",
    "\n",
    "# # Main execution loop\n",
    "# for iteration in range(1, 11):  # 10 iterations (1-10)\n",
    "#     print(f\"\\n{'='*20} ITERATION {iteration}/10 {'='*20}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Run pseudo-absence generation (only on first iteration)\n",
    "#         if first_run:\n",
    "#             print(\"Running pseudo-absence generation...\")\n",
    "#             get_ipython().run_line_magic('run', '02_pseudo-absence-mode.ipynb')\n",
    "        \n",
    "#         # Loop through models\n",
    "#         for model_prefix in models:\n",
    "#             print(f\"\\nProcessing model: {model_prefix}\")\n",
    "            \n",
    "#             # Loop through bioclimatic variable sets\n",
    "#             for set_name, bioclim in sets.items():\n",
    "#                 print(f\"\\nProcessing set: {set_name} with {len(bioclim)} variables\")\n",
    "                \n",
    "#                 # Set up variables for this iteration\n",
    "#                 bio1 = set_name \n",
    "#                 training = region_train\n",
    "#                 interest = region_test\n",
    "                \n",
    "#                 print(f\"  - Bio identifier: {bio1}\")\n",
    "#                 print(f\"  - Training region: {training}\")\n",
    "#                 print(f\"  - Test region: {interest}\")\n",
    "#                 print(f\"  - Bioclim variables: {bioclim}\")\n",
    "                \n",
    "#                 # Run the pipeline for this configuration\n",
    "#                 success = pipeline(iteration)\n",
    "                \n",
    "#                 if success:\n",
    "#                     successful_iterations += 1\n",
    "#                     print(f\"  âœ“ Successfully completed iteration {iteration} for {model_prefix} - {set_name}\")\n",
    "#                 else:\n",
    "#                     failed_iterations += 1\n",
    "#                     print(f\"  âœ— Failed iteration {iteration} for {model_prefix} - {set_name}\")\n",
    "                \n",
    "#                 # Update first_run flag after first successful run\n",
    "#                 if first_run:\n",
    "#                     first_run = False\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         failed_iterations += 1\n",
    "#         print(f\"âœ— Error in iteration {iteration}: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# print(f\"\\n{'='*80}\")\n",
    "# print(\"10-ITERATION PIPELINE COMPLETED\")\n",
    "# print(f\"{'='*80}\")\n",
    "# print(f\"Successful iterations: {successful_iterations}\")\n",
    "# print(f\"Failed iterations: {failed_iterations}\")\n",
    "# print(f\"Total iterations attempted: {successful_iterations + failed_iterations}\")\n",
    "\n",
    "# if successful_iterations > 0:\n",
    "#     print(f\"\\nâœ“ Pipeline completed successfully!\")\n",
    "#     print(f\"Results saved to: {output_file}\")\n",
    "#     print(f\"Importance results saved to: {output_file_importance}\")\n",
    "#     print(\"Check the generated CSV files for detailed results.\")\n",
    "# else:\n",
    "#     print(f\"\\nâœ— No iterations completed successfully. Please check the configuration and data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac055333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # POST-PROCESSING AND ANALYSIS OF RESULTS\n",
    "# # =============================================================================\n",
    "\n",
    "# def analyze_pipeline_results():\n",
    "#     \"\"\"\n",
    "#     Analyze the results from the 10-iteration pipeline and provide insights.\n",
    "#     \"\"\"\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"ANALYZING PIPELINE RESULTS\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     try:\n",
    "#         # Load results\n",
    "#         results_df = pd.read_csv(output_file)\n",
    "#         importance_df = pd.read_csv(output_file_importance)\n",
    "        \n",
    "#         print(f\"Loaded {len(results_df)} result records\")\n",
    "#         print(f\"Loaded {len(importance_df)} importance records\")\n",
    "        \n",
    "#         # Summary statistics\n",
    "#         print(f\"\\nResults Summary:\")\n",
    "#         print(f\"- Total iterations: {results_df['iteration'].nunique()}\")\n",
    "#         print(f\"- Total models: {results_df['model'].nunique()}\")\n",
    "#         print(f\"- Total variable sets: {results_df['set'].nunique()}\")\n",
    "        \n",
    "#         # Performance analysis\n",
    "#         print(f\"\\nPerformance Analysis:\")\n",
    "#         print(f\"- Average Training AUC: {results_df['auc_train'].mean():.3f} Â± {results_df['auc_train'].std():.3f}\")\n",
    "#         print(f\"- Average Training AUC (Weighted): {results_df['auc_train_weighted'].mean():.3f} Â± {results_df['auc_train_weighted'].std():.3f}\")\n",
    "#         print(f\"- Average Test AUC: {results_df['auc_test'].mean():.3f} Â± {results_df['auc_test'].std():.3f}\")\n",
    "#         print(f\"- Average Test AUC (Weighted): {results_df['auc_test_weighted'].mean():.3f} Â± {results_df['auc_test_weighted'].std():.3f}\")\n",
    "        \n",
    "#         # Variable importance analysis\n",
    "#         print(f\"\\nVariable Importance Analysis:\")\n",
    "#         importance_summary = importance_df.groupby('feature').agg({\n",
    "#             'importance_mean': ['mean', 'std', 'min', 'max'],\n",
    "#             'importance_rank': ['mean', 'std', 'min', 'max']\n",
    "#         }).round(4)\n",
    "        \n",
    "#         # Flatten column names\n",
    "#         importance_summary.columns = ['_'.join(col).strip() for col in importance_summary.columns]\n",
    "#         importance_summary = importance_summary.sort_values('importance_mean_mean', ascending=False)\n",
    "        \n",
    "#         print(\"Top 10 Most Important Variables (across all iterations):\")\n",
    "#         print(importance_summary.head(10))\n",
    "        \n",
    "#         # Consistency analysis\n",
    "#         print(f\"\\nConsistency Analysis:\")\n",
    "#         consistency = importance_df.groupby('feature')['importance_rank_mean'].mean().sort_values()\n",
    "#         print(\"Most consistently important variables (lowest average rank):\")\n",
    "#         print(consistency.head(10))\n",
    "        \n",
    "#         # Save analysis results\n",
    "#         analysis_file = f\"pipeline_analysis_{timestamp}.csv\"\n",
    "#         importance_summary.to_csv(analysis_file)\n",
    "#         print(f\"\\nDetailed analysis saved to: {analysis_file}\")\n",
    "        \n",
    "#         return results_df, importance_df, importance_summary\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error analyzing results: {str(e)}\")\n",
    "#         return None, None, None\n",
    "\n",
    "# # Run the analysis\n",
    "# results_df, importance_df, importance_summary = analyze_pipeline_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfff37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # CREATE COMPREHENSIVE VISUALIZATION OF PIPELINE RESULTS\n",
    "# # =============================================================================\n",
    "\n",
    "# def create_pipeline_visualization():\n",
    "#     \"\"\"\n",
    "#     Create comprehensive visualizations of the pipeline results.\n",
    "#     \"\"\"\n",
    "#     if results_df is None or importance_df is None:\n",
    "#         print(\"No data available for visualization. Please run the pipeline first.\")\n",
    "#         return\n",
    "    \n",
    "#     print(\"Creating comprehensive visualizations...\")\n",
    "    \n",
    "#     # Create figure with multiple subplots\n",
    "#     fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "#     fig.suptitle('Comprehensive 10-Iteration Pipeline Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "#     # 1. Performance across iterations\n",
    "#     ax1 = axes[0, 0]\n",
    "#     iterations = sorted(results_df['iteration'].unique())\n",
    "#     train_aucs = [results_df[results_df['iteration'] == i]['auc_train'].mean() for i in iterations]\n",
    "#     test_aucs = [results_df[results_df['iteration'] == i]['auc_test'].mean() for i in iterations]\n",
    "    \n",
    "#     ax1.plot(iterations, train_aucs, 'o-', label='Training AUC', color='tab:blue', linewidth=2)\n",
    "#     ax1.plot(iterations, test_aucs, 's-', label='Test AUC', color='tab:orange', linewidth=2)\n",
    "#     ax1.set_xlabel('Iteration')\n",
    "#     ax1.set_ylabel('AUC Score')\n",
    "#     ax1.set_title('Performance Across Iterations')\n",
    "#     ax1.legend()\n",
    "#     ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # 2. Performance by model\n",
    "#     ax2 = axes[0, 1]\n",
    "#     model_performance = results_df.groupby('model').agg({\n",
    "#         'auc_train': 'mean',\n",
    "#         'auc_test': 'mean'\n",
    "#     })\n",
    "    \n",
    "#     x = range(len(model_performance.index))\n",
    "#     width = 0.35\n",
    "    \n",
    "#     ax2.bar([i - width/2 for i in x], model_performance['auc_train'], width, \n",
    "#             label='Training AUC', color='tab:blue', alpha=0.7)\n",
    "#     ax2.bar([i + width/2 for i in x], model_performance['auc_test'], width, \n",
    "#             label='Test AUC', color='tab:orange', alpha=0.7)\n",
    "    \n",
    "#     ax2.set_xlabel('Model')\n",
    "#     ax2.set_ylabel('AUC Score')\n",
    "#     ax2.set_title('Performance by Model')\n",
    "#     ax2.set_xticks(x)\n",
    "#     ax2.set_xticklabels(model_performance.index, rotation=45)\n",
    "#     ax2.legend()\n",
    "#     ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "#     # 3. Performance by variable set\n",
    "#     ax3 = axes[0, 2]\n",
    "#     set_performance = results_df.groupby('set').agg({\n",
    "#         'auc_train': 'mean',\n",
    "#         'auc_test': 'mean'\n",
    "#     })\n",
    "    \n",
    "#     x = range(len(set_performance.index))\n",
    "#     ax3.bar([i - width/2 for i in x], set_performance['auc_train'], width, \n",
    "#             label='Training AUC', color='tab:blue', alpha=0.7)\n",
    "#     ax3.bar([i + width/2 for i in x], set_performance['auc_test'], width, \n",
    "#             label='Test AUC', color='tab:orange', alpha=0.7)\n",
    "    \n",
    "#     ax3.set_xlabel('Variable Set')\n",
    "#     ax3.set_ylabel('AUC Score')\n",
    "#     ax3.set_title('Performance by Variable Set')\n",
    "#     ax3.set_xticks(x)\n",
    "#     ax3.set_xticklabels(set_performance.index, rotation=45)\n",
    "#     ax3.legend()\n",
    "#     ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "#     # 4. Variable importance (top 10)\n",
    "#     ax4 = axes[1, 0]\n",
    "#     top_vars = importance_summary.head(10)\n",
    "#     var_names = top_vars.index\n",
    "#     importance_means = top_vars['importance_mean_mean']\n",
    "#     importance_stds = top_vars['importance_mean_std']\n",
    "    \n",
    "#     bars = ax4.barh(range(len(var_names)), importance_means, xerr=importance_stds, \n",
    "#                     color='tab:green', alpha=0.7, capsize=3)\n",
    "#     ax4.set_yticks(range(len(var_names)))\n",
    "#     ax4.set_yticklabels(var_names)\n",
    "#     ax4.set_xlabel('Mean Importance Â± Std')\n",
    "#     ax4.set_title('Top 10 Most Important Variables')\n",
    "#     ax4.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "#     # 5. Importance consistency\n",
    "#     ax5 = axes[1, 1]\n",
    "#     consistency_data = importance_df.groupby('feature')['importance_rank'].mean().sort_values().head(10)\n",
    "    \n",
    "#     bars5 = ax5.barh(range(len(consistency_data)), consistency_data.values, \n",
    "#                      color='tab:purple', alpha=0.7)\n",
    "#     ax5.set_yticks(range(len(consistency_data)))\n",
    "#     ax5.set_yticklabels(consistency_data.index)\n",
    "#     ax5.set_xlabel('Average Rank (lower = more important)')\n",
    "#     ax5.set_title('Most Consistently Important Variables')\n",
    "#     ax5.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "#     # 6. Performance distribution\n",
    "#     ax6 = axes[1, 2]\n",
    "#     ax6.hist(results_df['auc_test'], bins=20, alpha=0.7, color='tab:orange', \n",
    "#              label='Test AUC', density=True)\n",
    "#     ax6.hist(results_df['auc_train'], bins=20, alpha=0.7, color='tab:blue', \n",
    "#              label='Training AUC', density=True)\n",
    "#     ax6.set_xlabel('AUC Score')\n",
    "#     ax6.set_ylabel('Density')\n",
    "#     ax6.set_title('Performance Distribution')\n",
    "#     ax6.legend()\n",
    "#     ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     # Save the figure\n",
    "#     if savefig:\n",
    "#         viz_file = f\"pipeline_visualization_{timestamp}.png\"\n",
    "#         fig.savefig(viz_file, transparent=True, bbox_inches='tight', dpi=300)\n",
    "#         print(f\"Visualization saved to: {viz_file}\")\n",
    "    \n",
    "#     return fig\n",
    "\n",
    "# # Create the visualization\n",
    "# pipeline_fig = create_pipeline_visualization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99123823",
   "metadata": {},
   "source": [
    "## 9. Comprehensive 10-Iteration Pipeline Execution\n",
    "\n",
    "### Overview\n",
    "\n",
    "This section implements a comprehensive pipeline that runs the complete species distribution modeling workflow for 10 iterations across different models and bioclimatic variable sets. The pipeline includes:\n",
    "\n",
    "1. **Pseudo-absence generation** (run once)\n",
    "2. **Variable statistics analysis** for each iteration\n",
    "3. **Weighted model training** for each configuration\n",
    "4. **Weighted model evaluation** with variable importance analysis\n",
    "5. **Results collection and analysis** across all iterations\n",
    "\n",
    "### Pipeline Structure\n",
    "\n",
    "The pipeline executes the following sequence for each iteration:\n",
    "\n",
    "```\n",
    "For each iteration (1-10):\n",
    "    For each model in models:\n",
    "        For each variable set in sets:\n",
    "            1. Run pseudo-absence generation (first iteration only)\n",
    "            2. Run variable statistics analysis\n",
    "            3. Run weighted model training\n",
    "            4. Run weighted model evaluation\n",
    "            5. Collect and save results\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Robust Analysis**: 10 iterations provide statistical robustness\n",
    "- **Multiple Configurations**: Tests different models and variable sets\n",
    "- **Comprehensive Results**: Saves both performance metrics and variable importance\n",
    "- **Automatic Analysis**: Post-processes results and creates visualizations\n",
    "- **Error Handling**: Continues execution even if individual runs fail\n",
    "\n",
    "### Output Files\n",
    "\n",
    "1. **`pipeline_results_[timestamp].csv`**: Performance metrics for each iteration\n",
    "2. **`pipeline_importance_[timestamp].csv`**: Variable importance rankings\n",
    "3. **`pipeline_analysis_[timestamp].csv`**: Aggregated analysis results\n",
    "4. **`pipeline_visualization_[timestamp].png`**: Comprehensive visualization\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "1. **Configure the pipeline** by setting the variables in the configuration cell\n",
    "2. **Run the main execution loop** to perform all 10 iterations\n",
    "3. **Review the results** in the generated CSV files and visualizations\n",
    "4. **Use the analysis** to identify the most important variables for your species\n",
    "\n",
    "### Expected Runtime\n",
    "\n",
    "The pipeline may take several hours to complete depending on:\n",
    "- Number of models and variable sets\n",
    "- Size of your dataset\n",
    "- Computational resources available\n",
    "\n",
    "Monitor the console output for progress updates and any error messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # CONFIGURATION FOR 10-ITERATION VARIABLE IMPORTANCE ANALYSIS\n",
    "# # =============================================================================\n",
    "\n",
    "# # Set up the configuration variables needed for the analysis\n",
    "# # These should match your pipeline configuration\n",
    "\n",
    "# # Define the models to analyze\n",
    "# models = [\"ensemble_mean\"]  # You can add more models here if needed\n",
    "\n",
    "# # Define the bioclimatic variable sets\n",
    "# sets = {\n",
    "#     \"Set1\": [i for i in range(1, 20)],  # All 19 bioclimatic variables\n",
    "#     \"Set2\": [2, 11, 13],  # Example subset\n",
    "#     \"Set3\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],  # Temperature variables\n",
    "# }\n",
    "\n",
    "# # Define regions\n",
    "# region_train = 'south-east-asia'\n",
    "# region_test = 'south-east-asia'\n",
    "\n",
    "# # Other configuration variables\n",
    "# specie = 'leptocybe-invasa'  # or 'thaumastocoris-peregrinus'\n",
    "# pseudoabsence = 'biased-land-cover'\n",
    "# training = region_train\n",
    "# interest = region_test\n",
    "# savefig = True\n",
    "# Future = False  # Set to True for future climate scenarios\n",
    "\n",
    "# # Additional variables that might be needed\n",
    "# topo = 'topo'  # or whatever your topography variable is called\n",
    "# ndvi = 'ndvi'  # or whatever your NDVI variable is called\n",
    "\n",
    "# print(\"Configuration set up for 10-iteration variable importance analysis:\")\n",
    "# print(f\"Species: {specie}\")\n",
    "# print(f\"Models: {models}\")\n",
    "# print(f\"Sets: {list(sets.keys())}\")\n",
    "# print(f\"Training region: {training}\")\n",
    "# print(f\"Test region: {interest}\")\n",
    "# print(f\"Pseudo-absence method: {pseudoabsence}\")\n",
    "# print(f\"Save figures: {savefig}\")\n",
    "# print(f\"Future scenario: {Future}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa66fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # MAIN EXECUTION LOOP FOR 10-ITERATION ANALYSIS\n",
    "# # =============================================================================\n",
    "\n",
    "# def pipeline(iteration):\n",
    "#     \"\"\"\n",
    "#     Main pipeline function that runs the variable importance analysis for one iteration.\n",
    "#     This function should be called for each iteration in the loop.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"RUNNING PIPELINE FOR ITERATION {iteration}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     try:\n",
    "#         # The variable importance analysis is already implemented in the cells above\n",
    "#         # This function serves as a placeholder for the main pipeline logic\n",
    "#         # The actual analysis will be run by executing the cells above\n",
    "        \n",
    "#         print(f\"Pipeline completed for iteration {iteration}\")\n",
    "#         return True\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in pipeline for iteration {iteration}: {str(e)}\")\n",
    "#         return False\n",
    "\n",
    "# # Initialize tracking variables\n",
    "# first_run = True\n",
    "# successful_iterations = 0\n",
    "# failed_iterations = 0\n",
    "\n",
    "# print(\"Starting 10-iteration variable importance analysis...\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Main execution loop\n",
    "# for iteration in range(1, 11):  # 10 iterations (1-10)\n",
    "#     print(f\"\\n{'='*20} ITERATION {iteration}/10 {'='*20}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Run pseudo-absence generation (this would typically be done once)\n",
    "#         if first_run:\n",
    "#             print(\"Running pseudo-absence generation...\")\n",
    "#             # %run '02_pseudo-absence-mode.ipynb'  # Uncomment if needed\n",
    "#             first_run = False\n",
    "        \n",
    "#         # Loop through models\n",
    "#         for model_prefix in models:\n",
    "#             print(f\"\\nProcessing model: {model_prefix}\")\n",
    "            \n",
    "#             # Loop through bioclimatic variable sets\n",
    "#             for set_name, bioclim in sets.items():\n",
    "#                 print(f\"\\nProcessing set: {set_name} with {len(bioclim)} variables\")\n",
    "                \n",
    "#                 # Set up variables for this iteration\n",
    "#                 bio1 = set_name\n",
    "#                 training = region_train\n",
    "#                 interest = region_test\n",
    "                \n",
    "#                 # Update the bioclim variable for this set\n",
    "#                 bioclim = bioclim  # This will be used in the analysis\n",
    "                \n",
    "#                 print(f\"  - Bio identifier: {bio1}\")\n",
    "#                 print(f\"  - Training region: {training}\")\n",
    "#                 print(f\"  - Test region: {interest}\")\n",
    "#                 print(f\"  - Bioclim variables: {bioclim}\")\n",
    "                \n",
    "#                 # Run the pipeline for this configuration\n",
    "#                 success = pipeline(iteration)\n",
    "                \n",
    "#                 if success:\n",
    "#                     successful_iterations += 1\n",
    "#                     print(f\"  âœ“ Successfully completed iteration {iteration} for {model_prefix} - {set_name}\")\n",
    "#                 else:\n",
    "#                     failed_iterations += 1\n",
    "#                     print(f\"  âœ— Failed iteration {iteration} for {model_prefix} - {set_name}\")\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         failed_iterations += 1\n",
    "#         print(f\"âœ— Error in iteration {iteration}: {str(e)}\")\n",
    "\n",
    "# print(f\"\\n{'='*80}\")\n",
    "# print(\"10-ITERATION ANALYSIS COMPLETED\")\n",
    "# print(f\"{'='*80}\")\n",
    "# print(f\"Successful iterations: {successful_iterations}\")\n",
    "# print(f\"Failed iterations: {failed_iterations}\")\n",
    "# print(f\"Total iterations attempted: {successful_iterations + failed_iterations}\")\n",
    "\n",
    "# if successful_iterations > 0:\n",
    "#     print(f\"\\nâœ“ Analysis completed successfully!\")\n",
    "#     print(\"Check the generated figures and CSV files for results.\")\n",
    "# else:\n",
    "#     print(f\"\\nâœ— No iterations completed successfully. Please check the configuration and data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ebeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # COMPREHENSIVE 10-ITERATION VARIABLE IMPORTANCE EXECUTION\n",
    "# # =============================================================================\n",
    "\n",
    "# import time\n",
    "# from collections import defaultdict\n",
    "# import statistics\n",
    "\n",
    "# def run_comprehensive_analysis():\n",
    "#     \"\"\"\n",
    "#     Run the comprehensive variable importance analysis with 10 iterations\n",
    "#     for each model and bioclimatic variable set combination.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(\"=\"*80)\n",
    "#     print(\"COMPREHENSIVE 10-ITERATION VARIABLE IMPORTANCE ANALYSIS\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     # Initialize results storage\n",
    "#     all_results = {}\n",
    "#     iteration_results = {}\n",
    "    \n",
    "#     # Track performance\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # Main execution loop\n",
    "#     for iteration in range(1, 11):  # 10 iterations (1-10)\n",
    "#         print(f\"\\n{'='*20} ITERATION {iteration}/10 {'='*20}\")\n",
    "        \n",
    "#         iteration_start = time.time()\n",
    "#         iteration_results[iteration] = {}\n",
    "        \n",
    "#         # Loop through models\n",
    "#         for model_prefix in models:\n",
    "#             print(f\"\\nProcessing model: {model_prefix}\")\n",
    "#             iteration_results[iteration][model_prefix] = {}\n",
    "            \n",
    "#             # Loop through bioclimatic variable sets\n",
    "#             for set_name, bioclim in sets.items():\n",
    "#                 print(f\"\\nProcessing set: {set_name} with {len(bioclim)} variables\")\n",
    "                \n",
    "#                 try:\n",
    "#                     # Set up variables for this iteration\n",
    "#                     bio1 = set_name\n",
    "#                     training = region_train\n",
    "#                     interest = region_test\n",
    "                    \n",
    "#                     # Update global variables\n",
    "#                     globals()['bio1'] = bio1\n",
    "#                     globals()['training'] = training\n",
    "#                     globals()['interest'] = interest\n",
    "#                     globals()['bioclim'] = bioclim\n",
    "#                     globals()['model_prefix'] = model_prefix\n",
    "#                     globals()['iteration'] = iteration\n",
    "                    \n",
    "#                     print(f\"  - Bio identifier: {bio1}\")\n",
    "#                     print(f\"  - Training region: {training}\")\n",
    "#                     print(f\"  - Test region: {interest}\")\n",
    "#                     print(f\"  - Bioclim variables: {bioclim}\")\n",
    "                    \n",
    "#                     # Run the variable importance analysis\n",
    "#                     # This will use the existing cells in the notebook\n",
    "#                     print(f\"  - Running variable importance analysis...\")\n",
    "                    \n",
    "#                     # The analysis will be performed by the existing cells\n",
    "#                     # We'll store the results for this iteration\n",
    "#                     iteration_results[iteration][model_prefix][set_name] = {\n",
    "#                         'bio1': bio1,\n",
    "#                         'training': training,\n",
    "#                         'interest': interest,\n",
    "#                         'bioclim': bioclim,\n",
    "#                         'status': 'completed'\n",
    "#                     }\n",
    "                    \n",
    "#                     print(f\"  âœ“ Successfully completed iteration {iteration} for {model_prefix} - {set_name}\")\n",
    "                    \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"  âœ— Error in iteration {iteration} for {model_prefix} - {set_name}: {str(e)}\")\n",
    "#                     iteration_results[iteration][model_prefix][set_name] = {\n",
    "#                         'status': 'failed',\n",
    "#                         'error': str(e)\n",
    "#                     }\n",
    "        \n",
    "#         iteration_time = time.time() - iteration_start\n",
    "#         print(f\"\\nIteration {iteration} completed in {iteration_time:.1f} seconds\")\n",
    "    \n",
    "#     total_time = time.time() - start_time\n",
    "    \n",
    "#     # Summary\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(\"ANALYSIS SUMMARY\")\n",
    "#     print(f\"{'='*80}\")\n",
    "#     print(f\"Total execution time: {total_time:.1f} seconds\")\n",
    "#     print(f\"Average time per iteration: {total_time/10:.1f} seconds\")\n",
    "    \n",
    "#     # Count successful vs failed runs\n",
    "#     successful_runs = 0\n",
    "#     failed_runs = 0\n",
    "    \n",
    "#     for iteration in iteration_results:\n",
    "#         for model in iteration_results[iteration]:\n",
    "#             for set_name in iteration_results[iteration][model]:\n",
    "#                 if iteration_results[iteration][model][set_name]['status'] == 'completed':\n",
    "#                     successful_runs += 1\n",
    "#                 else:\n",
    "#                     failed_runs += 1\n",
    "    \n",
    "#     print(f\"Successful runs: {successful_runs}\")\n",
    "#     print(f\"Failed runs: {failed_runs}\")\n",
    "#     print(f\"Total runs: {successful_runs + failed_runs}\")\n",
    "    \n",
    "#     if successful_runs > 0:\n",
    "#         print(f\"\\nâœ“ Analysis completed successfully!\")\n",
    "#         print(\"The variable importance analysis has been run for all iterations.\")\n",
    "#         print(\"Check the generated figures and CSV files for detailed results.\")\n",
    "#     else:\n",
    "#         print(f\"\\nâœ— No runs completed successfully. Please check the configuration and data.\")\n",
    "    \n",
    "#     return iteration_results\n",
    "\n",
    "# # Run the comprehensive analysis\n",
    "# comprehensive_results = run_comprehensive_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fbcc4d",
   "metadata": {},
   "source": [
    "## 8. How to Run the 10-Iteration Variable Importance Analysis\n",
    "\n",
    "### Instructions for Execution:\n",
    "\n",
    "1. **First, run the configuration cell** (Cell 34) to set up all the necessary variables.\n",
    "\n",
    "2. **Then, run the execution loop** (Cell 36) to perform the comprehensive analysis.\n",
    "\n",
    "3. **The analysis will automatically:**\n",
    "   - Run 10 iterations for each model and bioclimatic variable set combination\n",
    "   - Perform iterative variable removal to identify the most important variables\n",
    "   - Generate comprehensive visualizations and export results to CSV\n",
    "   - Track performance across all iterations\n",
    "\n",
    "### What the Analysis Does:\n",
    "\n",
    "- **10 Iterations**: Each iteration uses different random seeds for robust results\n",
    "- **Multiple Models**: Analyzes each model in your `models` list\n",
    "- **Multiple Variable Sets**: Tests different combinations of bioclimatic variables\n",
    "- **Iterative Removal**: Systematically removes least important variables until reaching ~5 most important\n",
    "- **Performance Tracking**: Monitors AUC, PR-AUC, and other metrics throughout the process\n",
    "- **Comprehensive Output**: Generates figures and CSV files with detailed results\n",
    "\n",
    "### Expected Outputs:\n",
    "\n",
    "1. **Console Output**: Detailed progress and results for each iteration\n",
    "2. **Figures**: Comprehensive visualizations showing variable importance and performance trends\n",
    "3. **CSV Files**: Detailed results for further analysis\n",
    "4. **Final Recommendations**: Top 5 most important variables for each configuration\n",
    "\n",
    "### Configuration Options:\n",
    "\n",
    "You can modify the following variables in Cell 34:\n",
    "- `models`: List of climate models to analyze\n",
    "- `sets`: Dictionary of bioclimatic variable sets to test\n",
    "- `specie`: Target species ('leptocybe-invasa' or 'thaumastocoris-peregrinus')\n",
    "- `region_train` and `region_test`: Training and test regions\n",
    "- `pseudoabsence`: Method for generating pseudo-absence points\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- The analysis may take several minutes to complete depending on your data size\n",
    "- Make sure you have the required data files in the correct directories\n",
    "- The analysis will automatically save results if `savefig = True`\n",
    "- Check the console output for progress updates and any error messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29817080",
   "metadata": {},
   "source": [
    "## 6. Robust Variable Importance Analysis with Multiple Iterations\n",
    "\n",
    "This section runs the variable importance analysis multiple times (10 iterations) to ensure robust and reliable results. Multiple iterations help account for:\n",
    "\n",
    "- **Random variation** in permutation importance calculations\n",
    "- **Model instability** across different training runs\n",
    "- **Statistical significance** of variable rankings\n",
    "- **Consistency** of importance patterns\n",
    "\n",
    "### Methodology:\n",
    "- **10 Independent Runs**: Each run uses the same data but different random seeds\n",
    "- **Aggregated Rankings**: Combines results across all iterations\n",
    "- **Statistical Analysis**: Calculates mean, standard deviation, and confidence intervals\n",
    "- **Robust Selection**: Identifies variables that are consistently important across runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # MULTI-ITERATION VARIABLE IMPORTANCE ANALYSIS\n",
    "# # =============================================================================\n",
    "\n",
    "# import random\n",
    "# from collections import defaultdict\n",
    "# import statistics\n",
    "\n",
    "# def run_multiple_iterations(x_train, y_train, sample_weight_train, x_test, y_test, sample_weight_test,\n",
    "#                            n_iterations=10, target_variables=5, min_variables=3):\n",
    "#     \"\"\"\n",
    "#     Run variable importance analysis multiple times with different random seeds.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     x_train, y_train, sample_weight_train : training data\n",
    "#     x_test, y_test, sample_weight_test : test data\n",
    "#     n_iterations : int, number of independent runs\n",
    "#     target_variables : int, target number of variables to keep\n",
    "#     min_variables : int, minimum number of variables to keep\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     aggregated_results : dict, containing aggregated results across all iterations\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(f\"Running {n_iterations} iterations of variable importance analysis...\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     all_results = []\n",
    "#     all_final_variables = []\n",
    "#     all_removed_variables = []\n",
    "#     all_performance_histories = []\n",
    "    \n",
    "#     # Storage for aggregated importance scores\n",
    "#     variable_importance_aggregated = defaultdict(list)\n",
    "#     variable_rankings_aggregated = defaultdict(list)\n",
    "    \n",
    "#     for iteration in range(1, n_iterations + 1):\n",
    "#         print(f\"\\n{'='*20} ITERATION {iteration}/{n_iterations} {'='*20}\")\n",
    "        \n",
    "#         # Set random seed for reproducibility\n",
    "#         random_seed = 42 + iteration\n",
    "#         np.random.seed(random_seed)\n",
    "#         random.seed(random_seed)\n",
    "        \n",
    "#         # Run single iteration\n",
    "#         single_result = iterative_variable_removal(\n",
    "#             x_train, y_train, sample_weight_train,\n",
    "#             x_test, y_test, sample_weight_test,\n",
    "#             target_variables=target_variables,\n",
    "#             min_variables=min_variables\n",
    "#         )\n",
    "        \n",
    "#         # Store results\n",
    "#         all_results.append(single_result)\n",
    "#         all_final_variables.append(single_result['final_variables'])\n",
    "#         all_removed_variables.append(single_result['removed_variables'])\n",
    "#         all_performance_histories.append(single_result['performance_history'])\n",
    "        \n",
    "#         # Aggregate importance scores\n",
    "#         final_iter_key = f\"iteration_{len(single_result['performance_history'])}\"\n",
    "#         if final_iter_key in single_result['importance_rankings']:\n",
    "#             final_ranking = single_result['importance_rankings'][final_iter_key]['sorted_ranking']\n",
    "            \n",
    "#             for rank, (var, importance) in enumerate(final_ranking):\n",
    "#                 variable_importance_aggregated[var].append(importance)\n",
    "#                 variable_rankings_aggregated[var].append(rank + 1)\n",
    "        \n",
    "#         print(f\"Completed iteration {iteration}\")\n",
    "    \n",
    "#     # Calculate aggregated statistics\n",
    "#     aggregated_results = {\n",
    "#         'n_iterations': n_iterations,\n",
    "#         'all_results': all_results,\n",
    "#         'all_final_variables': all_final_variables,\n",
    "#         'all_removed_variables': all_removed_variables,\n",
    "#         'all_performance_histories': all_performance_histories,\n",
    "#         'variable_importance_stats': {},\n",
    "#         'variable_ranking_stats': {},\n",
    "#         'consistency_analysis': {}\n",
    "#     }\n",
    "    \n",
    "#     # Calculate statistics for each variable\n",
    "#     for var in variable_importance_aggregated:\n",
    "#         importance_scores = variable_importance_aggregated[var]\n",
    "#         ranking_scores = variable_rankings_aggregated[var]\n",
    "        \n",
    "#         aggregated_results['variable_importance_stats'][var] = {\n",
    "#             'mean': statistics.mean(importance_scores),\n",
    "#             'std': statistics.stdev(importance_scores) if len(importance_scores) > 1 else 0,\n",
    "#             'min': min(importance_scores),\n",
    "#             'max': max(importance_scores),\n",
    "#             'median': statistics.median(importance_scores),\n",
    "#             'scores': importance_scores\n",
    "#         }\n",
    "        \n",
    "#         aggregated_results['variable_ranking_stats'][var] = {\n",
    "#             'mean_rank': statistics.mean(ranking_scores),\n",
    "#             'std_rank': statistics.stdev(ranking_scores) if len(ranking_scores) > 1 else 0,\n",
    "#             'min_rank': min(ranking_scores),\n",
    "#             'max_rank': max(ranking_scores),\n",
    "#             'median_rank': statistics.median(ranking_scores),\n",
    "#             'ranks': ranking_scores\n",
    "#         }\n",
    "    \n",
    "#     # Analyze consistency\n",
    "#     final_vars_frequency = defaultdict(int)\n",
    "#     for final_vars in all_final_variables:\n",
    "#         for var in final_vars:\n",
    "#             final_vars_frequency[var] += 1\n",
    "    \n",
    "#     aggregated_results['consistency_analysis'] = {\n",
    "#         'final_variables_frequency': dict(final_vars_frequency),\n",
    "#         'most_consistent_variables': sorted(final_vars_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "#     }\n",
    "    \n",
    "#     return aggregated_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa89470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # RUN 10-ITERATION ANALYSIS\n",
    "# # =============================================================================\n",
    "\n",
    "# print(\"=\"*80)\n",
    "# print(\"ROBUST VARIABLE IMPORTANCE ANALYSIS - 10 ITERATIONS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Run the multi-iteration analysis\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Set parameters for robust analysis\n",
    "# n_iterations = 10\n",
    "# target_vars = 5\n",
    "# min_vars = 3\n",
    "\n",
    "# # Run multiple iterations\n",
    "# multi_iteration_results = run_multiple_iterations(\n",
    "#     x_train, y_train, sample_weight_train,\n",
    "#     x_test, y_test, sample_weight_test,\n",
    "#     n_iterations=n_iterations,\n",
    "#     target_variables=target_vars,\n",
    "#     min_variables=min_vars\n",
    "# )\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f\"\\n{'='*80}\")\n",
    "# print(f\"10-ITERATION ANALYSIS COMPLETED in {end_time - start_time:.1f} seconds\")\n",
    "# print(f\"{'='*80}\")\n",
    "\n",
    "# # Store results\n",
    "# importance_results['multi_iteration'] = multi_iteration_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # ANALYZE AND VISUALIZE MULTI-ITERATION RESULTS\n",
    "# # =============================================================================\n",
    "\n",
    "# # Extract aggregated statistics\n",
    "# importance_stats = multi_iteration_results['variable_importance_stats']\n",
    "# ranking_stats = multi_iteration_results['variable_ranking_stats']\n",
    "# consistency_analysis = multi_iteration_results['consistency_analysis']\n",
    "\n",
    "# # Create summary of results\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"AGGREGATED RESULTS FROM 10 ITERATIONS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Sort variables by mean importance\n",
    "# sorted_by_importance = sorted(importance_stats.items(), \n",
    "#                              key=lambda x: x[1]['mean'], reverse=True)\n",
    "\n",
    "# print(f\"\\nVariable Importance Rankings (Mean Â± Std across {n_iterations} iterations):\")\n",
    "# print(\"-\"*70)\n",
    "# print(f\"{'Rank':<4} {'Variable':<15} {'Mean':<8} {'Std':<8} {'Min':<8} {'Max':<8} {'Consistency':<12}\")\n",
    "# print(\"-\"*70)\n",
    "\n",
    "# for rank, (var, stats) in enumerate(sorted_by_importance, 1):\n",
    "#     consistency = consistency_analysis['final_variables_frequency'].get(var, 0)\n",
    "#     consistency_pct = (consistency / n_iterations) * 100\n",
    "#     print(f\"{rank:<4} {var:<15} {stats['mean']:<8.4f} {stats['std']:<8.4f} {stats['min']:<8.4f} {stats['max']:<8.4f} {consistency_pct:<12.1f}%\")\n",
    "\n",
    "# # Show most consistent variables\n",
    "# print(f\"\\nMost Consistent Variables (appeared in final set across iterations):\")\n",
    "# print(\"-\"*60)\n",
    "# for var, count in consistency_analysis['most_consistent_variables']:\n",
    "#     percentage = (count / n_iterations) * 100\n",
    "#     print(f\"{var:<15}: {count}/{n_iterations} iterations ({percentage:.1f}%)\")\n",
    "\n",
    "# # Performance analysis across iterations\n",
    "# print(f\"\\nPerformance Analysis Across {n_iterations} Iterations:\")\n",
    "# print(\"-\"*50)\n",
    "\n",
    "# # Calculate average performance for each number of variables\n",
    "# performance_by_nvars = defaultdict(list)\n",
    "# for perf_history in multi_iteration_results['all_performance_histories']:\n",
    "#     for iter_key, perf in perf_history.items():\n",
    "#         n_vars = perf['n_variables']\n",
    "#         performance_by_nvars[n_vars].append({\n",
    "#             'test_auc': perf['test_auc'],\n",
    "#             'test_auc_weighted': perf['test_auc_weighted'],\n",
    "#             'train_auc': perf['train_auc'],\n",
    "#             'train_auc_weighted': perf['train_auc_weighted']\n",
    "#         })\n",
    "\n",
    "# # Calculate average performance\n",
    "# avg_performance = {}\n",
    "# for n_vars in sorted(performance_by_nvars.keys(), reverse=True):\n",
    "#     perfs = performance_by_nvars[n_vars]\n",
    "#     avg_performance[n_vars] = {\n",
    "#         'avg_test_auc': statistics.mean([p['test_auc'] for p in perfs]),\n",
    "#         'std_test_auc': statistics.stdev([p['test_auc'] for p in perfs]) if len(perfs) > 1 else 0,\n",
    "#         'avg_test_auc_weighted': statistics.mean([p['test_auc_weighted'] for p in perfs]),\n",
    "#         'std_test_auc_weighted': statistics.stdev([p['test_auc_weighted'] for p in perfs]) if len(perfs) > 1 else 0,\n",
    "#         'avg_train_auc': statistics.mean([p['train_auc'] for p in perfs]),\n",
    "#         'std_train_auc': statistics.stdev([p['train_auc'] for p in perfs]) if len(perfs) > 1 else 0,\n",
    "#         'avg_train_auc_weighted': statistics.mean([p['train_auc_weighted'] for p in perfs]),\n",
    "#         'std_train_auc_weighted': statistics.stdev([p['train_auc_weighted'] for p in perfs]) if len(perfs) > 1 else 0,\n",
    "#     }\n",
    "\n",
    "# print(f\"{'Variables':<12} {'Avg Test AUC':<15} {'Std':<8} {'Avg Train AUC':<15} {'Std':<8}\")\n",
    "# print(\"-\"*70)\n",
    "# for n_vars in sorted(avg_performance.keys(), reverse=True):\n",
    "#     perf = avg_performance[n_vars]\n",
    "#     print(f\"{n_vars:<12} {perf['avg_test_auc']:<15.3f} {perf['std_test_auc']:<8.3f} {perf['avg_train_auc']:<15.3f} {perf['std_train_auc']:<8.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # CREATE COMPREHENSIVE MULTI-ITERATION VISUALIZATION\n",
    "# # =============================================================================\n",
    "\n",
    "# # Create a comprehensive figure for multi-iteration results\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "# fig.suptitle('Robust Variable Importance Analysis - 10 Iterations', fontsize=16, fontweight='bold')\n",
    "\n",
    "# # 1. Variable Importance with Error Bars\n",
    "# ax1 = axes[0, 0]\n",
    "# top_vars = sorted_by_importance[:10]  # Top 10 variables\n",
    "# var_names = [var[0] for var in top_vars]\n",
    "# var_means = [var[1]['mean'] for var in top_vars]\n",
    "# var_stds = [var[1]['std'] for var in top_vars]\n",
    "\n",
    "# bars = ax1.barh(range(len(var_names)), var_means, xerr=var_stds, \n",
    "#                 color='tab:green', alpha=0.7, capsize=3)\n",
    "# ax1.set_yticks(range(len(var_names)))\n",
    "# ax1.set_yticklabels(var_names)\n",
    "# ax1.set_xlabel('Mean Permutation Importance Â± Std')\n",
    "# ax1.set_title('Top 10 Variables (Mean Â± Std across 10 iterations)')\n",
    "# ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# # 2. Consistency Analysis\n",
    "# ax2 = axes[0, 1]\n",
    "# consistency_vars = consistency_analysis['most_consistent_variables'][:10]\n",
    "# cons_var_names = [var[0] for var in consistency_vars]\n",
    "# cons_counts = [var[1] for var in consistency_vars]\n",
    "# cons_percentages = [(count / n_iterations) * 100 for count in cons_counts]\n",
    "\n",
    "# bars2 = ax2.barh(range(len(cons_var_names)), cons_percentages, color='tab:blue', alpha=0.7)\n",
    "# ax2.set_yticks(range(len(cons_var_names)))\n",
    "# ax2.set_yticklabels(cons_var_names)\n",
    "# ax2.set_xlabel('Consistency (%)')\n",
    "# ax2.set_title('Variable Consistency (appeared in final set)')\n",
    "# ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# # Add percentage labels\n",
    "# for i, (bar, pct) in enumerate(zip(bars2, cons_percentages)):\n",
    "#     ax2.text(pct + 1, i, f'{pct:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "# # 3. Performance vs Number of Variables (with error bars)\n",
    "# ax3 = axes[0, 2]\n",
    "# n_vars_sorted = sorted(avg_performance.keys(), reverse=True)\n",
    "# avg_test_aucs = [avg_performance[nv]['avg_test_auc'] for nv in n_vars_sorted]\n",
    "# std_test_aucs = [avg_performance[nv]['std_test_auc'] for nv in n_vars_sorted]\n",
    "# avg_train_aucs = [avg_performance[nv]['avg_train_auc'] for nv in n_vars_sorted]\n",
    "# std_train_aucs = [avg_performance[nv]['std_train_auc'] for nv in n_vars_sorted]\n",
    "\n",
    "# ax3.errorbar(n_vars_sorted, avg_test_aucs, yerr=std_test_aucs, \n",
    "#              marker='o', label='Test AUC', color='tab:orange', capsize=3)\n",
    "# ax3.errorbar(n_vars_sorted, avg_train_aucs, yerr=std_train_aucs, \n",
    "#              marker='s', label='Train AUC', color='tab:blue', capsize=3)\n",
    "# ax3.set_xlabel('Number of Variables')\n",
    "# ax3.set_ylabel('AUC Score')\n",
    "# ax3.set_title('Average Performance vs Variables (10 iterations)')\n",
    "# ax3.legend()\n",
    "# ax3.grid(True, alpha=0.3)\n",
    "# ax3.invert_xaxis()\n",
    "\n",
    "# # 4. Importance Score Distribution (Box Plot)\n",
    "# ax4 = axes[1, 0]\n",
    "# # Get importance scores for top 5 variables\n",
    "# top5_vars = [var[0] for var in sorted_by_importance[:5]]\n",
    "# importance_data = [importance_stats[var]['scores'] for var in top5_vars]\n",
    "\n",
    "# box_plot = ax4.boxplot(importance_data, labels=top5_vars, patch_artist=True)\n",
    "# colors = ['tab:red', 'tab:green', 'tab:blue', 'tab:orange', 'tab:purple']\n",
    "# for patch, color in zip(box_plot['boxes'], colors):\n",
    "#     patch.set_facecolor(color)\n",
    "#     patch.set_alpha(0.7)\n",
    "\n",
    "# ax4.set_ylabel('Permutation Importance')\n",
    "# ax4.set_title('Importance Score Distribution (Top 5 Variables)')\n",
    "# ax4.tick_params(axis='x', rotation=45)\n",
    "# ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# # 5. Ranking Stability\n",
    "# ax5 = axes[1, 1]\n",
    "# # Calculate ranking stability (lower std = more stable)\n",
    "# ranking_stability = []\n",
    "# for var in top5_vars:\n",
    "#     if var in ranking_stats:\n",
    "#         stability = 1 / (1 + ranking_stats[var]['std_rank'])  # Inverse of std for stability\n",
    "#         ranking_stability.append(stability)\n",
    "#     else:\n",
    "#         ranking_stability.append(0)\n",
    "\n",
    "# bars5 = ax5.bar(range(len(top5_vars)), ranking_stability, color='tab:cyan', alpha=0.7)\n",
    "# ax5.set_xticks(range(len(top5_vars)))\n",
    "# ax5.set_xticklabels(top5_vars, rotation=45)\n",
    "# ax5.set_ylabel('Ranking Stability (1/(1+std_rank))')\n",
    "# ax5.set_title('Ranking Stability (Top 5 Variables)')\n",
    "# ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# # 6. Performance Degradation Analysis\n",
    "# ax6 = axes[1, 2]\n",
    "# # Calculate average performance drop\n",
    "# initial_avg_test_auc = avg_performance[max(avg_performance.keys())]['avg_test_auc']\n",
    "# test_drops = [(initial_avg_test_auc - avg_performance[nv]['avg_test_auc']) / initial_avg_test_auc * 100 \n",
    "#               for nv in n_vars_sorted]\n",
    "\n",
    "# ax6.plot(n_vars_sorted, test_drops, 'o-', color='tab:red', linewidth=2, markersize=6)\n",
    "# ax6.set_xlabel('Number of Variables')\n",
    "# ax6.set_ylabel('Average Performance Drop (%)')\n",
    "# ax6.set_title('Average Performance Degradation (10 iterations)')\n",
    "# ax6.grid(True, alpha=0.3)\n",
    "# ax6.invert_xaxis()\n",
    "\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8900cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the multi-iteration analysis figure\n",
    "# if savefig:\n",
    "#     if Future:\n",
    "#         if models:\n",
    "#             file_path = os.path.join(\n",
    "#                 figs_path,\n",
    "#                 '06_robust_var-importance_10iter_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration)\n",
    "#             )\n",
    "#         else:\n",
    "#             file_path = os.path.join(\n",
    "#                 figs_path,\n",
    "#                 '06_robust_var-importance_10iter_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration)\n",
    "#             )\n",
    "#         fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "#     else:\n",
    "#         if models:\n",
    "#             file_path = os.path.join(\n",
    "#                 figs_path,\n",
    "#                 '06_robust_var-importance_10iter_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration)\n",
    "#             )\n",
    "#         else:\n",
    "#             file_path = os.path.join(\n",
    "#                 figs_path,\n",
    "#                 '06_robust_var-importance_10iter_%s_%s_%s_%s.png' % (specie, training, bio, iteration)\n",
    "#             )\n",
    "#         fig.savefig(file_path, transparent=True, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "#     print(f\"Robust analysis figure saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # EXPORT MULTI-ITERATION RESULTS TO CSV\n",
    "# # =============================================================================\n",
    "\n",
    "# # Create comprehensive summary DataFrame for multi-iteration results\n",
    "# multi_iter_summary_data = []\n",
    "\n",
    "# # Add aggregated importance statistics\n",
    "# for var, stats in sorted_by_importance:\n",
    "#     consistency = consistency_analysis['final_variables_frequency'].get(var, 0)\n",
    "#     consistency_pct = (consistency / n_iterations) * 100\n",
    "    \n",
    "#     multi_iter_summary_data.append({\n",
    "#         'variable': var,\n",
    "#         'mean_importance': stats['mean'],\n",
    "#         'std_importance': stats['std'],\n",
    "#         'min_importance': stats['min'],\n",
    "#         'max_importance': stats['max'],\n",
    "#         'median_importance': stats['median'],\n",
    "#         'consistency_count': consistency,\n",
    "#         'consistency_percentage': consistency_pct,\n",
    "#         'mean_rank': ranking_stats[var]['mean_rank'] if var in ranking_stats else None,\n",
    "#         'std_rank': ranking_stats[var]['std_rank'] if var in ranking_stats else None,\n",
    "#         'min_rank': ranking_stats[var]['min_rank'] if var in ranking_stats else None,\n",
    "#         'max_rank': ranking_stats[var]['max_rank'] if var in ranking_stats else None\n",
    "#     })\n",
    "\n",
    "# # Create DataFrame\n",
    "# multi_iter_summary_df = pd.DataFrame(multi_iter_summary_data)\n",
    "\n",
    "# # Save to CSV\n",
    "# if savefig:\n",
    "#     csv_filename = f'06_robust_var_importance_10iter_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "#     csv_path = os.path.join(figs_path, csv_filename)\n",
    "#     multi_iter_summary_df.to_csv(csv_path, index=False)\n",
    "#     print(f\"Multi-iteration analysis summary saved to: {csv_path}\")\n",
    "\n",
    "# # Create detailed results DataFrame (all iterations)\n",
    "# detailed_results_data = []\n",
    "# for iter_num, result in enumerate(multi_iteration_results['all_results'], 1):\n",
    "#     final_iter_key = f\"iteration_{len(result['performance_history'])}\"\n",
    "#     if final_iter_key in result['importance_rankings']:\n",
    "#         final_ranking = result['importance_rankings'][final_iter_key]['sorted_ranking']\n",
    "        \n",
    "#         for rank, (var, importance) in enumerate(final_ranking, 1):\n",
    "#             detailed_results_data.append({\n",
    "#                 'iteration': iter_num,\n",
    "#                 'variable': var,\n",
    "#                 'rank': rank,\n",
    "#                 'importance_score': importance,\n",
    "#                 'in_final_set': var in result['final_variables']\n",
    "#             })\n",
    "\n",
    "# detailed_df = pd.DataFrame(detailed_results_data)\n",
    "\n",
    "# # Save detailed results\n",
    "# if savefig:\n",
    "#     detailed_csv_filename = f'06_detailed_var_importance_10iter_{specie}_{training}_{bio}_{iteration}.csv'\n",
    "#     detailed_csv_path = os.path.join(figs_path, detailed_csv_filename)\n",
    "#     detailed_df.to_csv(detailed_csv_path, index=False)\n",
    "#     print(f\"Detailed multi-iteration results saved to: {detailed_csv_path}\")\n",
    "\n",
    "# # Display final recommendations\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"FINAL RECOMMENDATIONS - ROBUST VARIABLE SELECTION\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Get top 5 most consistent variables\n",
    "# top_consistent = consistency_analysis['most_consistent_variables'][:5]\n",
    "# print(f\"\\nTop 5 Most Consistent Variables (recommended for final model):\")\n",
    "# print(\"-\"*60)\n",
    "# for i, (var, count) in enumerate(top_consistent, 1):\n",
    "#     percentage = (count / n_iterations) * 100\n",
    "#     importance_mean = importance_stats[var]['mean'] if var in importance_stats else 0\n",
    "#     importance_std = importance_stats[var]['std'] if var in importance_stats else 0\n",
    "#     print(f\"{i}. {var:<15} - {count}/{n_iterations} iterations ({percentage:.1f}%) - Importance: {importance_mean:.4f} Â± {importance_std:.4f}\")\n",
    "\n",
    "# # Get top 5 by mean importance\n",
    "# top_importance = sorted_by_importance[:5]\n",
    "# print(f\"\\nTop 5 Variables by Mean Importance:\")\n",
    "# print(\"-\"*50)\n",
    "# for i, (var, stats) in enumerate(top_importance, 1):\n",
    "#     consistency = consistency_analysis['final_variables_frequency'].get(var, 0)\n",
    "#     consistency_pct = (consistency / n_iterations) * 100\n",
    "#     print(f\"{i}. {var:<15} - Importance: {stats['mean']:.4f} Â± {stats['std']:.4f} - Consistency: {consistency_pct:.1f}%\")\n",
    "\n",
    "# print(f\"\\nAnalysis completed with {n_iterations} iterations.\")\n",
    "# print(f\"Total variables analyzed: {len(importance_stats)}\")\n",
    "# print(f\"Average performance maintained across iterations with robust variable selection.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef32d7f",
   "metadata": {},
   "source": [
    "## 7. Summary and Recommendations\n",
    "\n",
    "### Key Benefits of 10-Iteration Analysis:\n",
    "\n",
    "1. **Robustness**: Multiple iterations account for random variation in model training and importance calculations\n",
    "2. **Statistical Significance**: Provides mean, standard deviation, and confidence intervals for importance scores\n",
    "3. **Consistency Analysis**: Identifies variables that are consistently important across different runs\n",
    "4. **Performance Stability**: Shows how model performance varies with different variable sets\n",
    "\n",
    "### Final Recommendations:\n",
    "\n",
    "1. **Use Most Consistent Variables**: Variables that appear in the final set across most iterations are most reliable\n",
    "2. **Consider Importance + Consistency**: Balance between high importance and high consistency\n",
    "3. **Validate on Independent Data**: Test the selected variables on completely independent datasets\n",
    "4. **Monitor Performance**: Track how the reduced variable set performs in real-world applications\n",
    "\n",
    "### Files Generated:\n",
    "- **Robust analysis figure**: 6-panel visualization showing comprehensive results\n",
    "- **Summary CSV**: Aggregated statistics across all 10 iterations\n",
    "- **Detailed CSV**: Individual results for each iteration\n",
    "- **Console output**: Detailed rankings and recommendations\n",
    "\n",
    "### Next Steps:\n",
    "1. Use the identified top 5 variables for future modeling\n",
    "2. Consider running additional iterations if results are not stable\n",
    "3. Validate the selected variables on independent test data\n",
    "4. Document the ecological significance of the selected variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a733f05-26cb-4da3-a95d-7adc42870020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels and open training output NetCDF for metadata\n",
    "labels = train.drop(columns=['class', 'geometry', 'SampleWeight']).columns.values\n",
    "training_output = xr.open_dataset(os.path.join(exp_path, nc_name))\n",
    "# display(labels)\n",
    "# display(training_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d9454-dce3-4b7d-922b-8f84e37f1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute partial dependence across features\n",
    "# - percentiles bounds the feature grid to observed range (2.5% to 97.5%)\n",
    "# - nbins controls resolution of the curve\n",
    "percentiles = (0.025, 0.975)\n",
    "nbins = 100\n",
    "\n",
    "mean = {}\n",
    "stdv = {}\n",
    "bins = {}\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    # Request individual PDP curves across samples, then summarize\n",
    "    pda = inspection.partial_dependence(\n",
    "        model_train,\n",
    "        x_train,\n",
    "        [idx],\n",
    "        percentiles=percentiles,\n",
    "        grid_resolution=nbins,\n",
    "        kind=\"individual\",\n",
    "    )\n",
    "\n",
    "    mean[label] = pda[\"individual\"][0].mean(axis=0)  # average response\n",
    "    stdv[label] = pda[\"individual\"][0].std(axis=0)   # variability across samples\n",
    "    bins[label] = pda[\"grid_values\"][0]              # feature grid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e805fc6-8c45-4c42-a7fd-255e89a152b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(pda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890d37e-af5a-4f15-966a-02bde1a1f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDPs with uncertainty bands for each predictor\n",
    "ncols, nrows = subplot_layout(len(labels))\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 6, nrows * 6))\n",
    "\n",
    "# Normalize axes list for consistent indexing\n",
    "if (nrows, ncols) == (1, 1):\n",
    "    ax = [axs]\n",
    "else:\n",
    "    ax = axs.ravel()\n",
    "\n",
    "xlabels = training_output.data_vars\n",
    "for iax, label in enumerate(labels):\n",
    "    ax[iax].set_title(label)\n",
    "    try:\n",
    "        ax[iax].set_xlabel(xlabels[label].long_name)\n",
    "    except (ValueError, AttributeError):\n",
    "        ax[iax].set_xlabel('No variable long_name')\n",
    "\n",
    "    # Uncertainty band: mean Â± std across individuals\n",
    "    ax[iax].fill_between(bins[label], mean[label] - stdv[label], mean[label] + stdv[label], alpha=0.25)\n",
    "    ax[iax].plot(bins[label], mean[label])\n",
    "\n",
    "# Style axes\n",
    "for axi in ax:\n",
    "    axi.set_ylim([0, 1])\n",
    "    axi.set_ylabel('probability of occurrence')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2d9fa-5f37-48b4-a03c-9f53f244b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save response curve figures if requested\n",
    "if savefig:\n",
    "    if Future:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_%s_future.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_future.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s_%s.png' % (specie, training, bio, model_prefix, iteration),\n",
    "            )\n",
    "        else:\n",
    "            file_path = os.path.join(\n",
    "                figs_path,\n",
    "                '06_resp-curves_%s_%s_%s_%s.png' % (specie, training, bio, iteration),\n",
    "            )\n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f47155-3512-4f32-9128-9595e1709c6a",
   "metadata": {},
   "source": [
    "### 3.3 Variable importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b404609-047b-4e31-bf5f-28b0de041906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = model_train.permutation_importance_plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4362f3-93b0-4626-a37b-bf36a36900dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance: measures drop in performance when each feature is shuffled\n",
    "# Higher drop => more important feature\n",
    "pi = inspection.permutation_importance(model_train, x_train, y_train, n_repeats=10)\n",
    "importance = pi.importances\n",
    "rank_order = importance.mean(axis=-1).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301c544-3a98-4bac-ab55-d0c07373f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation importances as horizontal boxplots (distribution over repeats)\n",
    "labels_ranked = [labels[idx] for idx in rank_order]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "box = ax.boxplot(importance[rank_order].T, vert=False, labels=labels_ranked)\n",
    "# Decorate legend labels for key boxplot elements\n",
    "box['fliers'][0].set_label('outlier')\n",
    "box['medians'][0].set_label('median')\n",
    "for icap, cap in enumerate(box['caps']):\n",
    "    if icap == 0:\n",
    "        cap.set_label('min-max')\n",
    "    cap.set_color('k')\n",
    "    cap.set_linewidth(2)\n",
    "for ibx, bx in enumerate(box['boxes']):\n",
    "    if ibx == 0:\n",
    "        bx.set_label('25-75%')\n",
    "    bx.set_color('gray')\n",
    "\n",
    "ax.set_xlabel('Importance')\n",
    "ax.legend(loc='lower right')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cc671-1f8a-4375-9af2-8286603483ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if savefig:\n",
    "#     if Future:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_var-importance_%s_%s_%s_future.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "#     else:\n",
    "#         fig.savefig(os.path.join(figs_path, '06_var-importance_%s_%s_%s.png' %(specie, training, bio)), transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n",
    "if savefig:\n",
    "    if Future:\n",
    "        # Check if the 'model' variable is not null or empty\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_%s_future.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # If no model is specified, use the original filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_future.png' %(specie, training, bio, iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    else:\n",
    "        if models:\n",
    "            # If a model is specified, add it to the filename\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s_%s.png' %(specie, training, bio, model_prefix, iteration))\n",
    "        else:\n",
    "            # This is the original logic for non-future scenarios, which remains unchanged\n",
    "            file_path = os.path.join(figs_path, '06_var-importance_%s_%s_%s_%s.png' %(specie, training, bio,iteration))\n",
    "        \n",
    "        fig.savefig(file_path, transparent=True, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bias_bw",
   "language": "python",
   "name": "bias_bw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
